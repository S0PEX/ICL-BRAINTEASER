{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "def get_system_prompt(template_name: str):\n",
    "    # System message templates (priming)\n",
    "    system_templates = {\n",
    "        \"step_by_step\": \"You are a meticulous problem-solver. Analyze silently and return ONLY the correct answer text.\",\n",
    "        \"creative\": \"You excel at lateral thinking. Treat this as a riddle. Return ONLY the option text.\",\n",
    "        \"elimination\": \"Eliminate wrong options internally. Return ONLY the text of the correct answer.\",\n",
    "        \"metaphor\": \"Interpret keywords metaphorically. Return ONLY the correct answer text.\",\n",
    "        \"confidence\": \"Score options internally. Return ONLY the highest-scoring answer text.\",\n",
    "        \"perspective_shift\": \"Analyze through multiple perspectives silently. Return ONLY the answer text.\",\n",
    "        \"common_sense\": \"Combine logic and creativity. Return ONLY the correct answer text.\",\n",
    "        \"assumption_challenge\": \"Challenge hidden assumptions internally. Return ONLY the answer text.\",\n",
    "        \"pattern_matching\": \"Find patterns silently. Return ONLY the correct answer text.\",\n",
    "        \"intuitive\": \"Critique your intuition internally. Return ONLY the final answer text.\",\n",
    "    }\n",
    "\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "    Question: {question}\n",
    "    Choices:\n",
    "    {choices}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_prompt_template(\n",
    "    system_prompt_template_name: str = \"step_by_step\",\n",
    "):\n",
    "    system_prompt_template = get_system_prompt(system_prompt_template_name)\n",
    "    user_prompt_template = get_user_prompt()\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [system_prompt_template, user_prompt_template]\n",
    "    )\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.llm.lmm import LLM\n",
    "\n",
    "llm = LLM(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['choices', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a meticulous problem-solver. Analyze silently and return ONLY the correct answer text.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['choices', 'question'], input_types={}, partial_variables={}, template='\\nQuestion: {question}\\nChoices:\\n{choices}\\n'), additional_kwargs={})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'System: You are a meticulous problem-solver. Analyze silently and return ONLY the correct answer text.\\nHuman: \\nQuestion: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?\\nChoices:\\nOption 1: Some daughters get married and have their own family.\\nOption 2: Each daughter shares the same brother.\\nOption 3: Some brothers were not loved by family and moved away.\\nOption 4: None of above.\\nAnswer: Option 4: None of above.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_prompt_template = create_prompt_template(\"step_by_step\")\n",
    "\n",
    "riddle_question = dataset.sp_train[0]\n",
    "template_args = {\n",
    "    \"question\": riddle_question.question,\n",
    "    \"choices\": \"\\n\".join(\n",
    "        [\n",
    "            f\"Option {j + 1}: {choice}\"\n",
    "            for j, choice in enumerate(riddle_question.choice_list)\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "display(chat_prompt_template)\n",
    "result = llm.generate(chat_prompt_template, template_args)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.executor import ModelExecutor\n",
    "\n",
    "# executor = ModelExecutor(\n",
    "#     models=[\n",
    "#         \"google/gemma-2-2b-it\",\n",
    "#         \"google/gemma-2-9b-it\",\n",
    "#         \"microsoft/Phi-3.5-mini-instruct\",\n",
    "#         \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "#         \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "#         \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "#         \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "#         \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "#         \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "#     ],\n",
    "#     dataset=dataset.sp_train,\n",
    "# )\n",
    "\n",
    "# results = executor.run(create_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Prompt: [{'role': 'system', 'content': 'You are a helpful AI assistant. You will be given a question with multiple choice answers. Your task is to select the correct answer from the given choices.'}, {'role': 'user', 'content': '\\nQuestion: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?\\nChoices:\\nOption 1: Some daughters get married and have their own family.\\nOption 2: Each daughter shares the same brother.\\nOption 3: Some brothers were not loved by family and moved away.\\nOption 4: None of above.\\nAnswer: Please return the answer in the format of the choice.\\n'}]\n",
      "Output: Option 2: Each daughter shares the same brother.\n",
      "--------------------------------------------------\n",
      "Prompt: [{'role': 'system', 'content': 'You are a helpful AI assistant. You will be given a question with multiple choice answers. Your task is to select the correct answer from the given choices.'}, {'role': 'user', 'content': '\\nQuestion: The six daughters of Mr. and Mrs. Mustard each have one brother. However, the family only consists of nine people; how is that possible?\\nChoices:\\nOption 1: Some brothers were not loved by family and moved away.\\nOption 2: Some daughters get married and have their own family.\\nOption 3: Each daughter shares the same brother.\\nOption 4: None of above.\\nAnswer: Please return the answer in the format of the choice.\\n'}]\n",
      "Output: Option 1: Some brothers were not loved by family and moved away.\n",
      "--------------------------------------------------\n",
      "Prompt: [{'role': 'system', 'content': 'You are a helpful AI assistant. You will be given a question with multiple choice answers. Your task is to select the correct answer from the given choices.'}, {'role': 'user', 'content': '\\nQuestion: A chess team has five players, and each player has one coach. But there are only six participants in the team. How is that possible?\\nChoices:\\nOption 1: Each player shares the same coach.\\nOption 2: Some players are backups and not allowed to play.\\nOption 3: Some coaches get a raise.\\nOption 4: None of above.\\nAnswer: Please return the answer in the format of the choice.\\n'}]\n",
      "Output: Option 4: None of above\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for model, model_results in results.items():\n",
    "#     print(f\"Model: {model}\")\n",
    "#     for result in model_results:\n",
    "#         print(f\"Prompt: {result.prompt()}\")\n",
    "#         print(f\"Output: {result.model_response()}\")\n",
    "#         print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
