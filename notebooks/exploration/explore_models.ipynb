{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are a helpful assistant.\",\n",
    "    \"step_by_step\": \"You are a meticulous problem-solver.\",\n",
    "    \"creative\": \"You excel at lateral thinking. Treat this as a riddle.\",\n",
    "    \"elimination\": \"Eliminate wrong options internally.\",\n",
    "    \"metaphor\": \"Interpret keywords metaphorically.\",\n",
    "    \"confidence\": \"Score options internally.\",\n",
    "    \"perspective_shift\": \"Analyze through multiple perspectives silently.\",\n",
    "    \"common_sense\": \"Combine logic and creativity.\",\n",
    "    \"assumption_challenge\": \"Challenge hidden assumptions internally.\",\n",
    "    \"pattern_matching\": \"Find patterns silently.\",\n",
    "    \"intuitive\": \"Critique your intuition internally.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_prompt_template(\n",
    "    system_prompt_template_name: str = \"default\",\n",
    "):\n",
    "    system_prompt_template = get_system_prompt(system_prompt_template_name)\n",
    "    user_prompt_template = get_user_prompt()\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [system_prompt_template, user_prompt_template]\n",
    "    )\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:16:32,517 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        OllamaModel(\"llama3.2:1b\"),\n",
    "        OllamaModel(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        OllamaModel(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        OllamaModel(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(\"qwen2.5:0.5b\"),\n",
    "        OllamaModel(\"qwen2.5:1.5b\"),\n",
    "        OllamaModel(\"qwen2.5:3b\"),\n",
    "        OllamaModel(\"qwen2.5:7b\"),\n",
    "        OllamaModel(\"qwen2.5:14b\"),\n",
    "        OllamaModel(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        OllamaModel(\"gemma2:2b\"),\n",
    "        OllamaModel(\"gemma2:9b\"),\n",
    "        OllamaModel(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\"mistral-nemo:12b\"),\n",
    "        # # Deepseek R1\n",
    "        # OllamaLlm(\"deepseek-r1:1.5b\"),\n",
    "        # OllamaLlm(\"deepseek-r1:8b\"),\n",
    "        # OllamaLlm(\"deepseek-r1:14b\"),\n",
    "        # OllamaLlm(\"deepseek-r1:32b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:03:41,102 - INFO - Initialized executor with 15 models and 627 riddles\n",
      "2025-02-13 16:03:41,105 - INFO - Starting execution\n",
      "2025-02-13 16:03:41,105 - INFO - Processing llama3.1:8b\n",
      "2025-02-13 16:03:41,105 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9fd8b74b5e4669a17583e7c98ac9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:07:08,989 - INFO - Dumping results to results/checkpoints/llama3.1:8b_sp_results_naive.pkl\n",
      "2025-02-13 16:07:09,117 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-13 16:07:09,117 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:07:09,118 - INFO - Processing llama3.2:1b\n",
      "2025-02-13 16:07:09,118 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ce67617eef461fbdd0835fccae4ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:09:15,678 - INFO - Dumping results to results/checkpoints/llama3.2:1b_sp_results_naive.pkl\n",
      "2025-02-13 16:09:15,807 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-13 16:09:15,808 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:09:15,808 - INFO - Processing llama3.2:3b\n",
      "2025-02-13 16:09:15,808 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a870f415ca4b44ae7f905d6c05fdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:11:45,842 - INFO - Dumping results to results/checkpoints/llama3.2:3b_sp_results_naive.pkl\n",
      "2025-02-13 16:11:45,977 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-13 16:11:45,978 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:11:45,978 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-13 16:11:45,978 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3339d9d433f84d73af7b34fe6fee8e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:30:34,148 - INFO - Dumping results to results/checkpoints/phi3.5:3.8b_sp_results_naive.pkl\n",
      "2025-02-13 16:30:34,360 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-13 16:30:34,361 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:30:34,361 - INFO - Processing phi4:14b\n",
      "2025-02-13 16:30:34,361 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd20a0ac3444c32926f7cfdc2a6b5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:49:30,000 - INFO - Dumping results to results/checkpoints/phi4:14b_sp_results_naive.pkl\n",
      "2025-02-13 16:49:30,143 - INFO - Cleaning up phi4:14b\n",
      "2025-02-13 16:49:30,143 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:49:30,143 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-13 16:49:30,143 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6735df5583e94801a5a610a7324d2824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:53:50,679 - INFO - Dumping results to results/checkpoints/qwen2.5:0.5b_sp_results_naive.pkl\n",
      "2025-02-13 16:53:50,836 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-13 16:53:50,836 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:53:50,837 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-13 16:53:50,837 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2342571d7c94d7ab384acbeb2bb80a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:56:32,565 - INFO - Dumping results to results/checkpoints/qwen2.5:1.5b_sp_results_naive.pkl\n",
      "2025-02-13 16:56:32,802 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-13 16:56:32,803 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:56:32,803 - INFO - Processing qwen2.5:3b\n",
      "2025-02-13 16:56:32,803 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9ad310ecd742d2aaa8cd1bf9bbc67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:59:20,711 - INFO - Dumping results to results/checkpoints/qwen2.5:3b_sp_results_naive.pkl\n",
      "2025-02-13 16:59:20,842 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-13 16:59:20,842 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 16:59:20,842 - INFO - Processing qwen2.5:7b\n",
      "2025-02-13 16:59:20,842 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d648b4949d415aaadaf4670f82d867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:04:13,802 - INFO - Dumping results to results/checkpoints/qwen2.5:7b_sp_results_naive.pkl\n",
      "2025-02-13 17:04:14,034 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-13 17:04:14,034 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:04:14,034 - INFO - Processing qwen2.5:14b\n",
      "2025-02-13 17:04:14,034 - INFO - Pulling Ollama model: qwen2.5:14b\n",
      "2025-02-13 17:05:13,177 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-2049f5674b1e92b4464e5729975c9689fcfbf0b0e4443ccf10b5339f370f9a54-partial: no space left on device (status code: 500)\n",
      "2025-02-13 17:05:13,178 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c72a3957450432dabf15f089900ff9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:16:13,140 - INFO - Dumping results to results/checkpoints/qwen2.5:14b_sp_results_naive.pkl\n",
      "2025-02-13 17:16:13,275 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-13 17:16:13,275 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:16:13,276 - INFO - Processing qwen2.5:32b\n",
      "2025-02-13 17:16:13,276 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119970f7575e4e4bb78a9e0b5800b28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:35:17,577 - INFO - Dumping results to results/checkpoints/qwen2.5:32b_sp_results_naive.pkl\n",
      "2025-02-13 17:35:17,829 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-13 17:35:17,830 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:35:17,831 - INFO - Processing gemma2:2b\n",
      "2025-02-13 17:35:17,831 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6edd2bcb8b4a7aa766ab167ba919f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:38:20,660 - INFO - Dumping results to results/checkpoints/gemma2:2b_sp_results_naive.pkl\n",
      "2025-02-13 17:38:20,792 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-13 17:38:20,792 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:38:20,792 - INFO - Processing gemma2:9b\n",
      "2025-02-13 17:38:20,792 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2cd6155f774a66b4e81a156cce95ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:45:04,434 - INFO - Dumping results to results/checkpoints/gemma2:9b_sp_results_naive.pkl\n",
      "2025-02-13 17:45:04,697 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-13 17:45:04,698 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:45:04,698 - INFO - Processing gemma2:27b\n",
      "2025-02-13 17:45:04,698 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df5d5000cb547daa93d42d8d037c906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:56:58,153 - INFO - Dumping results to results/checkpoints/gemma2:27b_sp_results_naive.pkl\n",
      "2025-02-13 17:56:58,292 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-13 17:56:58,292 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 17:56:58,293 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-13 17:56:58,293 - INFO - Pulling Ollama model: mistral-nemo:12b\n",
      "2025-02-13 17:57:19,991 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-b559938ab7a0392fc9ea9675b82280f2a15669ec3e0e0fc491c9cb0a7681cf94-partial: no space left on device (status code: 500)\n",
      "2025-02-13 17:57:19,991 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1030679eacad465cb8658094d50e40f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:04:41,512 - INFO - Dumping results to results/checkpoints/mistral-nemo:12b_sp_results_naive.pkl\n",
      "2025-02-13 18:04:41,799 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-13 18:04:41,800 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:04:41,800 - INFO - Execution complete\n",
      "2025-02-13 18:04:41,800 - INFO - Dumping results to results/sp_results_naive\n"
     ]
    }
   ],
   "source": [
    "sp_results = executor.execute(\n",
    "    dataset.sp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"sp_results_naive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:16:37,538 - INFO - Starting execution\n",
      "2025-02-13 18:16:37,538 - INFO - Processing llama3.1:8b\n",
      "2025-02-13 18:16:37,539 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5561c6537fc489eb237b6c6471839bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:18:15,731 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.1:8b_wp_results_naive.pkl\n",
      "2025-02-13 18:18:15,922 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-13 18:18:15,923 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:18:15,923 - INFO - Processing llama3.2:1b\n",
      "2025-02-13 18:18:15,923 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f3acfc37074993b303f004823451ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:19:55,016 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.2:1b_wp_results_naive.pkl\n",
      "2025-02-13 18:19:55,127 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-13 18:19:55,128 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:19:55,128 - INFO - Processing llama3.2:3b\n",
      "2025-02-13 18:19:55,128 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f318118ba4d86a170df5d1e68497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:21:48,487 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.2:3b_wp_results_naive.pkl\n",
      "2025-02-13 18:21:48,666 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-13 18:21:48,666 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:21:48,667 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-13 18:21:48,667 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116322335fb45b381ce039bad497dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:36:22,407 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/phi3.5:3.8b_wp_results_naive.pkl\n",
      "2025-02-13 18:36:22,516 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-13 18:36:22,516 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:36:22,517 - INFO - Processing phi4:14b\n",
      "2025-02-13 18:36:22,517 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c33aa9ca594b78bafca55045b48a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:51:51,261 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/phi4:14b_wp_results_naive.pkl\n",
      "2025-02-13 18:51:51,364 - INFO - Cleaning up phi4:14b\n",
      "2025-02-13 18:51:51,365 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:51:51,365 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-13 18:51:51,365 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfca8a93e904626b9a6fc4dee1dbd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:54:12,117 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:0.5b_wp_results_naive.pkl\n",
      "2025-02-13 18:54:12,227 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-13 18:54:12,228 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:54:12,228 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-13 18:54:12,228 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b933f969ed9463f878e161f92f254f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:56:07,012 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:1.5b_wp_results_naive.pkl\n",
      "2025-02-13 18:56:07,211 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-13 18:56:07,211 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:56:07,212 - INFO - Processing qwen2.5:3b\n",
      "2025-02-13 18:56:07,212 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb9aed2b896474f8ffd7f1a2b1b110e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:58:28,646 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:3b_wp_results_naive.pkl\n",
      "2025-02-13 18:58:28,757 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-13 18:58:28,758 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:58:28,759 - INFO - Processing qwen2.5:7b\n",
      "2025-02-13 18:58:28,759 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aef65000d0e4023aca48835871b560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:02:40,346 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:7b_wp_results_naive.pkl\n",
      "2025-02-13 19:02:40,451 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-13 19:02:40,452 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:02:40,452 - INFO - Processing qwen2.5:14b\n",
      "2025-02-13 19:02:40,452 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2cf7c85f6847c5a01d7b762a978fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:10:21,899 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:14b_wp_results_naive.pkl\n",
      "2025-02-13 19:10:22,006 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-13 19:10:22,006 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:10:22,007 - INFO - Processing qwen2.5:32b\n",
      "2025-02-13 19:10:22,007 - INFO - Pulling Ollama model: qwen2.5:32b\n",
      "2025-02-13 19:11:58,642 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-eabc98a9bcbfce7fd70f3e07de599f8fda98120fefed5881934161ede8bd1a41-partial: no space left on device (status code: 500)\n",
      "2025-02-13 19:11:58,643 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c61f30df84a648ae3bed17fcd1cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:28:58,750 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:32b_wp_results_naive.pkl\n",
      "2025-02-13 19:28:58,859 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-13 19:28:58,859 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:28:58,860 - INFO - Processing gemma2:2b\n",
      "2025-02-13 19:28:58,860 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee8ced99504835a025f176d2b8dd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:31:16,516 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:2b_wp_results_naive.pkl\n",
      "2025-02-13 19:31:16,739 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-13 19:31:16,739 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:31:16,739 - INFO - Processing gemma2:9b\n",
      "2025-02-13 19:31:16,740 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a890a798a4569baf5678f47e91837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:35:37,128 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:9b_wp_results_naive.pkl\n",
      "2025-02-13 19:35:37,231 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-13 19:35:37,231 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:35:37,231 - INFO - Processing gemma2:27b\n",
      "2025-02-13 19:35:37,231 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d766a8e05efa4c85bb5049afa1c9d7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:43:17,563 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:27b_wp_results_naive.pkl\n",
      "2025-02-13 19:43:17,664 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-13 19:43:17,665 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:43:17,665 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-13 19:43:17,665 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cdaea39214499bbe522eec6054ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:50:35,816 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/mistral-nemo:12b_wp_results_naive.pkl\n",
      "2025-02-13 19:50:36,042 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-13 19:50:36,042 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:50:36,043 - INFO - Execution complete\n",
      "2025-02-13 19:50:36,043 - INFO - Dumping results to results/wp_results_naive.pkl\n"
     ]
    }
   ],
   "source": [
    "wp_results = executor.execute(\n",
    "    dataset.wp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"wp_results_naive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 07:22:14,310 - INFO - Initialized executor with 4 models.\n",
      "2025-02-14 07:22:14,324 - INFO - Starting execution\n",
      "2025-02-14 07:22:14,325 - INFO - Processing deepseek-r1:8b\n",
      "2025-02-14 07:22:14,325 - INFO - Pulling Ollama model: deepseek-r1:8b\n",
      "2025-02-14 07:22:42,308 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/pull \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773f83edb941484b8bbac9f2ace95c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:8b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 07:22:55,583 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 07:28:02,795 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 07:28:08,877 - INFO - Cleaning up deepseek-r1:8b\n",
      "2025-02-14 07:28:08,878 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Executor\n\u001b[1;32m      4\u001b[0m executor_deepseek \u001b[38;5;241m=\u001b[39m Executor(\n\u001b[1;32m      5\u001b[0m     models\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Deepseek R1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m wp_results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_deepseek\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_prompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdump_to_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msp_results_naive_deepseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m wp_results \u001b[38;5;241m=\u001b[39m executor_deepseek\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m     24\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mwp,\n\u001b[1;32m     25\u001b[0m     chat_prompt_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     result_file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwp_results_naive_deepseek\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:126\u001b[0m, in \u001b[0;36mExecutor.execute\u001b[0;34m(self, dataset, prompt_template, args_generator, dump_to_pickle, result_file_name, create_checkpoints)\u001b[0m\n\u001b[1;32m    120\u001b[0m model_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m riddle \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    122\u001b[0m     dataset,\n\u001b[1;32m    123\u001b[0m     desc\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    124\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset),\n\u001b[1;32m    125\u001b[0m ):\n\u001b[0;32m--> 126\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_riddle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mriddle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     model_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    133\u001b[0m results[model\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m model_results\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:74\u001b[0m, in \u001b[0;36mExecutor._execute_riddle\u001b[0;34m(self, model, riddle, prompt_template, args_generator)\u001b[0m\n\u001b[1;32m     72\u001b[0m template_args \u001b[38;5;241m=\u001b[39m args_generator(riddle)\n\u001b[1;32m     73\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m delta \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ExecutionResult(\n\u001b[1;32m     78\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     79\u001b[0m     riddle\u001b[38;5;241m=\u001b[39mriddle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     execution_time\u001b[38;5;241m=\u001b[39mdelta,\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/lmm.py:142\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, chat_template, args)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, chat_template: ChatPromptTemplate, args: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatHistory:\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text based on the provided chat template and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     messages \u001b[38;5;241m=\u001b[39m chat_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    145\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke(messages)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:701\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    696\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    700\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 701\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    705\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    706\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    707\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    712\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:602\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    601\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:589\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/ollama/_client.py:170\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m:=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:929\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:916\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    914\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 916\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 951\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "executor_deepseek = Executor(\n",
    "    models=[\n",
    "        # Deepseek R1\n",
    "        OllamaModel(\"deepseek-r1:8b\"),\n",
    "        OllamaModel(\"deepseek-r1:14b\"),\n",
    "        OllamaModel(\"deepseek-r1:32b\"),\n",
    "        OllamaModel(\"deepseek-r1:1.5b\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "wp_results = executor_deepseek.execute(\n",
    "    dataset.sp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"sp_results_naive_deepseek\",\n",
    ")\n",
    "\n",
    "wp_results = executor_deepseek.execute(\n",
    "    dataset.wp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"wp_results_naive_deepseek\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
