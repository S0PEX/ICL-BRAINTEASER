{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are a helpful assistant.\",\n",
    "    \"step_by_step\": \"You are a meticulous problem-solver.\",\n",
    "    \"creative\": \"You excel at lateral thinking. Treat this as a riddle.\",\n",
    "    \"elimination\": \"Eliminate wrong options internally.\",\n",
    "    \"metaphor\": \"Interpret keywords metaphorically.\",\n",
    "    \"confidence\": \"Score options internally.\",\n",
    "    \"perspective_shift\": \"Analyze through multiple perspectives silently.\",\n",
    "    \"common_sense\": \"Combine logic and creativity.\",\n",
    "    \"assumption_challenge\": \"Challenge hidden assumptions internally.\",\n",
    "    \"pattern_matching\": \"Find patterns silently.\",\n",
    "    \"intuitive\": \"Critique your intuition internally.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt_template(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_humand_prompt_template():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "        \"answer\": string.ascii_uppercase[riddle_question.label],\n",
    "    }\n",
    "\n",
    "    return template_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the few shot exampel but following the best practices from https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n",
    "# Thus we do not provide the examples in the initial client prompt but as a message history of the user asking and the system answering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "def get_few_shot_chat_template(\n",
    "    dataset: list[RiddleQuestion],\n",
    "    number_of_shots: int = 4,\n",
    "    system_template: str = \"default\",\n",
    "):\n",
    "    riddles_as_examples = dataset[:number_of_shots]\n",
    "    riddles_to_solve = dataset[number_of_shots:]\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_humand_prompt_template(),\n",
    "            (\"ai\", \"{answer}\"),\n",
    "        ]\n",
    "    )\n",
    "    few_shot_prompt_naive = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=[args_generator(example) for example in riddles_as_examples],\n",
    "    )\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_system_prompt_template(system_template),\n",
    "            few_shot_prompt_naive,\n",
    "            get_humand_prompt_template(),\n",
    "        ]\n",
    "    )\n",
    "    return (chat_prompt_template, riddles_to_solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "# riddles = dataset.sp\n",
    "# examples_full = [args_generator(riddle) for riddle in riddles]\n",
    "# example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "#     examples,\n",
    "#     OllamaEmbeddings(\n",
    "#         model=\"bge-m3\"\n",
    "#     ),  # bge-m3 excel at handling context-rich queries due to higher embedding dimensions, as fallback we could also use models like  nomic-embed-text for short semantic queries\n",
    "#     Chroma,\n",
    "#     k=4,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:07:06,030 - INFO - Initialized executor with 18 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        OllamaModel(\"llama3.2:1b\"),\n",
    "        OllamaModel(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        OllamaModel(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        OllamaModel(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(\"qwen2.5:0.5b\"),\n",
    "        OllamaModel(\"qwen2.5:1.5b\"),\n",
    "        OllamaModel(\"qwen2.5:3b\"),\n",
    "        OllamaModel(\"qwen2.5:7b\"),\n",
    "        OllamaModel(\"qwen2.5:14b\"),\n",
    "        OllamaModel(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        OllamaModel(\"gemma2:2b\"),\n",
    "        OllamaModel(\"gemma2:9b\"),\n",
    "        OllamaModel(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:07:06,047 - INFO - Starting asynchronous execution\n",
      "2025-02-15 11:07:06,047 - INFO - Split dataset of 623 items into 125 batches of size 5\n",
      "2025-02-15 11:07:06,048 - INFO - Processing llama3.1:8b\n",
      "2025-02-15 11:07:06,048 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3ad8baa21c4d47a7193d7fc6d70fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:07:44,743 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.1:8b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:07:45,286 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-15 11:07:45,286 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:07:45,286 - INFO - Processing llama3.2:1b\n",
      "2025-02-15 11:07:45,287 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6391bb3183a84f24aa3280029fadcc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:08:10,417 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.2:1b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:08:10,857 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-15 11:08:10,858 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:08:10,858 - INFO - Processing llama3.2:3b\n",
      "2025-02-15 11:08:10,858 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ff637e0d143e3a9d5e3f7626515ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:08:53,357 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.2:3b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:08:53,688 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-15 11:08:53,689 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:08:53,689 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-15 11:08:53,689 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be0bfdbbda549148f0457d1becd7f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:15:14,994 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/phi3.5:3.8b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:15:15,323 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-15 11:15:15,323 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:15:15,324 - INFO - Processing phi4:14b\n",
      "2025-02-15 11:15:15,324 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818d9532f33e4b4ab49e7bda8cf18a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:17:29,425 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/phi4:14b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:17:29,738 - INFO - Cleaning up phi4:14b\n",
      "2025-02-15 11:17:29,738 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:17:29,739 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-15 11:17:29,739 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f8ffc8e633473d8341d027a129bcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:19:10,844 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:0.5b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:19:11,142 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-15 11:19:11,143 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:19:11,143 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-15 11:19:11,144 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d4677934d45f8a1ca702f47724dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:20:51,475 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:1.5b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:20:52,007 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-15 11:20:52,007 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:20:52,008 - INFO - Processing qwen2.5:3b\n",
      "2025-02-15 11:20:52,008 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e90d395124494b2a31cf62fb5d664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:22:37,350 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:3b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:22:37,947 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-15 11:22:37,947 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:22:37,947 - INFO - Processing qwen2.5:7b\n",
      "2025-02-15 11:22:37,947 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e1aee7d15481ab1dd4568b7b87804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:24:39,800 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:7b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:24:40,149 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-15 11:24:40,150 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:24:40,150 - INFO - Processing qwen2.5:14b\n",
      "2025-02-15 11:24:40,150 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f212df761e4428a04e14c62819a713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:27:09,605 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:14b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:27:10,177 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-15 11:27:10,178 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:27:10,178 - INFO - Processing qwen2.5:32b\n",
      "2025-02-15 11:27:10,178 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1848057bd4964d8e984b5cc7b94527d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:30:38,513 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:32b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:30:39,105 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-15 11:30:39,105 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:30:39,105 - INFO - Processing gemma2:2b\n",
      "2025-02-15 11:30:39,105 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe6e93734254eb99ad7846c05706c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:31:34,618 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:2b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:31:34,918 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-15 11:31:34,918 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:31:34,919 - INFO - Processing gemma2:9b\n",
      "2025-02-15 11:31:34,919 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd27f27477c4587bbe3b6d3ceaf9995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:33:54,524 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:9b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:33:55,159 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-15 11:33:55,160 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:33:55,160 - INFO - Processing gemma2:27b\n",
      "2025-02-15 11:33:55,160 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a16025afd6416493e8c2cc0b9bd538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:36:47,414 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:27b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:36:47,727 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-15 11:36:47,727 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:36:47,728 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-15 11:36:47,728 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f3ecbe66484e698e3e04af74925469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:38:21,713 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/mistral-nemo:12b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 11:38:22,340 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-15 11:38:22,341 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 11:38:22,341 - INFO - Processing deepseek-r1:1.5b\n",
      "2025-02-15 11:38:22,341 - INFO - Pulling Ollama model: deepseek-r1:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c766692c8e894271899467d5b4e47f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:1.5b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:42:45,902 - INFO - Cleaning up deepseek-r1:1.5b\n",
      "2025-02-15 11:42:45,903 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_shots \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      2\u001b[0m chat_prompt_template, riddles_for_eval \u001b[38;5;241m=\u001b[39m get_few_shot_chat_template(dataset\u001b[38;5;241m.\u001b[39msp, n_shots)\n\u001b[0;32m----> 4\u001b[0m sp_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m executor\u001b[38;5;241m.\u001b[39maexecute(\n\u001b[1;32m      5\u001b[0m     riddles_for_eval,\n\u001b[1;32m      6\u001b[0m     chat_prompt_template,\n\u001b[1;32m      7\u001b[0m     args_generator,\n\u001b[1;32m      8\u001b[0m     dump_to_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     create_checkpoints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     result_file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp_results_few_shot_n_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_shots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:231\u001b[0m, in \u001b[0;36mExecutor.aexecute\u001b[0;34m(self, dataset, prompt_template, args_generator, batch_size, dump_to_pickle, result_file_name, create_checkpoints)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batched_dataset:\n\u001b[1;32m    225\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aexecute_riddle(\n\u001b[1;32m    227\u001b[0m             model, riddle, prompt_template, args_generator\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m riddle \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[1;32m    230\u001b[0m     ]\n\u001b[0;32m--> 231\u001b[0m     chunk_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    232\u001b[0m     model_results\u001b[38;5;241m.\u001b[39mextend(chunk_results)\n\u001b[1;32m    233\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:97\u001b[0m, in \u001b[0;36mExecutor._aexecute_riddle\u001b[0;34m(self, model, riddle, prompt_template, args_generator)\u001b[0m\n\u001b[1;32m     95\u001b[0m template_args \u001b[38;5;241m=\u001b[39m args_generator(riddle)\n\u001b[1;32m     96\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 97\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39magenerate(prompt_template, template_args)\n\u001b[1;32m     98\u001b[0m delta \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ExecutionResult(\n\u001b[1;32m    101\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    102\u001b[0m     riddle\u001b[38;5;241m=\u001b[39mriddle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     execution_time\u001b[38;5;241m=\u001b[39mdelta,\n\u001b[1;32m    106\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/lmm.py:219\u001b[0m, in \u001b[0;36mOllamaModel.agenerate\u001b[0;34m(self, chat_template, args)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate text based on the provided chat template and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m messages \u001b[38;5;241m=\u001b[39m chat_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 219\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mainvoke(messages)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatHistory([messages, response])\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:305\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 305\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    306\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    307\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    308\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    309\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    310\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:870\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    864\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    869\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    871\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    872\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m AsyncCallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    777\u001b[0m     callbacks,\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    784\u001b[0m )\n\u001b[1;32m    786\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    788\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 796\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_with_cache(\n\u001b[1;32m    799\u001b[0m             m,\n\u001b[1;32m    800\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    801\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages)\n\u001b[1;32m    805\u001b[0m     ],\n\u001b[1;32m    806\u001b[0m     return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    807\u001b[0m )\n\u001b[1;32m    808\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:998\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 998\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    999\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1000\u001b[0m         )\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:788\u001b[0m, in \u001b[0;36mChatOllama._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_agenerate\u001b[39m(\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    783\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 788\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_achat_stream_with_aggregation(\n\u001b[1;32m    789\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    790\u001b[0m     )\n\u001b[1;32m    791\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    792\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    793\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    794\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    798\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    799\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:645\u001b[0m, in \u001b[0;36mChatOllama._achat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_achat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    638\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    647\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    648\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    649\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 ),\n\u001b[1;32m    663\u001b[0m             )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:575\u001b[0m, in \u001b[0;36mChatOllama._acreate_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params):\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/ollama/_client.py:674\u001b[0m, in \u001b[0;36mAsyncClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    671\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m    672\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39maiter_lines():\n\u001b[1;32m    675\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m    676\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m:=\u001b[39m part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:1031\u001b[0m, in \u001b[0;36mResponse.aiter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maiter_text():\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m   1033\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:1018\u001b[0m, in \u001b[0;36mResponse.aiter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m   1016\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maiter_bytes():\n\u001b[1;32m   1019\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:997\u001b[0m, in \u001b[0;36mResponse.aiter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maiter_raw():\n\u001b[1;32m    998\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_models.py:1055\u001b[0m, in \u001b[0;36mResponse.aiter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m   1052\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:176\u001b[0m, in \u001b[0;36mBoundAsyncStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:271\u001b[0m, in \u001b[0;36mAsyncResponseStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__aiter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:203\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1246\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[1;32m   1244\u001b[0m ):\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/locks.py:213\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_shots = 4\n",
    "chat_prompt_template, riddles_for_eval = get_few_shot_chat_template(dataset.sp, n_shots)\n",
    "\n",
    "sp_results = await executor.aexecute(\n",
    "    riddles_for_eval,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=f\"sp_results_few_shot_n_{n_shots}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:16:37,538 - INFO - Starting execution\n",
      "2025-02-13 18:16:37,538 - INFO - Processing llama3.1:8b\n",
      "2025-02-13 18:16:37,539 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5561c6537fc489eb237b6c6471839bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:18:15,731 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.1:8b_wp_results_naive.pkl\n",
      "2025-02-13 18:18:15,922 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-13 18:18:15,923 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:18:15,923 - INFO - Processing llama3.2:1b\n",
      "2025-02-13 18:18:15,923 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f3acfc37074993b303f004823451ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:19:55,016 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.2:1b_wp_results_naive.pkl\n",
      "2025-02-13 18:19:55,127 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-13 18:19:55,128 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:19:55,128 - INFO - Processing llama3.2:3b\n",
      "2025-02-13 18:19:55,128 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f318118ba4d86a170df5d1e68497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:21:48,487 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/llama3.2:3b_wp_results_naive.pkl\n",
      "2025-02-13 18:21:48,666 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-13 18:21:48,666 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:21:48,667 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-13 18:21:48,667 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116322335fb45b381ce039bad497dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:36:22,407 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/phi3.5:3.8b_wp_results_naive.pkl\n",
      "2025-02-13 18:36:22,516 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-13 18:36:22,516 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:36:22,517 - INFO - Processing phi4:14b\n",
      "2025-02-13 18:36:22,517 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c33aa9ca594b78bafca55045b48a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:51:51,261 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/phi4:14b_wp_results_naive.pkl\n",
      "2025-02-13 18:51:51,364 - INFO - Cleaning up phi4:14b\n",
      "2025-02-13 18:51:51,365 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:51:51,365 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-13 18:51:51,365 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfca8a93e904626b9a6fc4dee1dbd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:54:12,117 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:0.5b_wp_results_naive.pkl\n",
      "2025-02-13 18:54:12,227 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-13 18:54:12,228 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:54:12,228 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-13 18:54:12,228 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b933f969ed9463f878e161f92f254f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:56:07,012 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:1.5b_wp_results_naive.pkl\n",
      "2025-02-13 18:56:07,211 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-13 18:56:07,211 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:56:07,212 - INFO - Processing qwen2.5:3b\n",
      "2025-02-13 18:56:07,212 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb9aed2b896474f8ffd7f1a2b1b110e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:58:28,646 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:3b_wp_results_naive.pkl\n",
      "2025-02-13 18:58:28,757 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-13 18:58:28,758 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 18:58:28,759 - INFO - Processing qwen2.5:7b\n",
      "2025-02-13 18:58:28,759 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aef65000d0e4023aca48835871b560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:02:40,346 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:7b_wp_results_naive.pkl\n",
      "2025-02-13 19:02:40,451 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-13 19:02:40,452 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:02:40,452 - INFO - Processing qwen2.5:14b\n",
      "2025-02-13 19:02:40,452 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2cf7c85f6847c5a01d7b762a978fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:10:21,899 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:14b_wp_results_naive.pkl\n",
      "2025-02-13 19:10:22,006 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-13 19:10:22,006 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:10:22,007 - INFO - Processing qwen2.5:32b\n",
      "2025-02-13 19:10:22,007 - INFO - Pulling Ollama model: qwen2.5:32b\n",
      "2025-02-13 19:11:58,642 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-eabc98a9bcbfce7fd70f3e07de599f8fda98120fefed5881934161ede8bd1a41-partial: no space left on device (status code: 500)\n",
      "2025-02-13 19:11:58,643 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c61f30df84a648ae3bed17fcd1cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:28:58,750 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/qwen2.5:32b_wp_results_naive.pkl\n",
      "2025-02-13 19:28:58,859 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-13 19:28:58,859 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:28:58,860 - INFO - Processing gemma2:2b\n",
      "2025-02-13 19:28:58,860 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee8ced99504835a025f176d2b8dd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:31:16,516 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:2b_wp_results_naive.pkl\n",
      "2025-02-13 19:31:16,739 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-13 19:31:16,739 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:31:16,739 - INFO - Processing gemma2:9b\n",
      "2025-02-13 19:31:16,740 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a890a798a4569baf5678f47e91837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:35:37,128 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:9b_wp_results_naive.pkl\n",
      "2025-02-13 19:35:37,231 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-13 19:35:37,231 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:35:37,231 - INFO - Processing gemma2:27b\n",
      "2025-02-13 19:35:37,231 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d766a8e05efa4c85bb5049afa1c9d7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:43:17,563 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/gemma2:27b_wp_results_naive.pkl\n",
      "2025-02-13 19:43:17,664 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-13 19:43:17,665 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:43:17,665 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-13 19:43:17,665 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cdaea39214499bbe522eec6054ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:50:35,816 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive/mistral-nemo:12b_wp_results_naive.pkl\n",
      "2025-02-13 19:50:36,042 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-13 19:50:36,042 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-13 19:50:36,043 - INFO - Execution complete\n",
      "2025-02-13 19:50:36,043 - INFO - Dumping results to results/wp_results_naive.pkl\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_template, riddles_for_eval = get_few_shot_chat_template(dataset.wp, n_shots)\n",
    "\n",
    "wp_results = await executor.aexecute(\n",
    "    riddles_for_eval,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=f\"wp_results_few_shot_n_{n_shots}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
