{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are a helpful assistant.\",\n",
    "    \"step_by_step\": \"You are a meticulous problem-solver.\",\n",
    "    \"creative\": \"You excel at lateral thinking. Treat this as a riddle.\",\n",
    "    \"elimination\": \"Eliminate wrong options internally.\",\n",
    "    \"metaphor\": \"Interpret keywords metaphorically.\",\n",
    "    \"confidence\": \"Score options internally.\",\n",
    "    \"perspective_shift\": \"Analyze through multiple perspectives silently.\",\n",
    "    \"common_sense\": \"Combine logic and creativity.\",\n",
    "    \"assumption_challenge\": \"Challenge hidden assumptions internally.\",\n",
    "    \"pattern_matching\": \"Find patterns silently.\",\n",
    "    \"intuitive\": \"Critique your intuition internally.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt_template(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_humand_prompt_template():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "        \"answer\": string.ascii_uppercase[riddle_question.label],\n",
    "    }\n",
    "\n",
    "    return template_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the few shot exampel but following the best practices from https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n",
    "# Thus we do not provide the examples in the initial client prompt but as a message history of the user asking and the system answering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "def get_few_shot_chat_template(\n",
    "    dataset: list[RiddleQuestion],\n",
    "    number_of_shots: int = 4,\n",
    "    system_template: str = \"default\",\n",
    "):\n",
    "    riddles_as_examples = dataset[:number_of_shots]\n",
    "    riddles_to_solve = dataset[number_of_shots:]\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_humand_prompt_template(),\n",
    "            (\"ai\", \"{answer}\"),\n",
    "        ]\n",
    "    )\n",
    "    few_shot_prompt_naive = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=[args_generator(example) for example in riddles_as_examples],\n",
    "    )\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_system_prompt_template(system_template),\n",
    "            few_shot_prompt_naive,\n",
    "            get_humand_prompt_template(),\n",
    "        ]\n",
    "    )\n",
    "    return (chat_prompt_template, riddles_to_solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "# riddles = dataset.sp\n",
    "# examples_full = [args_generator(riddle) for riddle in riddles]\n",
    "# example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "#     examples,\n",
    "#     OllamaEmbeddings(\n",
    "#         model=\"bge-m3\"\n",
    "#     ),  # bge-m3 excel at handling context-rich queries due to higher embedding dimensions, as fallback we could also use models like  nomic-embed-text for short semantic queries\n",
    "#     Chroma,\n",
    "#     k=4,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:27:19,791 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        OllamaModel(\"llama3.2:1b\"),\n",
    "        OllamaModel(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        OllamaModel(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        OllamaModel(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(\"qwen2.5:0.5b\"),\n",
    "        OllamaModel(\"qwen2.5:1.5b\"),\n",
    "        OllamaModel(\"qwen2.5:3b\"),\n",
    "        OllamaModel(\"qwen2.5:7b\"),\n",
    "        OllamaModel(\"qwen2.5:14b\"),\n",
    "        OllamaModel(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        OllamaModel(\"gemma2:2b\"),\n",
    "        OllamaModel(\"gemma2:9b\"),\n",
    "        OllamaModel(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:27:19,805 - INFO - Starting asynchronous execution\n",
      "2025-02-15 17:27:19,805 - INFO - Split dataset of 623 items into 125 batches of size 5\n",
      "2025-02-15 17:27:19,806 - INFO - Processing llama3.1:8b\n",
      "2025-02-15 17:27:19,806 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c23e7ba3c4242a40010ca3fca224c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:28:24,091 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.1:8b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:28:24,549 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-15 17:28:24,550 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:28:24,551 - INFO - Processing llama3.2:1b\n",
      "2025-02-15 17:28:24,551 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbda2ed431d41d89e6035e0cd07a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:29:03,619 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.2:1b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:29:04,043 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-15 17:29:04,044 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:29:04,044 - INFO - Processing llama3.2:3b\n",
      "2025-02-15 17:29:04,044 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f302d2dfe484e3ea2218ed653bbeda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:29:46,495 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/llama3.2:3b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:29:46,901 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-15 17:29:46,901 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:29:46,902 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-15 17:29:46,902 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9453057a267b40d09b2e21c2aa7481ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:37:25,461 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/phi3.5:3.8b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:37:25,923 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-15 17:37:25,924 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:37:25,924 - INFO - Processing phi4:14b\n",
      "2025-02-15 17:37:25,925 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418fa61daf704eeabbe23cd955977094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:39:24,300 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/phi4:14b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:39:24,798 - INFO - Cleaning up phi4:14b\n",
      "2025-02-15 17:39:24,799 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:39:24,799 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-15 17:39:24,800 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca2842fe40d451084854bac1c97c361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:41:21,925 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:0.5b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:41:22,442 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-15 17:41:22,443 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:41:22,443 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-15 17:41:22,443 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287e67e40b7a4d7381c03d3bcb7d685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:43:21,561 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:1.5b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:43:22,063 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-15 17:43:22,064 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:43:22,064 - INFO - Processing qwen2.5:3b\n",
      "2025-02-15 17:43:22,064 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1989b6ceb34ca8b1a4b7b748f2affc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:45:25,099 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:3b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:45:25,632 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-15 17:45:25,633 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:45:25,633 - INFO - Processing qwen2.5:7b\n",
      "2025-02-15 17:45:25,633 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7c6cb2b854e0a991c267ff8807066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:47:50,684 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:7b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:47:51,010 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-15 17:47:51,010 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:47:51,010 - INFO - Processing qwen2.5:14b\n",
      "2025-02-15 17:47:51,010 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c988cbb8484a438d9f2221777ae05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:50:36,282 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:14b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:50:36,582 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-15 17:50:36,583 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:50:36,583 - INFO - Processing qwen2.5:32b\n",
      "2025-02-15 17:50:36,583 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb260335a6848f8b8549942fff009a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:54:27,828 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/qwen2.5:32b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:54:28,330 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-15 17:54:28,331 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:54:28,331 - INFO - Processing gemma2:2b\n",
      "2025-02-15 17:54:28,331 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdab0058e7be41cfa2a71f4fda4ef6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:55:26,938 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:2b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:55:27,461 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-15 17:55:27,461 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:55:27,461 - INFO - Processing gemma2:9b\n",
      "2025-02-15 17:55:27,462 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044f803b45c14a96a2a0d3efc81073e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:57:59,104 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:9b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 17:57:59,435 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-15 17:57:59,435 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 17:57:59,435 - INFO - Processing gemma2:27b\n",
      "2025-02-15 17:57:59,435 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbefac814e34b788435e8dd30df6e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:01:00,459 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/gemma2:27b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:01:00,751 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-15 18:01:00,751 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:01:00,752 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-15 18:01:00,752 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d6cf8da78b49668ce8c34a40a4b8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:02:49,594 - INFO - Creating checkpoint: results/checkpoints/sp_results_few_shot_n_4/mistral-nemo:12b_sp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:02:50,218 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-15 18:02:50,219 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:02:50,219 - INFO - Asynchronous execution complete\n",
      "2025-02-15 18:02:50,219 - INFO - Dumping results to results/sp_results_few_shot_n_4.pkl\n"
     ]
    }
   ],
   "source": [
    "n_shots = 4\n",
    "chat_prompt_template, riddles_for_eval = get_few_shot_chat_template(dataset.sp, n_shots)\n",
    "\n",
    "sp_results = await executor.aexecute(\n",
    "    riddles_for_eval,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=f\"sp_results_few_shot_n_{n_shots}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:02:56,198 - INFO - Starting asynchronous execution\n",
      "2025-02-15 18:02:56,198 - INFO - Split dataset of 488 items into 98 batches of size 5\n",
      "2025-02-15 18:02:56,199 - INFO - Processing llama3.1:8b\n",
      "2025-02-15 18:02:56,199 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa95811a60b435a87fe4c2dcac33668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:03:25,118 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/llama3.1:8b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:03:25,335 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-15 18:03:25,336 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:03:25,336 - INFO - Processing llama3.2:1b\n",
      "2025-02-15 18:03:25,336 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae176d3edd746d2930478f0a5b53e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:03:45,042 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/llama3.2:1b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:03:45,270 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-15 18:03:45,270 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:03:45,270 - INFO - Processing llama3.2:3b\n",
      "2025-02-15 18:03:45,271 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb5a449a27b4f90b46fcf9025fd6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:04:09,508 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/llama3.2:3b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:04:09,728 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-15 18:04:09,729 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:04:09,729 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-15 18:04:09,729 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3f896775c424984e55157bf699292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:11:26,072 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/phi3.5:3.8b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:11:26,302 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-15 18:11:26,303 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:11:26,303 - INFO - Processing phi4:14b\n",
      "2025-02-15 18:11:26,303 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8add1a00a6af41559d2f191506f52e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:12:07,615 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/phi4:14b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:12:07,847 - INFO - Cleaning up phi4:14b\n",
      "2025-02-15 18:12:07,847 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:12:07,848 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-15 18:12:07,848 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab9a79d3e0e4c909b0d35d8789ec261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:13:27,959 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:0.5b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:13:28,177 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-15 18:13:28,178 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:13:28,178 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-15 18:13:28,179 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3c87ee61046979eaefbbad6041a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:14:50,045 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:1.5b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:14:50,670 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-15 18:14:50,670 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:14:50,670 - INFO - Processing qwen2.5:3b\n",
      "2025-02-15 18:14:50,671 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4258f98a154489a9df6802436c7a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:16:13,479 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:3b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:16:13,712 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-15 18:16:13,713 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:16:13,713 - INFO - Processing qwen2.5:7b\n",
      "2025-02-15 18:16:13,713 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d580a7b8744e749b0b3e2f507a2d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:17:38,519 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:7b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:17:38,749 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-15 18:17:38,750 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:17:38,750 - INFO - Processing qwen2.5:14b\n",
      "2025-02-15 18:17:38,750 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1af6b5bb40482684a7a6f324b0641d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:19:11,796 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:14b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:19:12,038 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-15 18:19:12,038 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:19:12,038 - INFO - Processing qwen2.5:32b\n",
      "2025-02-15 18:19:12,039 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7d79b148d149efbf2ece89e3b51a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:21:16,603 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/qwen2.5:32b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:21:16,830 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-15 18:21:16,831 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:21:16,831 - INFO - Processing gemma2:2b\n",
      "2025-02-15 18:21:16,831 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db3ec43180e4de08c8c37a643e8f8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:21:47,155 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/gemma2:2b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:21:47,880 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-15 18:21:47,881 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:21:47,881 - INFO - Processing gemma2:9b\n",
      "2025-02-15 18:21:47,881 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f54d206ce4369ab3df6de2b6c3ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:22:59,335 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/gemma2:9b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:22:59,557 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-15 18:22:59,558 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:22:59,558 - INFO - Processing gemma2:27b\n",
      "2025-02-15 18:22:59,558 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc67c48695645deb9b3b4d0e893e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:24:11,364 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/gemma2:27b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:24:11,591 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-15 18:24:11,591 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:24:11,592 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-15 18:24:11,592 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45c2e6f8a874424a6301edfb16756b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 18:25:02,481 - INFO - Creating checkpoint: results/checkpoints/wp_results_few_shot_n_4/mistral-nemo:12b_wp_results_few_shot_n_4.pkl\n",
      "2025-02-15 18:25:02,712 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-15 18:25:02,713 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 18:25:02,713 - INFO - Asynchronous execution complete\n",
      "2025-02-15 18:25:02,713 - INFO - Dumping results to results/wp_results_few_shot_n_4.pkl\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_template, riddles_for_eval = get_few_shot_chat_template(dataset.wp, n_shots)\n",
    "\n",
    "wp_results = await executor.aexecute(\n",
    "    riddles_for_eval,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=f\"wp_results_few_shot_n_{n_shots}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
