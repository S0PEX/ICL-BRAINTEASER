{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "from scripts.prompt_helpers import create_prompt_template\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,299 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://107.222.215.224:17001\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b\"),\n",
    "        model_builder.build_model(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:1.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:3b\"),\n",
    "        model_builder.build_model(\"qwen2.5:7b\"),\n",
    "        model_builder.build_model(\"qwen2.5:14b\"),\n",
    "        model_builder.build_model(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b\"),\n",
    "        model_builder.build_model(\"gemma2:9b\"),\n",
    "        model_builder.build_model(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scripts.executor import Dataset\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def create_test_dataset(data: list[RiddleQuestion], name: str, percentage: float = 0.1):\n",
    "    \"\"\"Create a test dataset by randomly sampling a percentage of the original data.\"\"\"\n",
    "    indices = np.random.choice(\n",
    "        len(data), size=int(len(data) * percentage), replace=False\n",
    "    )\n",
    "    return Dataset(name=name, riddles=[data[i] for i in indices])\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "sp_data = create_test_dataset(dataset.sp, \"sp\")\n",
    "wp_data = create_test_dataset(dataset.wp, \"wp\")\n",
    "\n",
    "# Prepare executor data\n",
    "executor_data = [sp_data, wp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,317 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'default'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008185bf45b747c5b8539cbaa5748b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(default):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,435 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'default_improved'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e5c0e5f4d44dc8b8b4873244ba8b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(default-improved):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,559 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'step_by_step'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67c0ad10a6f49b5b936459d4f34ff7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(step-by-step):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,585 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'creative'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bdf405a9714d1281b684f20b9ec522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(creative):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,707 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'elimination'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b199df9cfb544e68a2e6324d61804cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(elimination):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,746 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'metaphor'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98693c6fda944518ae2b01a63efb323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(metaphor):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,873 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'confidence'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d018498230d44bd9047b40f8d581293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(confidence):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:10,998 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'perspective_shift'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8f29fa6e37476792cef2a32812ebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(perspective-shift):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:11,028 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'common_sense'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c883f6a2e34a4e8886a376d8e42cf750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(common-sense):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:11,158 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'assumption_challenge'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365cdc48c9ed40c38dab4f33da595fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(assumption-challenge):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:46:11,291 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'pattern_matching'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfedf669c13440b1a5d433c27e516966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(pattern-matching):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 12:04:34,237 - INFO - Saving results to results/zero-shot-system-prompt/zero-shot-system-prompt_pattern-matching_results.pkl\n",
      "2025-02-27 12:04:34,681 - INFO - Starting execution 'zero-shot-system-prompt with suffix 'intuitive'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e5997133794c5eb90d2b722a6a17cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot-system-prompt(intuitive):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 12:31:31,580 - INFO - Saving results to results/zero-shot-system-prompt/zero-shot-system-prompt_intuitive_results.pkl\n"
     ]
    }
   ],
   "source": [
    "from scripts.prompt_helpers import system_templates\n",
    "\n",
    "total_results = {}\n",
    "\n",
    "for technique in system_templates:\n",
    "    chat_prompt_template = create_prompt_template(technique)\n",
    "    results = await executor.aexecute(\n",
    "        executor_data,\n",
    "        chat_prompt_template,\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        run_name=\"zero_shot_system_prompt\",\n",
    "        file_name_suffix=technique,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 result sets from disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the results directory path\n",
    "results_dir = Path(\"results/zero-shot-system-prompt\")\n",
    "\n",
    "# Get all result files\n",
    "result_files = glob.glob(str(results_dir / \"zero-shot-system-prompt_*_results.pkl\"))\n",
    "\n",
    "# Load all results into a dictionary\n",
    "# The first key is the suffix (technique name)\n",
    "total_results = {}\n",
    "\n",
    "for file_path in result_files:\n",
    "    # Extract the suffix from the filename\n",
    "    suffix = os.path.basename(file_path).split(\"_\")[1]\n",
    "\n",
    "    # Load the results from the pickle file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        wrapped_results = pickle.load(f)\n",
    "        total_results[suffix] = wrapped_results.results\n",
    "\n",
    "print(f\"Loaded {len(total_results)} result sets from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama3.1:8b': {'sp': {'prompt_type': 'confidence', 'score': 59.67741935483871}, 'wp': {'prompt_type': 'default-improved', 'score': 57.14285714285714}}, 'llama3.2:1b': {'sp': {'prompt_type': 'step-by-step', 'score': 19.35483870967742}, 'wp': {'prompt_type': 'step-by-step', 'score': 8.16326530612245}}, 'llama3.2:3b': {'sp': {'prompt_type': 'default-improved', 'score': 41.935483870967744}, 'wp': {'prompt_type': 'perspective-shift', 'score': 36.734693877551024}}, 'phi3.5:3.8b': {'sp': {'prompt_type': 'elimination', 'score': 25.806451612903224}, 'wp': {'prompt_type': 'creative', 'score': 46.93877551020408}}, 'phi4:14b': {'sp': {'prompt_type': 'default-improved', 'score': 77.41935483870968}, 'wp': {'prompt_type': 'default-improved', 'score': 67.3469387755102}}, 'qwen2.5:0.5b': {'sp': {'prompt_type': 'elimination', 'score': 29.03225806451613}, 'wp': {'prompt_type': 'common-sense', 'score': 34.69387755102041}}, 'qwen2.5:1.5b': {'sp': {'prompt_type': 'metaphor', 'score': 29.03225806451613}, 'wp': {'prompt_type': 'common-sense', 'score': 10.204081632653061}}, 'qwen2.5:3b': {'sp': {'prompt_type': 'step-by-step', 'score': 27.419354838709676}, 'wp': {'prompt_type': 'step-by-step', 'score': 26.53061224489796}}, 'qwen2.5:7b': {'sp': {'prompt_type': 'default-improved', 'score': 70.96774193548387}, 'wp': {'prompt_type': 'elimination', 'score': 14.285714285714285}}, 'qwen2.5:14b': {'sp': {'prompt_type': 'confidence', 'score': 70.96774193548387}, 'wp': {'prompt_type': 'creative', 'score': 57.14285714285714}}, 'qwen2.5:32b': {'sp': {'prompt_type': 'metaphor', 'score': 74.19354838709677}, 'wp': {'prompt_type': 'metaphor', 'score': 61.224489795918366}}, 'gemma2:2b': {'sp': {'prompt_type': 'elimination', 'score': 50.0}, 'wp': {'prompt_type': 'creative', 'score': 30.612244897959183}}, 'gemma2:9b': {'sp': {'prompt_type': 'metaphor', 'score': 83.87096774193549}, 'wp': {'prompt_type': 'creative', 'score': 73.46938775510205}}, 'gemma2:27b': {'sp': {'prompt_type': 'perspective-shift', 'score': 90.32258064516128}, 'wp': {'prompt_type': 'confidence', 'score': 79.59183673469387}}, 'mistral-nemo:12b': {'sp': {'prompt_type': 'default-improved', 'score': 61.29032258064516}, 'wp': {'prompt_type': 'step-by-step', 'score': 38.775510204081634}}}\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scripts.evaluation import eval_model_results\n",
    "\n",
    "\n",
    "def get_best_prompt_for_each_model(input_data):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            # For each model, we need to track its best score\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Assume eval_results returns a score based on the result data\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # If this model doesn't have a best score for this dataset yet or if the current score is better\n",
    "                if (\n",
    "                    dataset_type not in best_prompts[model]\n",
    "                    or score > best_prompts[model][dataset_type][\"score\"]\n",
    "                ):\n",
    "                    best_prompts[model][dataset_type] = {\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "\n",
    "    # Now best_prompts contains the best prompt type for each model and dataset\n",
    "    return best_prompts\n",
    "\n",
    "\n",
    "def get_best_n_prompts_for_each_model(input_data, n=3):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Calculate the score for the model with the current prompt type and dataset\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # Initialize the list of prompts for this model and dataset type if not created\n",
    "                if dataset_type not in best_prompts[model]:\n",
    "                    best_prompts[model][dataset_type] = []\n",
    "\n",
    "                # Append the prompt type and score to the list\n",
    "                best_prompts[model][dataset_type].append(\n",
    "                    {\"prompt_type\": prompt_type, \"score\": score}\n",
    "                )\n",
    "\n",
    "    # Now sort the list of prompts for each model and dataset type and keep the top n\n",
    "    top_n_prompts = {}\n",
    "    for model, dataset_dict in best_prompts.items():\n",
    "        top_n_prompts_for_model = {}\n",
    "        for dataset_type, prompts in dataset_dict.items():\n",
    "            # Get the top n prompts by sorting the list based on score (highest score first)\n",
    "            sorted_prompts = heapq.nlargest(n, prompts, key=lambda x: x[\"score\"])\n",
    "            top_n_prompts_for_model[dataset_type] = sorted_prompts\n",
    "        top_n_prompts[model] = top_n_prompts_for_model\n",
    "\n",
    "    return top_n_prompts\n",
    "\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "best_prompt_types = get_best_prompt_for_each_model(total_results)\n",
    "print(best_prompt_types)\n",
    "\n",
    "# Save the best prompt types\n",
    "with open(\"results/best_system_prompts_by_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_prompt_types, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
