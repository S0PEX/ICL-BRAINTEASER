{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "from scripts.prompt_helpers import create_prompt_template\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 17:00:53,526 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://142.214.185.26:40001\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b-instruct-q8_0\"),  # => 9 GB\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b-instruct-fp16\"),  # => 2.5 GB\n",
    "        model_builder.build_model(\"llama3.2:3b-instruct-fp16\"),  # => 6.4 GB\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b-mini-instruct-fp16\"),  # => 7.6 GB\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b-q8_0\"),  # => 16 GB\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b-instruct-fp16\"),  # => 1 GB\n",
    "        model_builder.build_model(\"qwen2.5:1.5b-instruct-fp16\"),  # => 3.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:3b-instruct-fp16\"),  # => 6.2 GB\n",
    "        model_builder.build_model(\"qwen2.5:7b-instruct-q8_0\"),  # => 8.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:14b-instruct-q8_0\"),  # => 16 GB\n",
    "        model_builder.build_model(\"qwen2.5:32b-instruct-q4_K_M\"),  # => 20 GB\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b-instruct-fp16\"),  # => 5.2 GB\n",
    "        model_builder.build_model(\"gemma2:9b-instruct-q8_0\"),  # => 9.8 GB\n",
    "        model_builder.build_model(\"gemma2:27b-instruct-q4_K_M\"),  # => 22 GB\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b-instruct-2407-q8_0\"),  # => 13 GB\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scripts.executor import Dataset\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def create_test_dataset(data: list[RiddleQuestion], name: str, percentage: float = 0.1):\n",
    "    \"\"\"Create a test dataset by randomly sampling a percentage of the original data.\"\"\"\n",
    "    indices = np.random.choice(\n",
    "        len(data), size=int(len(data) * percentage), replace=False\n",
    "    )\n",
    "    return Dataset(name=name, riddles=[data[i] for i in indices])\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "sp_data = create_test_dataset(dataset.sp, \"sp\")\n",
    "wp_data = create_test_dataset(dataset.wp, \"wp\")\n",
    "\n",
    "# Prepare executor data\n",
    "executor_data = [sp_data, wp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "with open(\"results/best_system_prompts_by_model.pkl\", \"rb\") as f:\n",
    "    best_prompt_types = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_prompt_template_by_model(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "):\n",
    "    best_system_template_name = best_prompt_types[model_name][dataset_name][\n",
    "        \"prompt_type\"\n",
    "    ]\n",
    "    return create_prompt_template(best_system_template_name)\n",
    "\n",
    "\n",
    "def get_prompt_template(model_name: str, dataset: Dataset):\n",
    "    # Split name after the b paramer, e.g., llama3.1:8b-instruct-fp16 => llama3.1:8b\n",
    "    model_name = model_name[0 : model_name.index(\"b-\") + 1]\n",
    "    chat_prompt_template = create_prompt_template_by_model(model_name, dataset.name)\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 17:00:53,576 - INFO - Starting execution 'model-speed with suffix 'run_0'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6044e768887f47f0822e864c2202dd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-speed(run-0):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 17:37:19,199 - INFO - Saving results to results/model-speed/model-speed_run-0_results.pkl\n",
      "2025-03-14 17:37:19,696 - INFO - Execution 'model-speed with suffix 'run_0'' completed successfully.\n",
      "2025-03-14 17:37:19,697 - INFO - Starting execution 'model-speed with suffix 'run_1'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116f06360f1c4e949411fff8b1dabba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-speed(run-1):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 18:00:47,564 - INFO - Saving results to results/model-speed/model-speed_run-1_results.pkl\n",
      "2025-03-14 18:00:48,069 - INFO - Execution 'model-speed with suffix 'run_1'' completed successfully.\n",
      "2025-03-14 18:00:48,070 - INFO - Starting execution 'model-speed with suffix 'run_2'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753eadfc2c8d4de4bc3f1ff0cbc7867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-speed(run-2):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 18:25:06,510 - INFO - Saving results to results/model-speed/model-speed_run-2_results.pkl\n",
      "2025-03-14 18:25:06,980 - INFO - Execution 'model-speed with suffix 'run_2'' completed successfully.\n",
      "2025-03-14 18:25:06,981 - INFO - Starting execution 'model-speed with suffix 'run_3'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0dff0a08d8404f9ba5293420570403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-speed(run-3):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 18:47:15,357 - INFO - Saving results to results/model-speed/model-speed_run-3_results.pkl\n",
      "2025-03-14 18:47:15,847 - INFO - Execution 'model-speed with suffix 'run_3'' completed successfully.\n",
      "2025-03-14 18:47:15,847 - INFO - Starting execution 'model-speed with suffix 'run_4'': 2 dataset(s) x 15 model(s) = 1665 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd329f8eb17e46919945f450cf8a5f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-speed(run-4):   0%|          | 0/1665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 19:11:45,852 - INFO - Saving results to results/model-speed/model-speed_run-4_results.pkl\n",
      "2025-03-14 19:11:46,344 - INFO - Execution 'model-speed with suffix 'run_4'' completed successfully.\n"
     ]
    }
   ],
   "source": [
    "runs = []\n",
    "for run_index in range(5):\n",
    "    results, time_per_model = await executor.aexecute(\n",
    "        executor_data,\n",
    "        get_prompt_template,\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        run_name=\"model_speed\",\n",
    "        file_name_suffix=f\"run_{run_index}\",\n",
    "    )\n",
    "    runs.append((results, time_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
