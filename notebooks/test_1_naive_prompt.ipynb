{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are a helpful assistant.\",\n",
    "    \"step_by_step\": \"You are a meticulous problem-solver.\",\n",
    "    \"creative\": \"You excel at lateral thinking. Treat this as a riddle.\",\n",
    "    \"elimination\": \"Eliminate wrong options internally.\",\n",
    "    \"metaphor\": \"Interpret keywords metaphorically.\",\n",
    "    \"confidence\": \"Score options internally.\",\n",
    "    \"perspective_shift\": \"Analyze through multiple perspectives silently.\",\n",
    "    \"common_sense\": \"Combine logic and creativity.\",\n",
    "    \"assumption_challenge\": \"Challenge hidden assumptions internally.\",\n",
    "    \"pattern_matching\": \"Find patterns silently.\",\n",
    "    \"intuitive\": \"Critique your intuition internally.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_prompt_template(\n",
    "    system_prompt_template_name: str = \"default\",\n",
    "):\n",
    "    system_prompt_template = get_system_prompt(system_prompt_template_name)\n",
    "    user_prompt_template = get_user_prompt()\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [system_prompt_template, user_prompt_template]\n",
    "    )\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:59:38,110 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        OllamaModel(\"llama3.2:1b\"),\n",
    "        OllamaModel(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        OllamaModel(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        OllamaModel(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(\"qwen2.5:0.5b\"),\n",
    "        OllamaModel(\"qwen2.5:1.5b\"),\n",
    "        OllamaModel(\"qwen2.5:3b\"),\n",
    "        OllamaModel(\"qwen2.5:7b\"),\n",
    "        OllamaModel(\"qwen2.5:14b\"),\n",
    "        OllamaModel(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        OllamaModel(\"gemma2:2b\"),\n",
    "        OllamaModel(\"gemma2:9b\"),\n",
    "        OllamaModel(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:59:38,127 - INFO - Starting asynchronous execution\n",
      "2025-02-15 11:59:38,127 - INFO - Split dataset of 627 items into 126 batches of size 5\n",
      "2025-02-15 11:59:38,128 - INFO - Processing llama3.1:8b\n",
      "2025-02-15 11:59:38,128 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ca3e95f75549fd9b3378e70aaa2905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:00:22,964 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/llama3.1:8b_sp_results_naive.pkl\n",
      "2025-02-15 12:00:23,086 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-15 12:00:23,087 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:00:23,087 - INFO - Processing llama3.2:1b\n",
      "2025-02-15 12:00:23,087 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1df935af814574962429ab90da84b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:00:48,034 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/llama3.2:1b_sp_results_naive.pkl\n",
      "2025-02-15 12:00:48,162 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-15 12:00:48,163 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:00:48,163 - INFO - Processing llama3.2:3b\n",
      "2025-02-15 12:00:48,163 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e40075a95fe4277810914dd24431554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:01:25,428 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/llama3.2:3b_sp_results_naive.pkl\n",
      "2025-02-15 12:01:25,636 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-15 12:01:25,636 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:01:25,636 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-15 12:01:25,636 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4c1ed14d2a41ab9d963e770cd0751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:10:17,950 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/phi3.5:3.8b_sp_results_naive.pkl\n",
      "2025-02-15 12:10:18,173 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-15 12:10:18,174 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:10:18,174 - INFO - Processing phi4:14b\n",
      "2025-02-15 12:10:18,174 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3cee066ca245aa81155ab81be40ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:17:45,457 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/phi4:14b_sp_results_naive.pkl\n",
      "2025-02-15 12:17:45,587 - INFO - Cleaning up phi4:14b\n",
      "2025-02-15 12:17:45,588 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:17:45,588 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-15 12:17:45,589 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc579829779f438fa6ef73183a952dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:19:11,344 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/qwen2.5:0.5b_sp_results_naive.pkl\n",
      "2025-02-15 12:19:11,472 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-15 12:19:11,472 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:19:11,473 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-15 12:19:11,473 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4630a45c04c241a99fe3a61c8dcef05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:19:58,831 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/qwen2.5:1.5b_sp_results_naive.pkl\n",
      "2025-02-15 12:19:58,955 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-15 12:19:58,955 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:19:58,956 - INFO - Processing qwen2.5:3b\n",
      "2025-02-15 12:19:58,956 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd92cb1c8ed341b29d2094998fb1c84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:20:38,336 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/qwen2.5:3b_sp_results_naive.pkl\n",
      "2025-02-15 12:20:38,467 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-15 12:20:38,467 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:20:38,468 - INFO - Processing qwen2.5:7b\n",
      "2025-02-15 12:20:38,468 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca940033cea484985f8fb55985a64e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:21:51,863 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/qwen2.5:7b_sp_results_naive.pkl\n",
      "2025-02-15 12:21:52,118 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-15 12:21:52,118 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:21:52,118 - INFO - Processing qwen2.5:14b\n",
      "2025-02-15 12:21:52,118 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5050da8c95c8413781045c4c504174e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:25:36,433 - INFO - Creating checkpoint: results/checkpoints/sp_results_naive/qwen2.5:14b_sp_results_naive.pkl\n",
      "2025-02-15 12:25:36,566 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-15 12:25:36,566 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-15 12:25:36,566 - INFO - Processing qwen2.5:32b\n",
      "2025-02-15 12:25:36,567 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52181b27598a4c1bba897b6da1039de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:28:42,094 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-15 12:28:42,095 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n"
     ]
    },
    {
     "ename": "RemoteProtocolError",
     "evalue": "Server disconnected without sending a response.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:136\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:106\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:177\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:231\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    230\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer disconnected without sending a response.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mreceive_data(data)\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sp_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m executor\u001b[38;5;241m.\u001b[39maexecute(\n\u001b[1;32m      2\u001b[0m     dataset\u001b[38;5;241m.\u001b[39msp,\n\u001b[1;32m      3\u001b[0m     chat_prompt_template,\n\u001b[1;32m      4\u001b[0m     args_generator,\n\u001b[1;32m      5\u001b[0m     dump_to_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     create_checkpoints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     result_file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp_results_naive\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:223\u001b[0m, in \u001b[0;36mExecutor.aexecute\u001b[0;34m(self, dataset, prompt_template, args_generator, batch_size, dump_to_pickle, result_file_name, create_checkpoints)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batched_dataset:\n\u001b[1;32m    217\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aexecute_riddle(\n\u001b[1;32m    219\u001b[0m             model, riddle, prompt_template, args_generator\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m riddle \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[1;32m    222\u001b[0m     ]\n\u001b[0;32m--> 223\u001b[0m     chunk_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    224\u001b[0m     model_results\u001b[38;5;241m.\u001b[39mextend(chunk_results)\n\u001b[1;32m    225\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/executor.py:90\u001b[0m, in \u001b[0;36mExecutor._aexecute_riddle\u001b[0;34m(self, model, riddle, prompt_template, args_generator)\u001b[0m\n\u001b[1;32m     88\u001b[0m template_args \u001b[38;5;241m=\u001b[39m args_generator(riddle)\n\u001b[1;32m     89\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 90\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39magenerate(prompt_template, template_args)\n\u001b[1;32m     91\u001b[0m delta \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ExecutionResult(\n\u001b[1;32m     94\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     95\u001b[0m     riddle\u001b[38;5;241m=\u001b[39mriddle,\n\u001b[1;32m     96\u001b[0m     model_output\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m     97\u001b[0m     execution_time\u001b[38;5;241m=\u001b[39mdelta,\n\u001b[1;32m     98\u001b[0m )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/scripts/lmm.py:219\u001b[0m, in \u001b[0;36mOllamaModel.agenerate\u001b[0;34m(self, chat_template, args)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate text based on the provided chat template and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m messages \u001b[38;5;241m=\u001b[39m chat_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 219\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mainvoke(messages)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatHistory([messages, response])\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:305\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 305\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    306\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    307\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    308\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    309\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    310\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:870\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    864\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    869\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    871\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    872\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:830\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    819\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    820\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m             ]\n\u001b[1;32m    829\u001b[0m         )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    831\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    832\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    834\u001b[0m ]\n\u001b[1;32m    835\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:998\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 998\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    999\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1000\u001b[0m         )\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:788\u001b[0m, in \u001b[0;36mChatOllama._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_agenerate\u001b[39m(\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    783\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 788\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_achat_stream_with_aggregation(\n\u001b[1;32m    789\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    790\u001b[0m     )\n\u001b[1;32m    791\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    792\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    793\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    794\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    798\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    799\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:645\u001b[0m, in \u001b[0;36mChatOllama._achat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_achat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    638\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    647\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    648\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    649\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 ),\n\u001b[1;32m    663\u001b[0m             )\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:575\u001b[0m, in \u001b[0;36mChatOllama._acreate_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params):\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/ollama/_client.py:667\u001b[0m, in \u001b[0;36mAsyncClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m():\n\u001b[0;32m--> 667\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:204\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:1583\u001b[0m, in \u001b[0;36mAsyncClient.stream\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m   1571\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   1572\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1582\u001b[0m )\n\u001b[0;32m-> 1583\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1584\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m   1585\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1586\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1587\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1588\u001b[0m )\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1654\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1733\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[0;32m--> 393\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Git/Privat/ICL-BRAINTEASER/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response."
     ]
    }
   ],
   "source": [
    "sp_results = await executor.aexecute(\n",
    "    dataset.sp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"sp_results_naive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3406860/2297537697.py:1: RuntimeWarning: coroutine 'Executor.aexecute' was never awaited\n",
      "  wp_results = executor.aexecute(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "wp_results = await executor.aexecute(\n",
    "    dataset.wp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    result_file_name=\"wp_results_naive\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
