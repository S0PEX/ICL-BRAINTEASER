{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are an AI assistant.\",\n",
    "    \"default_improved\": \"You are an AI assistant specialized in solving lateral thinking questions. You will receive a question with multiple choices and must determine the correct answer using logical reasoning and problem-solving techniques.\",\n",
    "    \"step_by_step\": \"You are a logical problem solver. Break down the question systematically, analyze each answer choice step by step, eliminate incorrect options, and select the best answer.\",\n",
    "    \"creative\": \"You are a lateral thinker. Approach each question with flexible reasoning, exploring unconventional yet valid interpretations before selecting the best answer.\",\n",
    "    \"elimination\": \"You are a strategic reasoner. First, identify and eliminate incorrect answer choices. Then, select the most logical remaining option.\",\n",
    "    \"metaphor\": \"You are skilled in abstract reasoning. Consider both literal and metaphorical meanings in the question and choices before selecting the most insightful answer.\",\n",
    "    \"confidence\": \"You are an analytical decision-maker. Assess the likelihood of correctness for each choice, score them internally, and select the answer with the highest confidence.\",\n",
    "    \"perspective_shift\": \"You are a multi-perspective analyst. Evaluate the question from different angles, considering alternative interpretations before determining the best answer.\",\n",
    "    \"common_sense\": \"You balance logic and practicality. Apply both structured reasoning and real-world common sense to determine the most reasonable answer.\",\n",
    "    \"assumption_challenge\": \"You are a critical thinker. Identify and question hidden assumptions in the question and choices before selecting the answer that best challenges or aligns with them.\",\n",
    "    \"pattern_matching\": \"You recognize patterns and relationships. Identify logical structures, recurring themes, or hidden connections in the question and choices before selecting the best answer.\",\n",
    "    \"intuitive\": \"You combine intuition with logic. Generate an initial answer instinctively, then critically evaluate it for logical soundness before finalizing your choice.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_prompt_template(\n",
    "    system_prompt_template_name: str = \"default\",\n",
    "):\n",
    "    system_prompt_template = get_system_prompt(system_prompt_template_name)\n",
    "    user_prompt_template = get_user_prompt()\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [system_prompt_template, user_prompt_template]\n",
    "    )\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:18:55,406 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://108.179.129.43:31701\"\n",
    "quanzization = \"q8_0\"\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(f\"llama3.1:8b-instruct-{quanzization}\", base_url=base_url),\n",
    "        # Llama3.2\n",
    "        OllamaModel(f\"llama3.2:1b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"llama3.2:3b-instruct-{quanzization}\", base_url=base_url),\n",
    "        # Phi3.5\n",
    "        OllamaModel(f\"phi3.5:3.8b-mini-instruct-{quanzization}\", base_url=base_url),\n",
    "        # Phi4\n",
    "        OllamaModel(f\"phi4:14b-{quanzization}\", base_url=base_url),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(f\"qwen2.5:0.5b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"qwen2.5:1.5b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"qwen2.5:3b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"qwen2.5:7b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"qwen2.5:14b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:32b-instruct-q5_K_M\", base_url=base_url),\n",
    "        # Gemma2\n",
    "        OllamaModel(f\"gemma2:2b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(f\"gemma2:9b-instruct-{quanzization}\", base_url=base_url),\n",
    "        OllamaModel(\"gemma2:27b-instruct-q6_K\", base_url=base_url),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\n",
    "            f\"mistral-nemo:12b-instruct-2407-{quanzization}\", base_url=base_url\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:18:55,461 - INFO - Loading results from result file: results/sp_results_naive_q8.pkl\n",
      "2025-02-18 08:18:55,954 - INFO - Results file is valid, returning results\n"
     ]
    }
   ],
   "source": [
    "sp_results = await executor.aexecute(\n",
    "    dataset.sp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    result_file_name=\"sp_results_naive_q8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:18:55,959 - INFO - Starting asynchronous execution\n",
      "2025-02-18 08:18:55,960 - INFO - Split dataset of 492 items into 99 batches of size 5\n",
      "2025-02-18 08:18:55,960 - INFO - Processing llama3.1:8b-instruct-q8_0\n",
      "2025-02-18 08:18:55,960 - INFO - Pulling Ollama model: llama3.1:8b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c93787cd11c4953af95d59cb37890fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:21:14,135 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/llama3.1:8b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:21:14,234 - INFO - Cleaning up llama3.1:8b-instruct-q8_0\n",
      "2025-02-18 08:21:14,235 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:21:14,235 - INFO - Processing llama3.2:1b-instruct-q8_0\n",
      "2025-02-18 08:21:14,236 - INFO - Pulling Ollama model: llama3.2:1b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd23be3a224d02be18d54ff7979763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:22:03,226 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/llama3.2:1b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:22:03,517 - INFO - Cleaning up llama3.2:1b-instruct-q8_0\n",
      "2025-02-18 08:22:03,518 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:22:03,518 - INFO - Processing llama3.2:3b-instruct-q8_0\n",
      "2025-02-18 08:22:03,518 - INFO - Pulling Ollama model: llama3.2:3b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a33fc198d44d09b50dc5f951813eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:23:17,009 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/llama3.2:3b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:23:17,113 - INFO - Cleaning up llama3.2:3b-instruct-q8_0\n",
      "2025-02-18 08:23:17,113 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:23:17,113 - INFO - Processing phi3.5:3.8b-mini-instruct-q8_0\n",
      "2025-02-18 08:23:17,114 - INFO - Pulling Ollama model: phi3.5:3.8b-mini-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbe0b4e4bfc4f6b8acdf7d718b1ea2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b-mini-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:31:02,862 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/phi3.5:3.8b-mini-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:31:02,968 - INFO - Cleaning up phi3.5:3.8b-mini-instruct-q8_0\n",
      "2025-02-18 08:31:02,968 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:31:02,968 - INFO - Processing phi4:14b-q8_0\n",
      "2025-02-18 08:31:02,969 - INFO - Pulling Ollama model: phi4:14b-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7967377af11446f94579720f38ef393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:49:17,459 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/phi4:14b-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:49:17,557 - INFO - Cleaning up phi4:14b-q8_0\n",
      "2025-02-18 08:49:17,557 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:49:17,558 - INFO - Processing qwen2.5:0.5b-instruct-q8_0\n",
      "2025-02-18 08:49:17,558 - INFO - Pulling Ollama model: qwen2.5:0.5b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2ad6c150aa426c9001ddb0288a9a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:50:05,930 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:0.5b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:50:06,030 - INFO - Cleaning up qwen2.5:0.5b-instruct-q8_0\n",
      "2025-02-18 08:50:06,030 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:50:06,031 - INFO - Processing qwen2.5:1.5b-instruct-q8_0\n",
      "2025-02-18 08:50:06,031 - INFO - Pulling Ollama model: qwen2.5:1.5b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085597dec8394d38870eb7aba7e6eb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:51:01,175 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:1.5b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:51:01,269 - INFO - Cleaning up qwen2.5:1.5b-instruct-q8_0\n",
      "2025-02-18 08:51:01,269 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:51:01,270 - INFO - Processing qwen2.5:3b-instruct-q8_0\n",
      "2025-02-18 08:51:01,270 - INFO - Pulling Ollama model: qwen2.5:3b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a04b9f049493a98cd24009568cd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:52:20,335 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:3b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:52:20,440 - INFO - Cleaning up qwen2.5:3b-instruct-q8_0\n",
      "2025-02-18 08:52:20,441 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:52:20,441 - INFO - Processing qwen2.5:7b-instruct-q8_0\n",
      "2025-02-18 08:52:20,441 - INFO - Pulling Ollama model: qwen2.5:7b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b51e1d347e64efd840969da42e679b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 08:54:39,526 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:7b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 08:54:39,919 - INFO - Cleaning up qwen2.5:7b-instruct-q8_0\n",
      "2025-02-18 08:54:39,920 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 08:54:39,920 - INFO - Processing qwen2.5:14b-instruct-q8_0\n",
      "2025-02-18 08:54:39,920 - INFO - Pulling Ollama model: qwen2.5:14b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acab5051f38c420f99e7c7e65c6bb81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:01:55,374 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:14b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:01:55,592 - INFO - Cleaning up qwen2.5:14b-instruct-q8_0\n",
      "2025-02-18 09:01:55,594 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:01:55,595 - INFO - Processing qwen2.5:32b-instruct-q5_K_M\n",
      "2025-02-18 09:01:55,596 - INFO - Pulling Ollama model: qwen2.5:32b-instruct-q5_K_M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1df565302b42c489658d64d042de34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b-instruct-q5_K_M:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:18:50,186 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/qwen2.5:32b-instruct-q5_K_M_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:18:50,292 - INFO - Cleaning up qwen2.5:32b-instruct-q5_K_M\n",
      "2025-02-18 09:18:50,293 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:18:50,293 - INFO - Processing gemma2:2b-instruct-q8_0\n",
      "2025-02-18 09:18:50,293 - INFO - Pulling Ollama model: gemma2:2b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd9e606cf174640b0500ede00d741b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:20:11,277 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/gemma2:2b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:20:11,374 - INFO - Cleaning up gemma2:2b-instruct-q8_0\n",
      "2025-02-18 09:20:11,374 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:20:11,375 - INFO - Processing gemma2:9b-instruct-q8_0\n",
      "2025-02-18 09:20:11,375 - INFO - Pulling Ollama model: gemma2:9b-instruct-q8_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939245fbdb54452b926dad6edcb5e6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b-instruct-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:23:28,290 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/gemma2:9b-instruct-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:23:28,650 - INFO - Cleaning up gemma2:9b-instruct-q8_0\n",
      "2025-02-18 09:23:28,651 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:23:28,651 - INFO - Processing gemma2:27b-instruct-q6_K\n",
      "2025-02-18 09:23:28,652 - INFO - Pulling Ollama model: gemma2:27b-instruct-q6_K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a6578934294d5aab11a787011e3817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b-instruct-q6_K:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:34:19,384 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/gemma2:27b-instruct-q6_K_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:34:19,486 - INFO - Cleaning up gemma2:27b-instruct-q6_K\n",
      "2025-02-18 09:34:19,487 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:34:19,487 - INFO - Processing mistral-nemo:12b-instruct-2407-q8_0\n",
      "2025-02-18 09:34:19,488 - INFO - Pulling Ollama model: mistral-nemo:12b-instruct-2407-q8_0\n",
      "2025-02-18 09:35:33,166 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-824229be17606dd8177fc91c1d330b065bc4f3de2873eab614376b988dcbf48a-partial: no space left on device (status code: 500)\n",
      "2025-02-18 09:35:33,167 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bfbea4738543668b52b042fcffa185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b-instruct-2407-q8_0:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 09:38:44,648 - INFO - Creating checkpoint: results/checkpoints/wp_results_naive_q8/mistral-nemo:12b-instruct-2407-q8_0_wp_results_naive_q8.pkl\n",
      "2025-02-18 09:38:44,747 - INFO - Cleaning up mistral-nemo:12b-instruct-2407-q8_0\n",
      "2025-02-18 09:38:44,747 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-18 09:38:44,748 - INFO - Asynchronous execution complete\n",
      "2025-02-18 09:38:44,748 - INFO - Dumping results to results/wp_results_naive_q8.pkl\n"
     ]
    }
   ],
   "source": [
    "wp_results = await executor.aexecute(\n",
    "    dataset.wp,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    result_file_name=\"wp_results_naive_q8\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
