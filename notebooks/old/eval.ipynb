{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama3.1:8b': {'sp': {'prompt_type': 'default_improved', 'score': 62.0}, 'wp': {'prompt_type': 'pattern_matching', 'score': 38.0}}, 'llama3.2:1b': {'sp': {'prompt_type': 'confidence', 'score': 6.0}, 'wp': {'prompt_type': 'confidence', 'score': 12.0}}, 'llama3.2:3b': {'sp': {'prompt_type': 'default', 'score': 32.0}, 'wp': {'prompt_type': 'intuitive', 'score': 24.0}}, 'phi3.5:3.8b': {'sp': {'prompt_type': 'default', 'score': 34.0}, 'wp': {'prompt_type': 'assumption_challenge', 'score': 30.0}}, 'phi4:14b': {'sp': {'prompt_type': 'default', 'score': 86.0}, 'wp': {'prompt_type': 'default_improved', 'score': 66.0}}, 'qwen2.5:0.5b': {'sp': {'prompt_type': 'step_by_step', 'score': 26.0}, 'wp': {'prompt_type': 'step_by_step', 'score': 28.000000000000004}}, 'qwen2.5:1.5b': {'sp': {'prompt_type': 'step_by_step', 'score': 14.000000000000002}, 'wp': {'prompt_type': 'step_by_step', 'score': 20.0}}, 'qwen2.5:3b': {'sp': {'prompt_type': 'intuitive', 'score': 20.0}, 'wp': {'prompt_type': 'intuitive', 'score': 18.0}}, 'qwen2.5:7b': {'sp': {'prompt_type': 'default', 'score': 62.0}, 'wp': {'prompt_type': 'intuitive', 'score': 20.0}}, 'qwen2.5:14b': {'sp': {'prompt_type': 'creative', 'score': 70.0}, 'wp': {'prompt_type': 'metaphor', 'score': 54.0}}, 'qwen2.5:32b': {'sp': {'prompt_type': 'default', 'score': 86.0}, 'wp': {'prompt_type': 'creative', 'score': 68.0}}, 'gemma2:2b': {'sp': {'prompt_type': 'step_by_step', 'score': 10.0}, 'wp': {'prompt_type': 'intuitive', 'score': 6.0}}, 'gemma2:9b': {'sp': {'prompt_type': 'default', 'score': 86.0}, 'wp': {'prompt_type': 'pattern_matching', 'score': 62.0}}, 'gemma2:27b': {'sp': {'prompt_type': 'creative', 'score': 92.0}, 'wp': {'prompt_type': 'creative', 'score': 68.0}}, 'mistral-nemo:12b': {'sp': {'prompt_type': 'perspective_shift', 'score': 62.0}, 'wp': {'prompt_type': 'confidence', 'score': 36.0}}}\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "from scripts.evaluation import eval_model_results\n",
    "\n",
    "\n",
    "def get_best_prompt_for_each_model(input_data):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            # For each model, we need to track its best score\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Assume eval_results returns a score based on the result data\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # If this model doesn't have a best score for this dataset yet or if the current score is better\n",
    "                if (\n",
    "                    dataset_type not in best_prompts[model]\n",
    "                    or score > best_prompts[model][dataset_type][\"score\"]\n",
    "                ):\n",
    "                    best_prompts[model][dataset_type] = {\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "\n",
    "    # Now best_prompts contains the best prompt type for each model and dataset\n",
    "    return best_prompts\n",
    "\n",
    "\n",
    "def get_best_n_prompts_for_each_model(input_data, n=3):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Calculate the score for the model with the current prompt type and dataset\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # Initialize the list of prompts for this model and dataset type if not created\n",
    "                if dataset_type not in best_prompts[model]:\n",
    "                    best_prompts[model][dataset_type] = []\n",
    "\n",
    "                # Append the prompt type and score to the list\n",
    "                best_prompts[model][dataset_type].append(\n",
    "                    {\"prompt_type\": prompt_type, \"score\": score}\n",
    "                )\n",
    "\n",
    "    # Now sort the list of prompts for each model and dataset type and keep the top n\n",
    "    top_n_prompts = {}\n",
    "    for model, dataset_dict in best_prompts.items():\n",
    "        top_n_prompts_for_model = {}\n",
    "        for dataset_type, prompts in dataset_dict.items():\n",
    "            # Get the top n prompts by sorting the list based on score (highest score first)\n",
    "            sorted_prompts = heapq.nlargest(n, prompts, key=lambda x: x[\"score\"])\n",
    "            top_n_prompts_for_model[dataset_type] = sorted_prompts\n",
    "        top_n_prompts[model] = top_n_prompts_for_model\n",
    "\n",
    "    return top_n_prompts\n",
    "\n",
    "\n",
    "# Load the results\n",
    "results_wrapper = None\n",
    "with open(\"results/results_test_0_system-prime-messages.pkl\", \"rb\") as f:\n",
    "    results_wrapper = pickle.load(f)\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "best_prompt_types = get_best_prompt_for_each_model(results_wrapper)\n",
    "print(best_prompt_types)\n",
    "\n",
    "# Save the best prompt types\n",
    "with open(\"results/best_prompt_types.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_prompt_types, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "results_wrapper = None\n",
    "with open(\"results/results_test_0_system-prime-messages.pkl\", \"rb\") as f:\n",
    "    results_wrapper = pickle.load(f)\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "top_n_prompts = get_best_n_prompts_for_each_model(results_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best Prompt Types for Model: llama3.1:8b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default_improved (Score: 62.00)\n",
      "    2. pattern_matching (Score: 62.00)\n",
      "    3. intuitive (Score: 54.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. pattern_matching (Score: 38.00)\n",
      "    2. metaphor (Score: 36.00)\n",
      "    3. default (Score: 34.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: llama3.2:1b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. confidence (Score: 6.00)\n",
      "    2. default (Score: 2.00)\n",
      "    3. default_improved (Score: 0.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. confidence (Score: 12.00)\n",
      "    2. step_by_step (Score: 10.00)\n",
      "    3. default_improved (Score: 8.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: llama3.2:3b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 32.00)\n",
      "    2. intuitive (Score: 26.00)\n",
      "    3. step_by_step (Score: 24.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. intuitive (Score: 24.00)\n",
      "    2. creative (Score: 22.00)\n",
      "    3. perspective_shift (Score: 20.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: phi3.5:3.8b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 34.00)\n",
      "    2. default_improved (Score: 28.00)\n",
      "    3. elimination (Score: 24.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. assumption_challenge (Score: 30.00)\n",
      "    2. default (Score: 26.00)\n",
      "    3. step_by_step (Score: 26.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: phi4:14b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 86.00)\n",
      "    2. default_improved (Score: 84.00)\n",
      "    3. pattern_matching (Score: 80.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. default_improved (Score: 66.00)\n",
      "    2. default (Score: 58.00)\n",
      "    3. pattern_matching (Score: 56.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:0.5b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. step_by_step (Score: 26.00)\n",
      "    2. perspective_shift (Score: 24.00)\n",
      "    3. intuitive (Score: 24.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. step_by_step (Score: 28.00)\n",
      "    2. intuitive (Score: 28.00)\n",
      "    3. creative (Score: 26.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:1.5b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. step_by_step (Score: 14.00)\n",
      "    2. confidence (Score: 14.00)\n",
      "    3. intuitive (Score: 14.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. step_by_step (Score: 20.00)\n",
      "    2. confidence (Score: 16.00)\n",
      "    3. creative (Score: 8.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:3b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. intuitive (Score: 20.00)\n",
      "    2. elimination (Score: 10.00)\n",
      "    3. metaphor (Score: 10.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. intuitive (Score: 18.00)\n",
      "    2. elimination (Score: 12.00)\n",
      "    3. default (Score: 10.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:7b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 62.00)\n",
      "    2. intuitive (Score: 60.00)\n",
      "    3. perspective_shift (Score: 58.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. intuitive (Score: 20.00)\n",
      "    2. common_sense (Score: 14.00)\n",
      "    3. default (Score: 12.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:14b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. creative (Score: 70.00)\n",
      "    2. confidence (Score: 68.00)\n",
      "    3. metaphor (Score: 66.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. metaphor (Score: 54.00)\n",
      "    2. creative (Score: 52.00)\n",
      "    3. intuitive (Score: 46.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: qwen2.5:32b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 86.00)\n",
      "    2. creative (Score: 86.00)\n",
      "    3. metaphor (Score: 86.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. creative (Score: 68.00)\n",
      "    2. default_improved (Score: 66.00)\n",
      "    3. pattern_matching (Score: 66.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: gemma2:2b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. step_by_step (Score: 10.00)\n",
      "    2. default_improved (Score: 8.00)\n",
      "    3. assumption_challenge (Score: 8.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. intuitive (Score: 6.00)\n",
      "    2. step_by_step (Score: 2.00)\n",
      "    3. default (Score: 0.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: gemma2:9b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. default (Score: 86.00)\n",
      "    2. pattern_matching (Score: 86.00)\n",
      "    3. creative (Score: 84.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. pattern_matching (Score: 62.00)\n",
      "    2. default_improved (Score: 58.00)\n",
      "    3. creative (Score: 58.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: gemma2:27b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. creative (Score: 92.00)\n",
      "    2. default (Score: 90.00)\n",
      "    3. default_improved (Score: 90.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. creative (Score: 68.00)\n",
      "    2. metaphor (Score: 64.00)\n",
      "    3. confidence (Score: 64.00)\n",
      "\n",
      "==================================================\n",
      "--- Best Prompt Types for Model: mistral-nemo:12b ---\n",
      "\n",
      "  Dataset: sp\n",
      "  ------------------------------\n",
      "    1. perspective_shift (Score: 62.00)\n",
      "    2. default_improved (Score: 50.00)\n",
      "    3. intuitive (Score: 50.00)\n",
      "\n",
      "  Dataset: wp\n",
      "  ------------------------------\n",
      "    1. confidence (Score: 36.00)\n",
      "    2. metaphor (Score: 32.00)\n",
      "    3. intuitive (Score: 30.00)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for model, dataset_dict in top_n_prompts.items():\n",
    "    print(f\"--- Best Prompt Types for Model: {model} ---\")\n",
    "    for dataset_type, prompts in dataset_dict.items():\n",
    "        print(f\"\\n  Dataset: {dataset_type}\")\n",
    "        print(f\"  {'-' * 30}\")\n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"    {i}. {prompt['prompt_type']} (Score: {prompt['score']:.2f})\")\n",
    "    print(\"\\n\" + \"=\" * 50)  # Separator between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the q4 prompted models\n",
      "llama3.1:8b: 45.13556618819777%\n",
      "llama3.2:1b: 9.090909090909092%\n",
      "llama3.2:3b: 25.996810207336523%\n",
      "phi3.5:3.8b: 1.1164274322169059%\n",
      "phi4:14b: 75.11961722488039%\n",
      "qwen2.5:0.5b: 17.384370015948964%\n",
      "qwen2.5:1.5b: 15.94896331738437%\n",
      "qwen2.5:3b: 20.095693779904305%\n",
      "qwen2.5:7b: 51.03668261562998%\n",
      "qwen2.5:14b: 56.61881977671451%\n",
      "qwen2.5:32b: 68.74003189792663%\n",
      "gemma2:2b: 11.164274322169058%\n",
      "gemma2:9b: 76.55502392344498%\n",
      "gemma2:27b: 82.93460925039872%\n",
      "mistral-nemo:12b: 49.122807017543856%\n",
      "\n",
      "\n",
      "\n",
      "Results for the q8 prompted models\n",
      "llama3.1:8b-instruct-q8_0: 36.68261562998405%\n",
      "llama3.2:1b-instruct-q8_0: 9.090909090909092%\n",
      "llama3.2:3b-instruct-q8_0: 26.6347687400319%\n",
      "phi3.5:3.8b-mini-instruct-q8_0: 4.944178628389154%\n",
      "phi4:14b-q8_0: 76.55502392344498%\n",
      "qwen2.5:0.5b-instruct-q8_0: 13.716108452950559%\n",
      "qwen2.5:1.5b-instruct-q8_0: 15.789473684210526%\n",
      "qwen2.5:3b-instruct-q8_0: 27.91068580542265%\n",
      "qwen2.5:7b-instruct-q8_0: 56.77830940988836%\n",
      "qwen2.5:14b-instruct-q8_0: 59.01116427432217%\n",
      "qwen2.5:32b-instruct-q5_K_M: 68.89952153110048%\n",
      "gemma2:2b-instruct-q8_0: 15.151515151515152%\n",
      "gemma2:9b-instruct-q8_0: 78.30940988835727%\n",
      "gemma2:27b-instruct-q6_K: 82.61562998405104%\n",
      "mistral-nemo:12b-instruct-2407-q8_0: 47.04944178628389%\n",
      "\n",
      "\n",
      "\n",
      "Results for the final prompted models best templates\n",
      "llama3.1:8b-instruct-q8_0: 40.98883572567783%\n",
      "llama3.2:1b-instruct-fp16: 10.845295055821373%\n",
      "llama3.2:3b-instruct-fp16: 27.432216905901118%\n",
      "phi3.5:3.8b-mini-instruct-fp16: 4.784688995215311%\n",
      "phi4:14b-q4_K_M: 75.27910685805422%\n",
      "qwen2.5:0.5b-instruct-fp16: 27.591706539074963%\n",
      "qwen2.5:1.5b-instruct-fp16: 26.6347687400319%\n",
      "qwen2.5:3b-instruct-fp16: 30.62200956937799%\n",
      "qwen2.5:7b-instruct-q8_0: 59.64912280701754%\n",
      "qwen2.5:14b-instruct-q4_K_M: 68.10207336523126%\n",
      "qwen2.5:32b-instruct-q4_K_M: 70.33492822966507%\n",
      "gemma2:2b-instruct-fp16: 22.48803827751196%\n",
      "gemma2:9b-instruct-q8_0: 78.62838915470495%\n",
      "gemma2:27b-instruct-q4_K_M: 81.97767145135566%\n",
      "mistral-nemo:12b-instruct-2407-q4_K_M: 45.45454545454545%\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def print_results(path: Path | str):\n",
    "    if isinstance(path, str):\n",
    "        path = Path(path)\n",
    "    results_wrapper = None\n",
    "    with open(path, \"rb\") as f:\n",
    "        results_wrapper = pickle.load(f)\n",
    "\n",
    "    for model, results in results_wrapper.items():\n",
    "        print(f\"{model}: {eval_model_results(results)}%\")\n",
    "\n",
    "\n",
    "print(\"Results for the q4 prompted models\")\n",
    "print_results(\"results/sp_results_naive.pkl\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Results for the q8 prompted models\")\n",
    "print_results(\"results/sp_results_naive_q8.pkl\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Results for the final prompted models best templates\")\n",
    "print_results(\"results/sp_results_best_templates.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
