{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import dill as pickle\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are an AI assistant.\",\n",
    "    \"default_improved\": \"You are an AI assistant specialized in solving lateral thinking questions. You will receive a question with multiple choices and must determine the correct answer using logical reasoning and problem-solving techniques.\",\n",
    "    \"step_by_step\": \"You are a logical problem solver. Break down the question systematically, analyze each answer choice step by step, eliminate incorrect options, and select the best answer.\",\n",
    "    \"creative\": \"You are a lateral thinker. Approach each question with flexible reasoning, exploring unconventional yet valid interpretations before selecting the best answer.\",\n",
    "    \"elimination\": \"You are a strategic reasoner. First, identify and eliminate incorrect answer choices. Then, select the most logical remaining option.\",\n",
    "    \"metaphor\": \"You are skilled in abstract reasoning. Consider both literal and metaphorical meanings in the question and choices before selecting the most insightful answer.\",\n",
    "    \"confidence\": \"You are an analytical decision-maker. Assess the likelihood of correctness for each choice, score them internally, and select the answer with the highest confidence.\",\n",
    "    \"perspective_shift\": \"You are a multi-perspective analyst. Evaluate the question from different angles, considering alternative interpretations before determining the best answer.\",\n",
    "    \"common_sense\": \"You balance logic and practicality. Apply both structured reasoning and real-world common sense to determine the most reasonable answer.\",\n",
    "    \"assumption_challenge\": \"You are a critical thinker. Identify and question hidden assumptions in the question and choices before selecting the answer that best challenges or aligns with them.\",\n",
    "    \"pattern_matching\": \"You recognize patterns and relationships. Identify logical structures, recurring themes, or hidden connections in the question and choices before selecting the best answer.\",\n",
    "    \"intuitive\": \"You combine intuition with logic. Generate an initial answer instinctively, then critically evaluate it for logical soundness before finalizing your choice.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt_template(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_human_prompt_template():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "best_prompt_types = None\n",
    "with open(\"results/dict_best_system_prompt_for_models.pkl\", \"rb\") as f:\n",
    "    best_prompt_types = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "        \"answer\": string.ascii_uppercase[riddle_question.label],\n",
    "    }\n",
    "\n",
    "    return template_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the few shot exampel but following the best practices from https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n",
    "# Thus we do not provide the examples in the initial client prompt but as a message history of the user asking and the system answering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "def get_few_shot_dataset(dataset: list[RiddleQuestion], number_of_shots: int = 4):\n",
    "    riddles_as_examples = dataset[:number_of_shots]\n",
    "    riddles_to_solve = dataset[number_of_shots:]\n",
    "    return (riddles_as_examples, riddles_to_solve)\n",
    "\n",
    "\n",
    "def get_few_shot_chat_template(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    dataset: list[RiddleQuestion],\n",
    "    number_of_shots: int = 4,\n",
    "):\n",
    "    riddles_as_examples, riddles_to_solve = get_few_shot_dataset(\n",
    "        dataset, number_of_shots\n",
    "    )\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_human_prompt_template(),\n",
    "            (\"ai\", \"{answer}\"),\n",
    "        ]\n",
    "    )\n",
    "    few_shot_prompt_naive = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=[args_generator(example) for example in riddles_as_examples],\n",
    "    )\n",
    "\n",
    "    best_system_template_name = best_prompt_types[model_name][dataset_name][\n",
    "        \"prompt_type\"\n",
    "    ]\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            get_system_prompt_template(best_system_template_name),\n",
    "            few_shot_prompt_naive,\n",
    "            get_human_prompt_template(),\n",
    "        ]\n",
    "    )\n",
    "    return (chat_prompt_template, riddles_to_solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:22:50,622 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://108.179.129.43:31701\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b\"),\n",
    "        model_builder.build_model(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:1.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:3b\"),\n",
    "        model_builder.build_model(\"qwen2.5:7b\"),\n",
    "        model_builder.build_model(\"qwen2.5:14b\"),\n",
    "        model_builder.build_model(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b\"),\n",
    "        model_builder.build_model(\"gemma2:9b\"),\n",
    "        model_builder.build_model(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size = 50\n",
    "sp_data = dataset.sp[0:test_sample_size]\n",
    "wp_data = dataset.wp[0:test_sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:22:50,662 - INFO - Restored results from results file results/sp_results_explore_best_few_shot_n_1.pkl, skipping execution for this model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:22:50,676 - INFO - Restored results from results file results/wp_results_explore_best_few_shot_n_1.pkl, skipping execution for this model!\n",
      "2025-02-18 16:22:50,798 - INFO - Restored results from results file results/sp_results_explore_best_few_shot_n_2.pkl, skipping execution for this model!\n",
      "2025-02-18 16:22:50,813 - INFO - Restored results from results file results/wp_results_explore_best_few_shot_n_2.pkl, skipping execution for this model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running few-shot evaluation with 1 shots\n",
      "Running few-shot evaluation with 2 shots\n",
      "Running few-shot evaluation with 3 shots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:22:50,964 - INFO - Restored results from results file results/sp_results_explore_best_few_shot_n_3.pkl, skipping execution for this model!\n",
      "2025-02-18 16:22:50,984 - INFO - Restored results from results file results/wp_results_explore_best_few_shot_n_3.pkl, skipping execution for this model!\n",
      "2025-02-18 16:22:51,177 - INFO - Restored results from results file results/sp_results_explore_best_few_shot_n_4.pkl, skipping execution for this model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running few-shot evaluation with 4 shots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:22:51,206 - INFO - Restored results from results file results/wp_results_explore_best_few_shot_n_4.pkl, skipping execution for this model!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "max_shots = 4\n",
    "\n",
    "total_results = {}\n",
    "for n_shots in range(1, max_shots + 1):\n",
    "    print(f\"Running few-shot evaluation with {n_shots} shots\")\n",
    "    _, sp_riddles_for_eval = get_few_shot_dataset(sp_data, n_shots)\n",
    "    _, wp_riddles_for_eval = get_few_shot_dataset(wp_data, n_shots)\n",
    "\n",
    "    sp_results = await executor.aexecute(\n",
    "        sp_riddles_for_eval,\n",
    "        lambda x, n_shots=n_shots: get_few_shot_chat_template(\n",
    "            x, \"sp\", sp_data, n_shots\n",
    "        )[0],\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        result_file_name=f\"sp_results_explore_best_few_shot_n_{n_shots}\",\n",
    "    )\n",
    "\n",
    "    wp_results = await executor.aexecute(\n",
    "        wp_riddles_for_eval,\n",
    "        lambda x, n_shots=n_shots: get_few_shot_chat_template(\n",
    "            x, \"wp\", wp_data, n_shots\n",
    "        )[0],\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        result_file_name=f\"wp_results_explore_best_few_shot_n_{n_shots}\",\n",
    "    )\n",
    "\n",
    "    total_results[n_shots] = {\"sp\": sp_results, \"wp\": wp_results}\n",
    "\n",
    "results_file = Path(\"results/results_test_4_explore_best_n_few_shot_prompts.pkl\")\n",
    "results_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with results_file.open(\"wb\") as f:\n",
    "    pickle.dump(total_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama3.1:8b': {'sp': {'n': 4, 'score': 78.26086956521739}, 'wp': {'n': 3, 'score': 48.93617021276596}}, 'llama3.2:1b': {'sp': {'n': 3, 'score': 19.148936170212767}, 'wp': {'n': 3, 'score': 21.27659574468085}}, 'llama3.2:3b': {'sp': {'n': 1, 'score': 48.97959183673469}, 'wp': {'n': 1, 'score': 28.57142857142857}}, 'phi3.5:3.8b': {'sp': {'n': 4, 'score': 60.86956521739131}, 'wp': {'n': 3, 'score': 29.78723404255319}}, 'phi4:14b': {'sp': {'n': 4, 'score': 91.30434782608695}, 'wp': {'n': 1, 'score': 69.38775510204081}}, 'qwen2.5:0.5b': {'sp': {'n': 1, 'score': 26.53061224489796}, 'wp': {'n': 4, 'score': 21.73913043478261}}, 'qwen2.5:1.5b': {'sp': {'n': 1, 'score': 57.14285714285714}, 'wp': {'n': 2, 'score': 33.33333333333333}}, 'qwen2.5:3b': {'sp': {'n': 4, 'score': 36.95652173913043}, 'wp': {'n': 4, 'score': 39.130434782608695}}, 'qwen2.5:7b': {'sp': {'n': 3, 'score': 82.97872340425532}, 'wp': {'n': 4, 'score': 50.0}}, 'qwen2.5:14b': {'sp': {'n': 3, 'score': 74.46808510638297}, 'wp': {'n': 1, 'score': 63.26530612244898}}, 'qwen2.5:32b': {'sp': {'n': 1, 'score': 87.75510204081633}, 'wp': {'n': 4, 'score': 73.91304347826086}}, 'gemma2:2b': {'sp': {'n': 4, 'score': 34.78260869565217}, 'wp': {'n': 4, 'score': 21.73913043478261}}, 'gemma2:9b': {'sp': {'n': 3, 'score': 89.36170212765957}, 'wp': {'n': 4, 'score': 65.21739130434783}}, 'gemma2:27b': {'sp': {'n': 1, 'score': 89.79591836734694}, 'wp': {'n': 4, 'score': 69.56521739130434}}, 'mistral-nemo:12b': {'sp': {'n': 1, 'score': 51.02040816326531}, 'wp': {'n': 4, 'score': 36.95652173913043}}}\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "from scripts.evaluation import eval_model_results\n",
    "\n",
    "\n",
    "def get_best_prompt_for_each_model(input_data):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for n, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            # For each model, we need to track its best score\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Assume eval_results returns a score based on the result data\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # If this model doesn't have a best score for this dataset yet or if the current score is better\n",
    "                if (\n",
    "                    dataset_type not in best_prompts[model]\n",
    "                    or score > best_prompts[model][dataset_type][\"score\"]\n",
    "                ):\n",
    "                    best_prompts[model][dataset_type] = {\n",
    "                        \"n\": n,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "\n",
    "    # Now best_prompts contains the best prompt type for each model and dataset\n",
    "    return best_prompts\n",
    "\n",
    "\n",
    "def get_best_n_prompts_for_each_model(input_data, n=3):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for n, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Calculate the score for the model with the current prompt type and dataset\n",
    "                score = eval_model_results(result)\n",
    "\n",
    "                # Initialize the list of prompts for this model and dataset type if not created\n",
    "                if dataset_type not in best_prompts[model]:\n",
    "                    best_prompts[model][dataset_type] = []\n",
    "\n",
    "                # Append the prompt type and score to the list\n",
    "                best_prompts[model][dataset_type].append({\"n\": n, \"score\": score})\n",
    "\n",
    "    # Now sort the list of prompts for each model and dataset type and keep the top n\n",
    "    top_n_prompts = {}\n",
    "    for model, dataset_dict in best_prompts.items():\n",
    "        top_n_prompts_for_model = {}\n",
    "        for dataset_type, prompts in dataset_dict.items():\n",
    "            # Get the top n prompts by sorting the list based on score (highest score first)\n",
    "            sorted_prompts = heapq.nlargest(n, prompts, key=lambda x: x[\"score\"])\n",
    "            top_n_prompts_for_model[dataset_type] = sorted_prompts\n",
    "        top_n_prompts[model] = top_n_prompts_for_model\n",
    "\n",
    "    return top_n_prompts\n",
    "\n",
    "\n",
    "# Load the results\n",
    "results_wrapper = None\n",
    "with open(results_file, \"rb\") as f:\n",
    "    results_wrapper = pickle.load(f)\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "best_prompt_types = get_best_prompt_for_each_model(results_wrapper)\n",
    "print(best_prompt_types)\n",
    "\n",
    "# Save the best prompt types\n",
    "with open(\"results/dict_best_n_few_shot_for_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_prompt_types, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
