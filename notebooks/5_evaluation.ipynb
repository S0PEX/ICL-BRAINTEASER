{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as dill\n",
    "from pathlib import Path\n",
    "\n",
    "from scripts.executor import WrappedResults\n",
    "\n",
    "\n",
    "def load_wrapped_results(path: Path | str) -> WrappedResults:\n",
    "    \"\"\"\n",
    "    Load WrappedResults from a file.\n",
    "    :param path: Path to the file.\n",
    "    :return: WrappedResults object.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        return dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scripts.executor import ExecutionResult\n",
    "from scripts.evaluation import is_model_response_correct\n",
    "\n",
    "\n",
    "class AnswerExtractorType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for the different types of answer extractors.\n",
    "    \"\"\"\n",
    "\n",
    "    RAW = \"raw\"\n",
    "    POSTPROCESSED = \"postprocessed\"\n",
    "\n",
    "\n",
    "# Modified version from https://github.com/1171-jpg/BrainTeaser/blob/main/Prompting/utils.py\n",
    "# to match the format of the results\n",
    "def getResultdata(\n",
    "    execution_results: list[ExecutionResult], extractor_type: AnswerExtractorType\n",
    ") -> dict:\n",
    "    word_play = {}\n",
    "    reverse_play = {}\n",
    "    for execution_result in execution_results:\n",
    "        item = execution_result.riddle\n",
    "        item_type = item.id.split(\"-\")[0]\n",
    "        item_id = item.id.split(\"-\")[1].split(\"_\")[0]\n",
    "        if item_type == \"WP\":\n",
    "            if item_id not in word_play:\n",
    "                word_play[item_id] = [0, 0, 0]\n",
    "        else:\n",
    "            if item_id not in reverse_play:\n",
    "                reverse_play[item_id] = [0, 0, 0]\n",
    "\n",
    "    for execution_result in execution_results:\n",
    "        item = execution_result.riddle\n",
    "        item_type = item.id.split(\"-\")[0]\n",
    "        item_id = item.id.split(\"-\")[1].split(\"_\")[0]\n",
    "        ad_type = 0\n",
    "        if \"SR\" in item.id:\n",
    "            ad_type = 1\n",
    "        elif \"CR\" in item.id:\n",
    "            ad_type = 2\n",
    "        else:\n",
    "            ad_type = 0\n",
    "\n",
    "        raw_correct, postprocessed_correct = is_model_response_correct(execution_result)\n",
    "\n",
    "        if item_type == \"WP\":\n",
    "            word_play[item_id][ad_type] = (\n",
    "                raw_correct\n",
    "                if extractor_type == AnswerExtractorType.RAW\n",
    "                else postprocessed_correct\n",
    "            )\n",
    "        else:\n",
    "            reverse_play[item_id][ad_type] = (\n",
    "                raw_correct\n",
    "                if extractor_type == AnswerExtractorType.RAW\n",
    "                else postprocessed_correct\n",
    "            )\n",
    "\n",
    "    return word_play, reverse_play\n",
    "\n",
    "\n",
    "def getMetric(data_list):\n",
    "    data_list = np.array(data_list)\n",
    "    overall_accuracy = np.sum(data_list) / 3 / len(data_list)\n",
    "    original_accuracy = np.sum(data_list, axis=0)[0] / len(data_list)\n",
    "    semantic_accuracy = np.sum(data_list, axis=0)[1] / len(data_list)\n",
    "    context_accuracy = np.sum(data_list, axis=0)[2] / len(data_list)\n",
    "    ori_sema = np.sum(\n",
    "        [1 if item[0] == 1 and item[1] == 1 else 0 for item in data_list]\n",
    "    ) / len(data_list)\n",
    "    ori_sema_cont = np.sum(\n",
    "        [\n",
    "            1 if item[0] == 1 and item[1] == 1 and item[2] == 1 else 0\n",
    "            for item in data_list\n",
    "        ]\n",
    "    ) / len(data_list)\n",
    "\n",
    "    return {\n",
    "        \"over_all_accuracy\": np.round(overall_accuracy * 100, 2),\n",
    "        \"original_accuracy\": np.round(original_accuracy * 100, 2),\n",
    "        \"semantic_accuracy\": np.round(semantic_accuracy * 100, 2),\n",
    "        \"context_accuracy\": np.round(context_accuracy * 100, 2),\n",
    "        \"ori_sema\": np.round(ori_sema * 100, 2),\n",
    "        \"ori_sema_cont\": np.round(ori_sema_cont * 100, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "def getSeperateResult(word_play, reverse_thinking):\n",
    "    final_result = {}\n",
    "    word_data_list = []\n",
    "    for item in word_play.values():\n",
    "        word_data_list.append(item)\n",
    "    final_result[\"wordplay\"] = getMetric(word_data_list)\n",
    "\n",
    "    reverse_data_list = []\n",
    "    for item in reverse_thinking.values():\n",
    "        reverse_data_list.append(item)\n",
    "    final_result[\"sentence\"] = getMetric(reverse_data_list)\n",
    "\n",
    "    all_data = word_data_list + reverse_data_list\n",
    "    final_result[\"all\"] = getMetric(all_data)\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def getModelEvaluations(model_results: list[ExecutionResult]):\n",
    "    word_play_raw, sentence_play_raw = getResultdata(\n",
    "        model_results, AnswerExtractorType.RAW\n",
    "    )\n",
    "    word_play_postprocessed, sentence_play_postprocessed = getResultdata(\n",
    "        model_results, AnswerExtractorType.POSTPROCESSED\n",
    "    )\n",
    "    final_result_raw = getSeperateResult(word_play_raw, sentence_play_raw)\n",
    "    final_result_postprocessed = getSeperateResult(\n",
    "        word_play_postprocessed, sentence_play_postprocessed\n",
    "    )\n",
    "\n",
    "    return final_result_raw, final_result_postprocessed\n",
    "\n",
    "\n",
    "def results_to_texttable(\n",
    "    wrapped_results: WrappedResults,\n",
    "    caption: str,\n",
    "    label: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert WrappedResults to a LaTex Table.\n",
    "    :param wrapped_results: WrappedResults object.\n",
    "    :return: Texttable.\n",
    "    \"\"\"\n",
    "    combines_model_execution_results: dict[str, list[ExecutionResult]] = {}\n",
    "    for _, models_results in wrapped_results.results.items():\n",
    "        for model_name, results in models_results.items():\n",
    "            if model_name not in combines_model_execution_results:\n",
    "                combines_model_execution_results[model_name] = []\n",
    "            combines_model_execution_results[model_name].extend(results)\n",
    "\n",
    "    def print_line(\n",
    "        model_name: str, key: str, model_evaluation_raw, model_evaluation_postprocessed\n",
    "    ):\n",
    "        model_name = model_name.replace(\"_\", \"-\").replace(\"-instruct-\", \"-\")\n",
    "        model_text = f\"\"\"\n",
    "        \\multicolumn{{1}}{{c|}}{{{model_name}}} & {model_evaluation_raw[key][\"original_accuracy\"]} ({model_evaluation_postprocessed[key][\"original_accuracy\"]}) & {model_evaluation_raw[key][\"semantic_accuracy\"]} ({model_evaluation_postprocessed[key][\"semantic_accuracy\"]}) & \\multicolumn{{1}}{{c|}}{{{model_evaluation_raw[key][\"context_accuracy\"]} ({model_evaluation_postprocessed[key][\"context_accuracy\"]})}} & {model_evaluation_raw[key][\"ori_sema\"]} ({model_evaluation_postprocessed[key][\"ori_sema\"]}) & \\multicolumn{{1}}{{c|}}{{{model_evaluation_raw[key][\"ori_sema_cont\"]} ({model_evaluation_postprocessed[key][\"ori_sema_cont\"]})}} & {model_evaluation_raw[key][\"over_all_accuracy\"]} ({model_evaluation_postprocessed[key][\"over_all_accuracy\"]}) \\\\\\\\\n",
    "        \"\"\"\n",
    "        return model_text\n",
    "\n",
    "    text_sp = []\n",
    "    text_wp = []\n",
    "    for (\n",
    "        model_name,\n",
    "        models_execution_results,\n",
    "    ) in combines_model_execution_results.items():\n",
    "        model_evaluation_raw, model_evaluation_postprocessed = getModelEvaluations(\n",
    "            models_execution_results\n",
    "        )\n",
    "\n",
    "        text_sp.append(\n",
    "            print_line(\n",
    "                model_name,\n",
    "                \"sentence\",\n",
    "                model_evaluation_raw,\n",
    "                model_evaluation_postprocessed,\n",
    "            )\n",
    "        )\n",
    "        text_wp.append(\n",
    "            print_line(\n",
    "                model_name,\n",
    "                \"wordplay\",\n",
    "                model_evaluation_raw,\n",
    "                model_evaluation_postprocessed,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sp_data = \"\\n\".join(text_sp)\n",
    "    wp_data = \"\\n\".join(text_wp)\n",
    "    full_table = (\n",
    "        \"\"\"\n",
    "    \\\\begin{table}[]\\r\\n\\\\caption{{caption}}\\r\\n\\\\label{tab:{label}}\\r\\n\\\\resizebox{\\\\textwidth}{!}{%\\r\\n\\\\begin{tabular}{ccccccc}\\r\\n\\\\cline{1-7}\\r\\n & \\\\multicolumn{3}{|c|}{Instance-based} & \\\\multicolumn{2}{c|}{Group-based} \\\\\\\\ \\\\cline{1-6}\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Model}} & \\\\textbf{Original} & \\\\textbf{Semantic} & \\\\multicolumn{1}{c|}{\\\\textbf{Context}} & \\\\textbf{Ori \\\\& Sem} & \\\\multicolumn{1}{c|}{\\\\textbf{Ori \\\\& Sem \\\\& Con}} & \\\\textbf{Overall} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{7}{c}{Sentence Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 25.52 & 24.88 & \\\\multicolumn{1}{c|}{22.81} & 5.58 & \\\\multicolumn{1}{c|}{1.44} & 24.40 \\\\\\\\ \\\\hline\\r\\n{{sp_data}}\\r\\n\\\\hline\\r\\n\\\\multicolumn{7}{c}{Word Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 26.02 & 27.85 & \\\\multicolumn{1}{c|}{22.51} & 7.32 & \\\\multicolumn{1}{c|}{1.83} & 25.34 \\\\\\\\ \\\\hline\\r\\n{{wp_data}}\\r\\n\\\\\\\\ \\\\hline\\r\\n\\\\end{tabular}%\\r\\n}\\r\\n\\\\end{table}\n",
    "    \"\"\".replace(\"{{sp_data}}\", sp_data)\n",
    "        .replace(\"{{wp_data}}\", wp_data)\n",
    "        .replace(\"{caption}\", caption)\n",
    "        .replace(\"{label}\", label)\n",
    "    )\n",
    "    output_table = Path(\"tables\") / f\"{label}.tex\"\n",
    "    if not output_table.parent.exists():\n",
    "        output_table.parent.mkdir(parents=True)\n",
    "    with open(output_table, \"w\") as f:\n",
    "        f.write(full_table)\n",
    "\n",
    "\n",
    "def create_latex_table_from_results_file(results_file: Path, caption: str, label: str):\n",
    "    wrapped_results = load_wrapped_results(results_file)\n",
    "    results_to_texttable(wrapped_results, caption, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table_from_results_file(\n",
    "    \"results/baseline-zero-shot-evaluation/baseline-zero-shot-evaluation_results.pkl\",\n",
    "    \"Zero-Shot Accuracy (Baseline)\",\n",
    "    \"baseline-zero-shot-evaluation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table_from_results_file(\n",
    "    \"results/system-optimized-zero-shot-evaluation/system-optimized-zero-shot-evaluation_results.pkl\",\n",
    "    \"Zero-Shot Accuracy (with Prompt Engeneering)\",\n",
    "    \"optimized-zero-shot-evaluation\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
