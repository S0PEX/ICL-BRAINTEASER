{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as dill\n",
    "from pathlib import Path\n",
    "\n",
    "from scripts.executor import WrappedResults\n",
    "\n",
    "\n",
    "def load_wrapped_results(path: Path | str) -> WrappedResults:\n",
    "    \"\"\"\n",
    "    Load WrappedResults from a file.\n",
    "    :param path: Path to the file.\n",
    "    :return: WrappedResults object.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        return dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scripts.executor import ExecutionResult\n",
    "from scripts.evaluation import is_model_response_correct\n",
    "\n",
    "\n",
    "class AnswerExtractorType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for the different types of answer extractors.\n",
    "    \"\"\"\n",
    "\n",
    "    RAW = \"raw\"\n",
    "    POSTPROCESSED = \"postprocessed\"\n",
    "\n",
    "\n",
    "# Modified version from https://github.com/1171-jpg/BrainTeaser/blob/main/Prompting/utils.py\n",
    "# to match the format of the results\n",
    "def getResultdata(\n",
    "    execution_results: list[ExecutionResult], extractor_type: AnswerExtractorType\n",
    ") -> dict:\n",
    "    word_play = {}\n",
    "    reverse_play = {}\n",
    "    for execution_result in execution_results:\n",
    "        item = execution_result.riddle\n",
    "        item_type = item.id.split(\"-\")[0]\n",
    "        item_id = item.id.split(\"-\")[1].split(\"_\")[0]\n",
    "        if item_type == \"WP\":\n",
    "            if item_id not in word_play:\n",
    "                word_play[item_id] = [0, 0, 0]\n",
    "        else:\n",
    "            if item_id not in reverse_play:\n",
    "                reverse_play[item_id] = [0, 0, 0]\n",
    "\n",
    "    for execution_result in execution_results:\n",
    "        item = execution_result.riddle\n",
    "        item_type = item.id.split(\"-\")[0]\n",
    "        item_id = item.id.split(\"-\")[1].split(\"_\")[0]\n",
    "        ad_type = 0\n",
    "        if \"SR\" in item.id:\n",
    "            ad_type = 1\n",
    "        elif \"CR\" in item.id:\n",
    "            ad_type = 2\n",
    "        else:\n",
    "            ad_type = 0\n",
    "\n",
    "        raw_correct, postprocessed_correct = is_model_response_correct(execution_result)\n",
    "\n",
    "        if item_type == \"WP\":\n",
    "            word_play[item_id][ad_type] = (\n",
    "                raw_correct\n",
    "                if extractor_type == AnswerExtractorType.RAW\n",
    "                else postprocessed_correct\n",
    "            )\n",
    "        else:\n",
    "            reverse_play[item_id][ad_type] = (\n",
    "                raw_correct\n",
    "                if extractor_type == AnswerExtractorType.RAW\n",
    "                else postprocessed_correct\n",
    "            )\n",
    "\n",
    "    return word_play, reverse_play\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Dataclass to store the evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    overall_accuracy: float\n",
    "    original_accuracy: float\n",
    "    semantic_accuracy: float\n",
    "    context_accuracy: float\n",
    "    ori_sema: float\n",
    "    ori_sema_cont: float\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Metrics(\n",
    "            overall_accuracy=self.overall_accuracy - other.overall_accuracy,\n",
    "            original_accuracy=self.original_accuracy - other.original_accuracy,\n",
    "            semantic_accuracy=self.semantic_accuracy - other.semantic_accuracy,\n",
    "            context_accuracy=self.context_accuracy - other.context_accuracy,\n",
    "            ori_sema=self.ori_sema - other.ori_sema,\n",
    "            ori_sema_cont=self.ori_sema_cont - other.ori_sema_cont,\n",
    "        )\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Metrics(\n",
    "            overall_accuracy=self.overall_accuracy + other.overall_accuracy,\n",
    "            original_accuracy=self.original_accuracy + other.original_accuracy,\n",
    "            semantic_accuracy=self.semantic_accuracy + other.semantic_accuracy,\n",
    "            context_accuracy=self.context_accuracy + other.context_accuracy,\n",
    "            ori_sema=self.ori_sema + other.ori_sema,\n",
    "            ori_sema_cont=self.ori_sema_cont + other.ori_sema_cont,\n",
    "        )\n",
    "\n",
    "\n",
    "def getMetric(data_list) -> Metrics:\n",
    "    data_list = np.array(data_list)\n",
    "    overall_accuracy = np.sum(data_list) / 3 / len(data_list)\n",
    "    original_accuracy = np.sum(data_list, axis=0)[0] / len(data_list)\n",
    "    semantic_accuracy = np.sum(data_list, axis=0)[1] / len(data_list)\n",
    "    context_accuracy = np.sum(data_list, axis=0)[2] / len(data_list)\n",
    "    ori_sema = np.sum(\n",
    "        [1 if item[0] == 1 and item[1] == 1 else 0 for item in data_list]\n",
    "    ) / len(data_list)\n",
    "    ori_sema_cont = np.sum(\n",
    "        [\n",
    "            1 if item[0] == 1 and item[1] == 1 and item[2] == 1 else 0\n",
    "            for item in data_list\n",
    "        ]\n",
    "    ) / len(data_list)\n",
    "\n",
    "    return Metrics(\n",
    "        overall_accuracy=np.round(overall_accuracy, 4) * 100,\n",
    "        original_accuracy=np.round(original_accuracy, 4) * 100,\n",
    "        semantic_accuracy=np.round(semantic_accuracy, 4) * 100,\n",
    "        context_accuracy=np.round(context_accuracy, 4) * 100,\n",
    "        ori_sema=np.round(ori_sema, 4) * 100,\n",
    "        ori_sema_cont=np.round(ori_sema_cont, 4) * 100,\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationMetrics:\n",
    "    wordplay: Metrics\n",
    "    sentence: Metrics\n",
    "    all: Metrics\n",
    "\n",
    "\n",
    "def getSeperateResult(word_play, reverse_thinking) -> ModelEvaluationMetrics:\n",
    "    word_data_list = []\n",
    "    for item in word_play.values():\n",
    "        word_data_list.append(item)\n",
    "\n",
    "    reverse_data_list = []\n",
    "    for item in reverse_thinking.values():\n",
    "        reverse_data_list.append(item)\n",
    "\n",
    "    all_data = word_data_list + reverse_data_list\n",
    "\n",
    "    result = ModelEvaluationMetrics(\n",
    "        wordplay=getMetric(word_data_list),\n",
    "        sentence=getMetric(reverse_data_list),\n",
    "        all=getMetric(all_data),\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationReport:\n",
    "    raw_results: ModelEvaluationMetrics\n",
    "    postprocessed_results: ModelEvaluationMetrics\n",
    "\n",
    "\n",
    "def generateEvaluationReport(model_results: list[ExecutionResult]) -> EvaluationReport:\n",
    "    word_play_raw, sentence_play_raw = getResultdata(\n",
    "        model_results, AnswerExtractorType.RAW\n",
    "    )\n",
    "    word_play_postprocessed, sentence_play_postprocessed = getResultdata(\n",
    "        model_results, AnswerExtractorType.POSTPROCESSED\n",
    "    )\n",
    "    final_result_raw = getSeperateResult(word_play_raw, sentence_play_raw)\n",
    "    final_result_postprocessed = getSeperateResult(\n",
    "        word_play_postprocessed, sentence_play_postprocessed\n",
    "    )\n",
    "\n",
    "    return EvaluationReport(\n",
    "        raw_results=final_result_raw, postprocessed_results=final_result_postprocessed\n",
    "    )\n",
    "\n",
    "\n",
    "def create_evaluation_from_results(\n",
    "    wrapped_results_path: Path | str,\n",
    ") -> dict[str, EvaluationReport]:\n",
    "    with open(wrapped_results_path, \"rb\") as f:\n",
    "        wrapped_results = dill.load(f)\n",
    "\n",
    "    combines_model_execution_results: dict[str, list[ExecutionResult]] = {}\n",
    "    for _, models_results in wrapped_results.results.items():\n",
    "        for model_name, results in models_results.items():\n",
    "            if model_name not in combines_model_execution_results:\n",
    "                combines_model_execution_results[model_name] = []\n",
    "            combines_model_execution_results[model_name].extend(results)\n",
    "\n",
    "    results: dict[str, EvaluationReport] = {}\n",
    "    for (\n",
    "        model_name,\n",
    "        models_execution_results,\n",
    "    ) in combines_model_execution_results.items():\n",
    "        report = generateEvaluationReport(models_execution_results)\n",
    "        results[model_name] = report\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_texttable(\n",
    "    results: dict[str, EvaluationReport],\n",
    "    optimized_results: dict[str, EvaluationReport],\n",
    "    caption: str,\n",
    "    label: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert WrappedResults to a LaTex Table.\n",
    "    :param wrapped_results: WrappedResults object.\n",
    "    :return: Texttable.\n",
    "    \"\"\"\n",
    "\n",
    "    def print_line(\n",
    "        model_name: str,\n",
    "        metrics_baseline: Metrics,\n",
    "        metrics_optimized: Metrics,\n",
    "        postfix: str = \"\",\n",
    "    ):\n",
    "        is_post = False  # \"POST\" in model_name\n",
    "        model_name = model_name.replace(\"_\", \"-\").replace(\"-instruct-\", \"-\")\n",
    "        model_text = f\"\"\"\n",
    "        \\multicolumn{{1}}{{{\"r\" if is_post else \"c\"}|}}{{{model_name}}} & {metrics_baseline.original_accuracy:.2f} ({metrics_optimized.original_accuracy:.2f}) & {metrics_baseline.semantic_accuracy:.2f} ({metrics_optimized.semantic_accuracy:.2f}) & \\multicolumn{{1}}{{c|}}{{{metrics_baseline.context_accuracy:.2f} ({metrics_optimized.context_accuracy:.2f})}} & {metrics_baseline.ori_sema:.2f} ({metrics_optimized.ori_sema:.2f}) & \\multicolumn{{1}}{{c|}}{{{metrics_baseline.ori_sema_cont:.2f} ({metrics_optimized.ori_sema_cont:.2f})}} & {metrics_baseline.overall_accuracy:.2f} ({metrics_optimized.overall_accuracy:.2f}) \\\\\\\\\n",
    "        {postfix}\n",
    "        \"\"\"\n",
    "        return model_text\n",
    "\n",
    "    text_sp = []\n",
    "    text_wp = []\n",
    "\n",
    "    for model_name in results:\n",
    "        model1_raw = results[model_name].raw_results\n",
    "        model1_postprocessed = results[model_name].postprocessed_results\n",
    "        model2_raw = optimized_results[model_name].raw_results\n",
    "        model2_postprocessed = optimized_results[model_name].postprocessed_results\n",
    "\n",
    "        text_sp.append(\n",
    "            print_line(\n",
    "                f\"{model_name} (RAW)\",\n",
    "                model1_raw.sentence,\n",
    "                model2_raw.sentence,\n",
    "            )\n",
    "        )\n",
    "        text_sp.append(\n",
    "            print_line(\n",
    "                '\" (POST)',\n",
    "                model1_postprocessed.sentence,\n",
    "                model2_postprocessed.sentence,\n",
    "                postfix=\"\\cline{1-7}\",\n",
    "            )\n",
    "        )\n",
    "        text_wp.append(\n",
    "            print_line(\n",
    "                f\"{model_name} (RAW)\",\n",
    "                model1_raw.wordplay,\n",
    "                model2_raw.wordplay,\n",
    "            )\n",
    "        )\n",
    "        text_wp.append(\n",
    "            print_line(\n",
    "                '\" (POST)',\n",
    "                model1_postprocessed.wordplay,\n",
    "                model2_postprocessed.wordplay,\n",
    "                postfix=\"\\cline{1-7}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sp_data = \"\\n\".join(text_sp)\n",
    "    wp_data = \"\\n\".join(text_wp)\n",
    "\n",
    "    suffix = \"'Ori & Sem' means 'Original & Semantic' while 'O & S & C' means 'Original & Semantic & Context'. 'Overall' is the average of 'Ori', 'Sem', and 'Con'. 'RAW' refers to unprocessed model responses, while 'POST' refers to postprocessed model responses.\"\n",
    "    suffix = suffix.replace(\"&\", \"\\\\&\")\n",
    "    full_table = (\n",
    "        \"\"\"\n",
    "    \\\\begin{table}[]\\r\\n\\\\caption[{caption}]{{caption}: {suffix}}\\r\\n\\\\label{tab:{label}}\\r\\n\\\\resizebox{\\\\textwidth}{!}{%\\r\\n\\\\begin{tabular}{ccccccc}\\r\\n\\\\cline{1-7}\\r\\n & \\\\multicolumn{3}{|c|}{Instance-based} & \\\\multicolumn{2}{c|}{Group-based} \\\\\\\\ \\\\cline{1-6}\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Model}} & \\\\textbf{Original} & \\\\textbf{Semantic} & \\\\multicolumn{1}{c|}{\\\\textbf{Context}} & \\\\textbf{Ori \\\\& Sem} & \\\\multicolumn{1}{c|}{\\\\textbf{O \\\\& S \\\\& C}} & \\\\textbf{Overall} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{7}{c}{Sentence Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 25.52 & 24.88 & \\\\multicolumn{1}{c|}{22.81} & 5.58 & \\\\multicolumn{1}{c|}{1.44} & 24.40 \\\\\\\\ \\\\hline\\r\\n{{sp_data}}\\r\\n\\\\hline\\r\\n\\\\multicolumn{7}{c}{Word Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 26.02 & 27.85 & \\\\multicolumn{1}{c|}{22.51} & 7.32 & \\\\multicolumn{1}{c|}{1.83} & 25.34 \\\\\\\\ \\\\hline\\r\\n{{wp_data}}\\r\\n\\\\\\\\ \\\\hline\\r\\n\\\\end{tabular}%\\r\\n}\\r\\n\\\\end{table}\n",
    "    \"\"\".replace(\"{{sp_data}}\", sp_data)\n",
    "        .replace(\"{{wp_data}}\", wp_data)\n",
    "        .replace(\"{caption}\", caption.replace(\"&\", \"\\\\&\"))\n",
    "        .replace(\"{label}\", label)\n",
    "        .replace(\"{suffix}\", suffix)\n",
    "    )\n",
    "    output_table = Path(\"tables\") / f\"{label}.tex\"\n",
    "    if not output_table.parent.exists():\n",
    "        output_table.parent.mkdir(parents=True)\n",
    "    with open(output_table, \"w\") as f:\n",
    "        f.write(full_table)\n",
    "\n",
    "\n",
    "def results_to_texttable_hightliting(\n",
    "    results: dict[str, EvaluationReport],\n",
    "    optimized_results: dict[str, EvaluationReport],\n",
    "    caption: str,\n",
    "    label: str,\n",
    "    postprocessed: bool = True,\n",
    "    same_data: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert WrappedResults to a LaTex Table.\n",
    "    :param wrapped_results: WrappedResults object.\n",
    "    :return: Texttable.\n",
    "    \"\"\"\n",
    "\n",
    "    def find_best_values(all_results, raw=True, puzzle_type=\"sentence\"):\n",
    "        best_values = {}\n",
    "        for col in [\n",
    "            \"original_accuracy\",\n",
    "            \"semantic_accuracy\",\n",
    "            \"context_accuracy\",\n",
    "            \"ori_sema\",\n",
    "            \"ori_sema_cont\",\n",
    "            \"overall_accuracy\",\n",
    "        ]:\n",
    "            best_values[col] = -float(\"inf\")\n",
    "\n",
    "        for model_name in all_results:\n",
    "            metrics = (\n",
    "                all_results[model_name].raw_results\n",
    "                if raw\n",
    "                else all_results[model_name].postprocessed_results\n",
    "            )\n",
    "\n",
    "            # Only consider the specific puzzle type (sentence or wordplay)\n",
    "            metrics_for_type = getattr(metrics, puzzle_type)\n",
    "            for col in best_values:\n",
    "                val = getattr(metrics_for_type, col)\n",
    "                best_values[col] = max(best_values[col], val)\n",
    "\n",
    "        return best_values\n",
    "\n",
    "    # Find best values separately for sentence and wordplay puzzles\n",
    "    best_baseline_sp = find_best_values(results, not postprocessed, \"sentence\")\n",
    "    best_optimized_sp = find_best_values(\n",
    "        optimized_results, not postprocessed, \"sentence\"\n",
    "    )\n",
    "    best_baseline_wp = find_best_values(results, not postprocessed, \"wordplay\")\n",
    "    best_optimized_wp = find_best_values(\n",
    "        optimized_results, not postprocessed, \"wordplay\"\n",
    "    )\n",
    "\n",
    "    def print_line(\n",
    "        model_name: str,\n",
    "        metrics_baseline: Metrics,\n",
    "        metrics_optimized: Metrics,\n",
    "        puzzle_type=\"sentence\",\n",
    "    ):\n",
    "        model_name = model_name.replace(\"_\", \"-\").replace(\"-instruct-\", \"-\")\n",
    "\n",
    "        # Select the appropriate best values based on puzzle type\n",
    "        best_baseline = (\n",
    "            best_baseline_sp if puzzle_type == \"sentence\" else best_baseline_wp\n",
    "        )\n",
    "        best_optimized = (\n",
    "            best_optimized_sp if puzzle_type == \"sentence\" else best_optimized_wp\n",
    "        )\n",
    "\n",
    "        # Format each value with highlighting if it's the best\n",
    "        def format_value(value, column, is_baseline):\n",
    "            best_values = best_baseline if is_baseline else best_optimized\n",
    "            diff = abs(value - best_values[column])\n",
    "            if diff < 0.01:  # Using a small epsilon for float comparison\n",
    "                return (\n",
    "                    f\"\\\\textbf{{{value:.2f}}}\"\n",
    "                    if is_baseline\n",
    "                    else f\"\\\\underline{{{value:.2f}}}\"\n",
    "                )\n",
    "            return f\"{value:.2f}\"\n",
    "\n",
    "        # Format each pair of values\n",
    "        orig_base = format_value(\n",
    "            metrics_baseline.original_accuracy, \"original_accuracy\", True\n",
    "        )\n",
    "        orig_opt = format_value(\n",
    "            metrics_optimized.original_accuracy, \"original_accuracy\", False\n",
    "        )\n",
    "\n",
    "        sem_base = format_value(\n",
    "            metrics_baseline.semantic_accuracy, \"semantic_accuracy\", True\n",
    "        )\n",
    "        sem_opt = format_value(\n",
    "            metrics_optimized.semantic_accuracy, \"semantic_accuracy\", False\n",
    "        )\n",
    "\n",
    "        ctx_base = format_value(\n",
    "            metrics_baseline.context_accuracy, \"context_accuracy\", True\n",
    "        )\n",
    "        ctx_opt = format_value(\n",
    "            metrics_optimized.context_accuracy, \"context_accuracy\", False\n",
    "        )\n",
    "\n",
    "        ori_sem_base = format_value(metrics_baseline.ori_sema, \"ori_sema\", True)\n",
    "        ori_sem_opt = format_value(metrics_optimized.ori_sema, \"ori_sema\", False)\n",
    "\n",
    "        ori_sem_ctx_base = format_value(\n",
    "            metrics_baseline.ori_sema_cont, \"ori_sema_cont\", True\n",
    "        )\n",
    "        ori_sem_ctx_opt = format_value(\n",
    "            metrics_optimized.ori_sema_cont, \"ori_sema_cont\", False\n",
    "        )\n",
    "\n",
    "        overall_base = format_value(\n",
    "            metrics_baseline.overall_accuracy, \"overall_accuracy\", True\n",
    "        )\n",
    "        overall_opt = format_value(\n",
    "            metrics_optimized.overall_accuracy, \"overall_accuracy\", False\n",
    "        )\n",
    "\n",
    "        model_text = f\"\"\"\n",
    "        \\multicolumn{{1}}{{c|}}{{{model_name}}} & {orig_base} ({orig_opt}) & {sem_base} ({sem_opt}) & \\multicolumn{{1}}{{c|}}{{{ctx_base} ({ctx_opt})}} & {ori_sem_base} ({ori_sem_opt}) & \\multicolumn{{1}}{{c|}}{{{ori_sem_ctx_base} ({ori_sem_ctx_opt})}} & {overall_base} ({overall_opt}) \\\\\\\\\n",
    "        \"\"\"\n",
    "        return model_text\n",
    "\n",
    "    text_sp = []\n",
    "    text_wp = []\n",
    "\n",
    "    for model_name in results:\n",
    "        model_baseline = (\n",
    "            results[model_name].raw_results\n",
    "            if not postprocessed\n",
    "            else results[model_name].postprocessed_results\n",
    "        )\n",
    "        model_optimized = (\n",
    "            optimized_results[model_name].raw_results\n",
    "            if not postprocessed\n",
    "            else optimized_results[model_name].postprocessed_results\n",
    "        )\n",
    "        text_sp.append(\n",
    "            print_line(\n",
    "                model_name,\n",
    "                model_baseline.sentence,\n",
    "                model_optimized.sentence,\n",
    "                puzzle_type=\"sentence\",\n",
    "            )\n",
    "        )\n",
    "        text_wp.append(\n",
    "            print_line(\n",
    "                model_name,\n",
    "                model_baseline.wordplay,\n",
    "                model_optimized.wordplay,\n",
    "                puzzle_type=\"wordplay\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sp_data = \"\\n\".join(text_sp)\n",
    "    wp_data = \"\\n\".join(text_wp)\n",
    "    label = f\"{label}-{'postprocessed' if postprocessed else 'raw'}\"\n",
    "    suffix = \"'Ori & Sem' means 'Original & Semantic' while 'O & S & C' means 'Original & Semantic & Context'. 'Overall' is the average of 'Ori', 'Sem', and 'Con'.\"\n",
    "    if not postprocessed:\n",
    "        suffix = (\n",
    "            suffix\n",
    "            + \" 'RAW' means that the accuracy values were calculated from raw model responses.\"\n",
    "        )\n",
    "    else:\n",
    "        suffix = (\n",
    "            suffix\n",
    "            + \" 'Postprocessed' means that the accuracy values were calculated from postprocessed model responses.\"\n",
    "        )\n",
    "    suffix = suffix.replace(\"&\", \"\\\\&\")\n",
    "    full_table = (\n",
    "        \"\"\"\n",
    "    \\\\begin{table}[hbp]\\r\\n\\\\caption[{caption}]{{caption}: {suffix}}\\r\\n\\\\label{tab:{label}}\\r\\n\\\\resizebox{\\\\textwidth}{!}{%\\r\\n\\\\begin{tabular}{ccccccc}\\r\\n\\\\cline{1-7}\\r\\n & \\\\multicolumn{3}{|c|}{Instance-based} & \\\\multicolumn{2}{c|}{Group-based} \\\\\\\\ \\\\cline{1-6}\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Model}} & \\\\textbf{Original} & \\\\textbf{Semantic} & \\\\multicolumn{1}{c|}{\\\\textbf{Context}} & \\\\textbf{Ori \\\\& Sem} & \\\\multicolumn{1}{c|}{\\\\textbf{O \\\\& S \\\\& C}} & \\\\textbf{Overall} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{7}{c}{Sentence Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 25.52 & 24.88 & \\\\multicolumn{1}{c|}{22.81} & 5.58 & \\\\multicolumn{1}{c|}{1.44} & 24.40 \\\\\\\\ \\\\hline\\r\\n{{sp_data}}\\r\\n\\\\hline\\r\\n\\\\multicolumn{7}{c}{Word Puzzle} \\\\\\\\ \\\\hline\\r\\n\\\\multicolumn{1}{c|}{\\\\textbf{Random*}} & 26.02 & 27.85 & \\\\multicolumn{1}{c|}{22.51} & 7.32 & \\\\multicolumn{1}{c|}{1.83} & 25.34 \\\\\\\\ \\\\hline\\r\\n{{wp_data}}\\r\\n\\\\\\\\ \\\\hline\\r\\n\\\\end{tabular}%\\r\\n}\\r\\n\\\\end{table}\n",
    "    \"\"\".replace(\"{{sp_data}}\", sp_data)\n",
    "        .replace(\"{{wp_data}}\", wp_data)\n",
    "        .replace(\n",
    "            \"{caption}\",\n",
    "            f\"{caption} ({'Postprocessed' if postprocessed else 'Raw'})\".replace(\n",
    "                \"&\", \"\\\\&\"\n",
    "            ),\n",
    "        )\n",
    "        .replace(\"{suffix}\", suffix)\n",
    "        .replace(\"{label}\", label)\n",
    "    )\n",
    "    output_table = Path(\"tables\") / f\"{label}.tex\"\n",
    "    if not output_table.parent.exists():\n",
    "        output_table.parent.mkdir(parents=True)\n",
    "    with open(output_table, \"w\") as f:\n",
    "        f.write(full_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_zero_shot = create_evaluation_from_results(\n",
    "    \"results/baseline-zero-shot-evaluation/baseline-zero-shot-evaluation_results.pkl\",\n",
    ")\n",
    "optimized_zero_shot = create_evaluation_from_results(\n",
    "    \"results/system-optimized-zero-shot-evaluation/system-optimized-zero-shot-evaluation_results.pkl\",\n",
    ")\n",
    "\n",
    "baseline_few_shot = create_evaluation_from_results(\n",
    "    \"results/baseline-few-shot-evaluation/baseline-few-shot-evaluation_results.pkl\",\n",
    ")\n",
    "optimized_few_shot = create_evaluation_from_results(\n",
    "    \"results/system-optimized-few-shot-evaluation/system-optimized-few-shot-evaluation_results.pkl\",\n",
    ")\n",
    "\n",
    "\n",
    "results_to_texttable(\n",
    "    baseline_zero_shot,\n",
    "    optimized_zero_shot,\n",
    "    \"Zero-Shot: Default vs. Optimized System Prompt\",\n",
    "    \"full-zero-shot\",\n",
    ")\n",
    "\n",
    "results_to_texttable(\n",
    "    baseline_few_shot,\n",
    "    optimized_few_shot,\n",
    "    \"Few-Shot: Default vs. Optimized System Prompt\",\n",
    "    \"full-few-shot\",\n",
    ")\n",
    "\n",
    "results_to_texttable(\n",
    "    optimized_zero_shot,\n",
    "    optimized_few_shot,\n",
    "    \"Optimized Zero-Shot vs. Optimized Few-Shot\",\n",
    "    \"optimized-zero-shot-vs-optimized-few-shot\",\n",
    ")\n",
    "\n",
    "results_to_texttable_hightliting(\n",
    "    baseline_zero_shot,\n",
    "    optimized_zero_shot,\n",
    "    \"Zero-Shot: Default vs. Optimized System Prompt\",\n",
    "    \"baseline-vs-optimized-zero-shot\",\n",
    "    postprocessed=False,\n",
    ")\n",
    "results_to_texttable_hightliting(\n",
    "    baseline_zero_shot,\n",
    "    baseline_few_shot,\n",
    "    \"Default System Prompt: Zero-Shot vs. Few-Shot\",\n",
    "    \"baseline-zero-shot-vs-baseline-few-shot\",\n",
    "    postprocessed=False,\n",
    ")\n",
    "\n",
    "results_to_texttable_hightliting(\n",
    "    baseline_few_shot,\n",
    "    optimized_few_shot,\n",
    "    \"Few-Shot: Default vs. Optimized System Prompt\",\n",
    "    \"baseline-vs-optimized-few-shot\",\n",
    "    postprocessed=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_to_texttable_hightliting(\n",
    "#     baseline_few_shot,\n",
    "#     optimized_few_shot,\n",
    "#     \"Baseline vs. Optimized Few-Shot\",\n",
    "#     \"baseline-vs-optimized-few-shot\",\n",
    "#     postprocessed=False,\n",
    "# )\n",
    "# results_to_texttable_hightliting(\n",
    "#     baseline_few_shot,\n",
    "#     optimized_few_shot,\n",
    "#     \"Baseline vs. Optimized Few-Shot\",\n",
    "#     \"baseline-vs-optimized-few-shot\",\n",
    "#     postprocessed=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def results_to_csv(baseline_results, optimized_results, filename, postprocessed=False):\n",
    "    \"\"\"\n",
    "    Export evaluation results to a CSV file with metrics as column headers.\n",
    "\n",
    "    Args:\n",
    "        baseline_results: Evaluation results for the baseline model\n",
    "        optimized_results: Evaluation results for the optimized model\n",
    "        filename: Base filename for the CSV\n",
    "        postprocessed: Whether to use postprocessed results (True) or raw results (False)\n",
    "    \"\"\"\n",
    "\n",
    "    data_sp: dict[str, dict[str, float]] = {}\n",
    "    data_wp: dict[str, dict[str, float]] = {}\n",
    "\n",
    "    columns = []\n",
    "\n",
    "    for model_name, results in baseline_results.items():\n",
    "        baseline_results_model: ModelEvaluationMetrics = (\n",
    "            results.raw_results if not postprocessed else results.postprocessed_results\n",
    "        )\n",
    "        optimized_results_model: ModelEvaluationMetrics = (\n",
    "            optimized_results[model_name].raw_results\n",
    "            if not postprocessed\n",
    "            else optimized_results[model_name].postprocessed_results\n",
    "        )\n",
    "        sp = {\n",
    "            \"model_name\": model_name,\n",
    "            \"original_accuracy_baseline\": baseline_results_model.sentence.original_accuracy,\n",
    "            \"original_accuracy_optimized\": optimized_results_model.sentence.original_accuracy,\n",
    "            \"original_accuracy_diff\": optimized_results_model.sentence.original_accuracy\n",
    "            - baseline_results_model.sentence.original_accuracy,\n",
    "            \"semantic_accuracy_baseline\": baseline_results_model.sentence.semantic_accuracy,\n",
    "            \"semantic_accuracy_optimized\": optimized_results_model.sentence.semantic_accuracy,\n",
    "            \"semantic_accuracy_diff\": optimized_results_model.sentence.semantic_accuracy\n",
    "            - baseline_results_model.sentence.semantic_accuracy,\n",
    "            \"context_accuracy_baseline\": baseline_results_model.sentence.context_accuracy,\n",
    "            \"context_accuracy_optimized\": optimized_results_model.sentence.context_accuracy,\n",
    "            \"context_accuracy_diff\": optimized_results_model.sentence.context_accuracy\n",
    "            - baseline_results_model.sentence.context_accuracy,\n",
    "            \"ori_sema_baseline\": baseline_results_model.sentence.ori_sema,\n",
    "            \"ori_sema_optimized\": optimized_results_model.sentence.ori_sema,\n",
    "            \"ori_sema_diff\": optimized_results_model.sentence.ori_sema\n",
    "            - baseline_results_model.sentence.ori_sema,\n",
    "            \"ori_sema_cont_baseline\": baseline_results_model.sentence.ori_sema_cont,\n",
    "            \"ori_sema_cont_optimized\": optimized_results_model.sentence.ori_sema_cont,\n",
    "            \"ori_sema_cont_diff\": optimized_results_model.sentence.ori_sema_cont\n",
    "            - baseline_results_model.sentence.ori_sema_cont,\n",
    "            \"overall_accuracy_baseline\": baseline_results_model.sentence.overall_accuracy,\n",
    "            \"overall_accuracy_optimized\": optimized_results_model.sentence.overall_accuracy,\n",
    "            \"overall_accuracy_diff\": optimized_results_model.sentence.overall_accuracy\n",
    "            - baseline_results_model.sentence.overall_accuracy,\n",
    "        }\n",
    "\n",
    "        columns = sp.keys()\n",
    "        wp = {\n",
    "            \"model_name\": model_name,\n",
    "            \"original_accuracy_baseline\": baseline_results_model.wordplay.original_accuracy,\n",
    "            \"original_accuracy_optimized\": optimized_results_model.wordplay.original_accuracy,\n",
    "            \"original_accuracy_diff\": optimized_results_model.wordplay.original_accuracy\n",
    "            - baseline_results_model.wordplay.original_accuracy,\n",
    "            \"semantic_accuracy_baseline\": baseline_results_model.wordplay.semantic_accuracy,\n",
    "            \"semantic_accuracy_optimized\": optimized_results_model.wordplay.semantic_accuracy,\n",
    "            \"semantic_accuracy_diff\": optimized_results_model.wordplay.semantic_accuracy\n",
    "            - baseline_results_model.wordplay.semantic_accuracy,\n",
    "            \"context_accuracy_baseline\": baseline_results_model.wordplay.context_accuracy,\n",
    "            \"context_accuracy_optimized\": optimized_results_model.wordplay.context_accuracy,\n",
    "            \"context_accuracy_diff\": optimized_results_model.wordplay.context_accuracy\n",
    "            - baseline_results_model.wordplay.context_accuracy,\n",
    "            \"ori_sema_baseline\": baseline_results_model.wordplay.ori_sema,\n",
    "            \"ori_sema_optimized\": optimized_results_model.wordplay.ori_sema,\n",
    "            \"ori_sema_diff\": optimized_results_model.wordplay.ori_sema\n",
    "            - baseline_results_model.wordplay.ori_sema,\n",
    "            \"ori_sema_cont_baseline\": baseline_results_model.wordplay.ori_sema_cont,\n",
    "            \"ori_sema_cont_optimized\": optimized_results_model.wordplay.ori_sema_cont,\n",
    "            \"ori_sema_cont_diff\": optimized_results_model.wordplay.ori_sema_cont\n",
    "            - baseline_results_model.wordplay.ori_sema_cont,\n",
    "            \"overall_accuracy_baseline\": baseline_results_model.wordplay.overall_accuracy,\n",
    "            \"overall_accuracy_optimized\": optimized_results_model.wordplay.overall_accuracy,\n",
    "            \"overall_accuracy_diff\": optimized_results_model.wordplay.overall_accuracy\n",
    "            - baseline_results_model.wordplay.overall_accuracy,\n",
    "        }\n",
    "\n",
    "        data_sp[model_name] = sp.values()\n",
    "        data_wp[model_name] = wp.values()\n",
    "\n",
    "    combined_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame.from_dict(data_sp, orient=\"index\", columns=columns),\n",
    "            pd.DataFrame.from_dict(data_wp, orient=\"index\", columns=columns),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    combined_data.to_csv(\n",
    "        f\"csvs/{filename}-{'postprocessed' if postprocessed else 'raw'}.csv\",\n",
    "        index=False,\n",
    "        float_format=\"%.2f\",\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "\n",
    "def results_to_diffs_csv(baseline_results, optimized_results, filename):\n",
    "    \"\"\"\n",
    "    Export evaluation results to a CSV file with metrics as column headers.\n",
    "\n",
    "    Args:\n",
    "        baseline_results: Evaluation results for the baseline model\n",
    "        optimized_results: Evaluation results for the optimized model\n",
    "        filename: Base filename for the CSV\n",
    "        postprocessed: Whether to use postprocessed results (True) or raw results (False)\n",
    "    \"\"\"\n",
    "\n",
    "    data_sp: dict[str, dict[str, float]] = {}\n",
    "    data_wp: dict[str, dict[str, float]] = {}\n",
    "\n",
    "    columns = []\n",
    "\n",
    "    for model_name, results in baseline_results.items():\n",
    "        baseline_results_model_raw: ModelEvaluationMetrics = results.raw_results\n",
    "        optimized_results_model_raw: ModelEvaluationMetrics = optimized_results[\n",
    "            model_name\n",
    "        ].raw_results\n",
    "        baseline_results_model_postprocessed: ModelEvaluationMetrics = (\n",
    "            results.postprocessed_results\n",
    "        )\n",
    "        optimized_results_model_postprocessed: ModelEvaluationMetrics = (\n",
    "            optimized_results[model_name].postprocessed_results\n",
    "        )\n",
    "\n",
    "        sp = {\n",
    "            \"model_name\": model_name,\n",
    "            \"original_accuracy_diff_raw\": optimized_results_model_raw.sentence.original_accuracy\n",
    "            - baseline_results_model_raw.sentence.original_accuracy,\n",
    "            \"semantic_accuracy_diff_raw\": optimized_results_model_raw.sentence.semantic_accuracy\n",
    "            - baseline_results_model_raw.sentence.semantic_accuracy,\n",
    "            \"context_accuracy_diff_raw\": optimized_results_model_raw.sentence.context_accuracy\n",
    "            - baseline_results_model_raw.sentence.context_accuracy,\n",
    "            \"ori_sema_diff_raw\": optimized_results_model_raw.sentence.ori_sema\n",
    "            - baseline_results_model_raw.sentence.ori_sema,\n",
    "            \"ori_sema_cont_diff_raw\": optimized_results_model_raw.sentence.ori_sema_cont\n",
    "            - baseline_results_model_raw.sentence.ori_sema_cont,\n",
    "            \"overall_accuracy_diff_raw\": optimized_results_model_raw.sentence.overall_accuracy\n",
    "            - baseline_results_model_raw.sentence.overall_accuracy,\n",
    "            \"original_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.sentence.original_accuracy\n",
    "            - baseline_results_model_postprocessed.sentence.original_accuracy,\n",
    "            \"semantic_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.sentence.semantic_accuracy\n",
    "            - baseline_results_model_postprocessed.sentence.semantic_accuracy,\n",
    "            \"context_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.sentence.context_accuracy\n",
    "            - baseline_results_model_postprocessed.sentence.context_accuracy,\n",
    "            \"ori_sema_diff_postprocessed\": optimized_results_model_postprocessed.sentence.ori_sema\n",
    "            - baseline_results_model_postprocessed.sentence.ori_sema,\n",
    "            \"ori_sema_cont_diff_postprocessed\": optimized_results_model_postprocessed.sentence.ori_sema_cont\n",
    "            - baseline_results_model_postprocessed.sentence.ori_sema_cont,\n",
    "            \"overall_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.sentence.overall_accuracy\n",
    "            - baseline_results_model_postprocessed.sentence.overall_accuracy,\n",
    "        }\n",
    "\n",
    "        columns = sp.keys()\n",
    "        wp = {\n",
    "            \"model_name\": model_name,\n",
    "            \"original_accuracy_diff_raw\": optimized_results_model_raw.wordplay.original_accuracy\n",
    "            - baseline_results_model_raw.wordplay.original_accuracy,\n",
    "            \"semantic_accuracy_diff_raw\": optimized_results_model_raw.wordplay.semantic_accuracy\n",
    "            - baseline_results_model_raw.wordplay.semantic_accuracy,\n",
    "            \"context_accuracy_diff_raw\": optimized_results_model_raw.wordplay.context_accuracy\n",
    "            - baseline_results_model_raw.wordplay.context_accuracy,\n",
    "            \"ori_sema_diff_raw\": optimized_results_model_raw.wordplay.ori_sema\n",
    "            - baseline_results_model_raw.wordplay.ori_sema,\n",
    "            \"ori_sema_cont_diff_raw\": optimized_results_model_raw.wordplay.ori_sema_cont\n",
    "            - baseline_results_model_raw.wordplay.ori_sema_cont,\n",
    "            \"overall_accuracy_diff_raw\": optimized_results_model_raw.wordplay.overall_accuracy\n",
    "            - baseline_results_model_raw.wordplay.overall_accuracy,\n",
    "            \"original_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.original_accuracy\n",
    "            - baseline_results_model_postprocessed.wordplay.original_accuracy,\n",
    "            \"semantic_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.semantic_accuracy\n",
    "            - baseline_results_model_postprocessed.wordplay.semantic_accuracy,\n",
    "            \"context_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.context_accuracy\n",
    "            - baseline_results_model_postprocessed.wordplay.context_accuracy,\n",
    "            \"ori_sema_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.ori_sema\n",
    "            - baseline_results_model_postprocessed.wordplay.ori_sema,\n",
    "            \"ori_sema_cont_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.ori_sema_cont\n",
    "            - baseline_results_model_postprocessed.wordplay.ori_sema_cont,\n",
    "            \"overall_accuracy_diff_postprocessed\": optimized_results_model_postprocessed.wordplay.overall_accuracy\n",
    "            - baseline_results_model_postprocessed.wordplay.overall_accuracy,\n",
    "        }\n",
    "\n",
    "        data_sp[model_name] = sp.values()\n",
    "        data_wp[model_name] = wp.values()\n",
    "\n",
    "    combined_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame.from_dict(data_sp, orient=\"index\", columns=columns),\n",
    "            pd.DataFrame.from_dict(data_wp, orient=\"index\", columns=columns),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    combined_data.to_csv(\n",
    "        f\"csvs/{filename}-diffs.csv\",\n",
    "        index=False,\n",
    "        float_format=\"%.2f\",\n",
    "        sep=\";\",\n",
    "    )\n",
    "\n",
    "\n",
    "results_to_diffs_csv(\n",
    "    baseline_zero_shot,\n",
    "    optimized_zero_shot,\n",
    "    \"baseline-vs-optimized-zero-shot\",\n",
    ")\n",
    "\n",
    "results_to_diffs_csv(\n",
    "    baseline_zero_shot,\n",
    "    baseline_few_shot,\n",
    "    \"baseline-zero-shot-vs-baseline-few-shot\",\n",
    ")\n",
    "\n",
    "results_to_diffs_csv(\n",
    "    optimized_zero_shot,\n",
    "    baseline_zero_shot,\n",
    "    \"optimized-zero-shot-vs-baseline-zero-shot\",\n",
    ")\n",
    "\n",
    "results_to_diffs_csv(\n",
    "    optimized_zero_shot,\n",
    "    optimized_few_shot,\n",
    "    \"optimized-zero-shot-vs-optimized-few-shot\",\n",
    ")\n",
    "\n",
    "\n",
    "results_to_diffs_csv(\n",
    "    baseline_few_shot,\n",
    "    optimized_few_shot,\n",
    "    \"baseline-vs-optimized-few-shot\",\n",
    ")\n",
    "\n",
    "\n",
    "results_to_csv(\n",
    "    baseline_zero_shot,\n",
    "    baseline_few_shot,\n",
    "    \"full-zero-shot-vs-full-few-shot\",\n",
    "    postprocessed=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
