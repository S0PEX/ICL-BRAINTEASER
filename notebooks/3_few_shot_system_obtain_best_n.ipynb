{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "from scripts.prompt_helpers import create_prompt_template\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "        \"answer\": string.ascii_uppercase[riddle_question.label],\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:20,637 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://50.173.30.254:40106\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b\"),\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b\"),\n",
    "        model_builder.build_model(\"llama3.2:3b\"),\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b\"),\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b\"),\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:1.5b\"),\n",
    "        model_builder.build_model(\"qwen2.5:3b\"),\n",
    "        model_builder.build_model(\"qwen2.5:7b\"),\n",
    "        model_builder.build_model(\"qwen2.5:14b\"),\n",
    "        model_builder.build_model(\"qwen2.5:32b\"),\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b\"),\n",
    "        model_builder.build_model(\"gemma2:9b\"),\n",
    "        model_builder.build_model(\"gemma2:27b\"),\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scripts.executor import Dataset\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "maximal_n = 8\n",
    "\n",
    "\n",
    "def create_test_dataset(\n",
    "    data: list[RiddleQuestion],\n",
    "    name: str,\n",
    "    percentage: float = 0.1,\n",
    "    example_count: int = 10,\n",
    ") -> tuple[list[RiddleQuestion], Dataset]:\n",
    "    \"\"\"\n",
    "    Create a test dataset by randomly sampling a percentage of the original data.\n",
    "    Also returns examples for few-shot learning with diverse answers.\n",
    "\n",
    "    Args:\n",
    "        data: List of riddle questions\n",
    "        name: Name of the dataset\n",
    "        percentage: Percentage of data to use for testing\n",
    "        example_count: Number of examples to use for few-shot learning\n",
    "\n",
    "    Returns:\n",
    "        tuple: (examples for few-shot learning, test dataset)\n",
    "    \"\"\"\n",
    "    # Group data by answer choice\n",
    "    answer_groups = {}\n",
    "    for i, question in enumerate(data):\n",
    "        answer = question.label\n",
    "        if answer not in answer_groups:\n",
    "            answer_groups[answer] = []\n",
    "        answer_groups[answer].append(i)\n",
    "\n",
    "    # Select diverse examples for few-shot learning\n",
    "    example_indices = []\n",
    "    answers = list(answer_groups.keys())\n",
    "\n",
    "    # Distribute examples evenly across answer choices\n",
    "    while len(example_indices) < example_count and answers:\n",
    "        for answer in list(answers):  # Use a copy to safely modify during iteration\n",
    "            if answer_groups[answer]:\n",
    "                example_indices.append(answer_groups[answer].pop(0))\n",
    "                if len(example_indices) >= example_count:\n",
    "                    break\n",
    "            else:\n",
    "                answers.remove(answer)\n",
    "\n",
    "        # If we don't have enough examples yet and ran out of diverse answers,\n",
    "        # just add remaining from whatever is available\n",
    "        if len(example_indices) < example_count and not any(answer_groups.values()):\n",
    "            break\n",
    "\n",
    "    # If we still need more examples, take from the beginning\n",
    "    if len(example_indices) < example_count:\n",
    "        remaining_indices = [i for i in range(len(data)) if i not in example_indices]\n",
    "        example_indices.extend(\n",
    "            remaining_indices[: example_count - len(example_indices)]\n",
    "        )\n",
    "\n",
    "    examples = [data[i] for i in sorted(example_indices[:example_count])]\n",
    "\n",
    "    # Sample from the remaining data for testing\n",
    "    remaining_indices = [\n",
    "        i for i in range(len(data)) if i not in example_indices[:example_count]\n",
    "    ]\n",
    "    remaining_data = [data[i] for i in remaining_indices]\n",
    "\n",
    "    indices = np.random.choice(\n",
    "        len(remaining_data), size=int(len(remaining_data) * percentage), replace=False\n",
    "    )\n",
    "    test_dataset = Dataset(name=name, riddles=[remaining_data[i] for i in indices])\n",
    "\n",
    "    return examples, test_dataset\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "sp_examples, sp_data = create_test_dataset(dataset.sp, \"sp\", example_count=maximal_n)\n",
    "wp_examples, wp_data = create_test_dataset(dataset.wp, \"wp\", example_count=maximal_n)\n",
    "\n",
    "# Prepare executor data\n",
    "executor_data = [sp_data, wp_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import dill as pickle\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from scripts.prompt_helpers import TemplateNameType, get_few_shot_chat_template\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "with open(\"results/best_system_prompts_by_model.pkl\", \"rb\") as f:\n",
    "    best_prompt_types = pickle.load(f)\n",
    "\n",
    "\n",
    "def few_shot_prompt_template_generator(\n",
    "    model_name: str, dataset: Dataset, number_of_shots: int\n",
    ") -> Callable[[str], ChatPromptTemplate]:\n",
    "    if dataset.name == \"sp\":\n",
    "        few_shot_examples = sp_examples\n",
    "    elif dataset.name == \"wp\":\n",
    "        few_shot_examples = wp_examples\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset.name}\")\n",
    "\n",
    "    best_system_template_name: TemplateNameType = best_prompt_types[model_name][\n",
    "        dataset.name\n",
    "    ][\"prompt_type\"]\n",
    "\n",
    "    template = get_few_shot_chat_template(\n",
    "        few_shot_examples,\n",
    "        args_generator,\n",
    "        best_system_template_name,\n",
    "        number_of_shots,\n",
    "    )\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:20,721 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=1'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff540f145bc84c07b2cddcb0b7fadd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-1):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:20,776 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=2'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6cb22d8ed945de991843822f7d8c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-2):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:20,829 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=3'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101db2721744bf1ba9f5d1c36ea9e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-3):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:21,331 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=4'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f0c38545734f8c88f688652805a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-4):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:21,433 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=5'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc7953e951b4ae4befcefcff973ea47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-5):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:21,988 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=6'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07182d565ed4e469d83f84bcf176c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-6):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:22,097 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=7'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c3e97fde0d4a3ca9b7c1396bdf23db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-7):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:20:22,692 - INFO - Starting execution 'few-shot-obtain-best-n-system-prompt with suffix 'n=8'': 2 dataset(s) x 15 model(s) = 1635 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4501822812d944638686a487a0987848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "few-shot-obtain-best-n-system-prompt(n-8):   0%|          | 0/1635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, maximal_n + 1):\n",
    "    results = await executor.aexecute(\n",
    "        executor_data,\n",
    "        lambda model_name,\n",
    "        dataset,\n",
    "        number_of_shots=i: few_shot_prompt_template_generator(\n",
    "            model_name, dataset, number_of_shots\n",
    "        ),\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        run_name=\"few-shot-obtain-best-n-system-prompt\",\n",
    "        file_name_suffix=f\"n={i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 result sets from disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the results directory path\n",
    "results_dir = Path(\"results/few-shot-obtain-best-n-system-prompt\")\n",
    "\n",
    "# Get all result files\n",
    "result_files = glob.glob(str(results_dir / \"few-shot-obtain-best-n*_n-*_results.pkl\"))\n",
    "\n",
    "# Load all results into a dictionary\n",
    "# The first key is the suffix (technique name)\n",
    "total_results = {}\n",
    "\n",
    "for file_path in result_files:\n",
    "    # Extract the suffix from the filename\n",
    "    suffix = os.path.basename(file_path).split(\"_\")[1]\n",
    "\n",
    "    # Load the results from the pickle file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        wrapped_results = pickle.load(f)\n",
    "        total_results[suffix] = wrapped_results.results\n",
    "\n",
    "print(f\"Loaded {len(total_results)} result sets from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best n value for each model:\n",
      "llama3.1:8b:\n",
      "  - sp: n=4\n",
      "  - wp: n=1\n",
      "llama3.2:1b:\n",
      "  - sp: n=7\n",
      "  - wp: n=5\n",
      "llama3.2:3b:\n",
      "  - sp: n=8\n",
      "  - wp: n=8\n",
      "phi3.5:3.8b:\n",
      "  - sp: n=4\n",
      "  - wp: n=8\n",
      "phi4:14b:\n",
      "  - sp: n=6\n",
      "  - wp: n=3\n",
      "qwen2.5:0.5b:\n",
      "  - sp: n=6\n",
      "  - wp: n=6\n",
      "qwen2.5:1.5b:\n",
      "  - sp: n=1\n",
      "  - wp: n=2\n",
      "qwen2.5:3b:\n",
      "  - sp: n=5\n",
      "  - wp: n=2\n",
      "qwen2.5:7b:\n",
      "  - sp: n=5\n",
      "  - wp: n=8\n",
      "qwen2.5:14b:\n",
      "  - sp: n=4\n",
      "  - wp: n=4\n",
      "qwen2.5:32b:\n",
      "  - sp: n=3\n",
      "  - wp: n=3\n",
      "gemma2:2b:\n",
      "  - sp: n=8\n",
      "  - wp: n=6\n",
      "gemma2:9b:\n",
      "  - sp: n=1\n",
      "  - wp: n=1\n",
      "gemma2:27b:\n",
      "  - sp: n=5\n",
      "  - wp: n=6\n",
      "mistral-nemo:12b:\n",
      "  - sp: n=1\n",
      "  - wp: n=2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scripts.evaluation import calculate_model_accuracy\n",
    "\n",
    "\n",
    "def get_best_prompt_for_each_model(input_data):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            # For each model, we need to track its best score\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Assume eval_results returns a score based on the result data\n",
    "                score_percentage_raw, _, score_percentage_postprocessed, _ = (\n",
    "                    calculate_model_accuracy(result)\n",
    "                )\n",
    "\n",
    "                score = score_percentage_raw\n",
    "                # If this model doesn't have a best score for this dataset yet or if the current score is better\n",
    "                if (\n",
    "                    dataset_type not in best_prompts[model]\n",
    "                    or score > best_prompts[model][dataset_type][\"score\"]\n",
    "                ):\n",
    "                    best_prompts[model][dataset_type] = {\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "\n",
    "    # Now best_prompts contains the best prompt type for each model and dataset\n",
    "    return best_prompts\n",
    "\n",
    "\n",
    "def get_best_n_prompts_for_each_model(input_data, n=5):\n",
    "    best_prompts = {}\n",
    "\n",
    "    # Iterate through each model\n",
    "    for prompt_type, datasets in input_data.items():\n",
    "        for dataset_type, models in datasets.items():\n",
    "            for model, result in models.items():\n",
    "                # Initialize the best prompt data structure for this model if not yet created\n",
    "                if model not in best_prompts:\n",
    "                    best_prompts[model] = {}\n",
    "\n",
    "                # Calculate the score for the model with the current prompt type and dataset\n",
    "                score_percentage_raw, _, score_percentage_postprocessed, _ = (\n",
    "                    calculate_model_accuracy(result)\n",
    "                )\n",
    "                score = score_percentage_raw\n",
    "\n",
    "                # Initialize the list of prompts for this model and dataset type if not created\n",
    "                if dataset_type not in best_prompts[model]:\n",
    "                    best_prompts[model][dataset_type] = []\n",
    "\n",
    "                # Append the prompt type, score, and length to the list\n",
    "                best_prompts[model][dataset_type].append(\n",
    "                    {\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Now sort the list of prompts for each model and dataset type and keep the top n\n",
    "    top_n_prompts = {}\n",
    "    for model, dataset_dict in best_prompts.items():\n",
    "        top_n_prompts_for_model = {}\n",
    "        for dataset_type, prompts in dataset_dict.items():\n",
    "            # First, sort all prompts by score (highest first)\n",
    "            all_sorted_prompts = sorted(prompts, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "            # Apply a penalty to higher n values when scores are close\n",
    "            # This will reorder prompts to prefer lower n values when scores are within threshold\n",
    "            penalized_prompts = []\n",
    "            for prompt in all_sorted_prompts:\n",
    "                n_value = int(prompt[\"prompt_type\"].split(\"-\")[1])\n",
    "                # Calculate a penalized score that favors lower n values when scores are close\n",
    "                # If two scores are within 3%, each additional n point reduces score by 0.5%\n",
    "                penalized_score = prompt[\"score\"]\n",
    "\n",
    "                # Compare with all better-scoring prompts\n",
    "                for better_prompt in all_sorted_prompts:\n",
    "                    if better_prompt[\"score\"] <= prompt[\"score\"]:\n",
    "                        continue\n",
    "\n",
    "                    better_n = int(better_prompt[\"prompt_type\"].split(\"-\")[1])\n",
    "                    score_diff = better_prompt[\"score\"] - prompt[\"score\"]\n",
    "\n",
    "                    # If the score difference is small but n is significantly smaller\n",
    "                    if score_diff <= 3.0 and n_value > better_n:\n",
    "                        # Boost the score of the smaller n value prompt\n",
    "                        penalized_score = better_prompt[\"score\"] + 2.0\n",
    "                        break\n",
    "\n",
    "                penalized_prompts.append(\n",
    "                    {\n",
    "                        \"prompt_type\": prompt[\"prompt_type\"],\n",
    "                        \"score\": prompt[\"score\"],\n",
    "                        \"penalized_score\": penalized_score,\n",
    "                        \"n_value\": n_value,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Sort by penalized score first, then by original score, then by lower n value\n",
    "            final_sorted_prompts = sorted(\n",
    "                penalized_prompts,\n",
    "                key=lambda x: (x[\"penalized_score\"], x[\"score\"], -x[\"n_value\"]),\n",
    "                reverse=True,\n",
    "            )\n",
    "\n",
    "            # Take the top n prompts after reordering\n",
    "            top_prompts = []\n",
    "            for p in final_sorted_prompts[:n]:\n",
    "                top_prompts.append(\n",
    "                    {\"prompt_type\": p[\"prompt_type\"], \"score\": p[\"score\"]}\n",
    "                )\n",
    "\n",
    "            # Make sure the final results are sorted by highest score on top\n",
    "            top_prompts = sorted(top_prompts, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "            top_n_prompts_for_model[dataset_type] = top_prompts\n",
    "        top_n_prompts[model] = top_n_prompts_for_model\n",
    "\n",
    "    return top_n_prompts\n",
    "\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "best_prompt_types = get_best_n_prompts_for_each_model(total_results, n=4)\n",
    "\n",
    "# Print the results as a formatted table using pandas\n",
    "\n",
    "# for model, dataset_dict in best_prompt_types.items():\n",
    "#     print(f\"\\n{'-' * 80}\\nModel: {model}\")\n",
    "#     for dataset_type, prompts in dataset_dict.items():\n",
    "#         print(f\"\\nDataset: {dataset_type}\")\n",
    "\n",
    "#         # Create a DataFrame from the prompts data\n",
    "#         df = pd.DataFrame(prompts)\n",
    "\n",
    "#         # Rename columns for better display\n",
    "#         df = df.rename(\n",
    "#             columns={\n",
    "#                 \"prompt_type\": \"Prompt Type\",\n",
    "#                 \"score\": \"Score\",\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Format the score column to 4 decimal places\n",
    "#         df[\"Score\"] = df[\"Score\"].map(\"{:.4f}\".format)\n",
    "\n",
    "#         # Display the DataFrame\n",
    "#         display(df)\n",
    "\n",
    "# Extract the best prompt type (highest score) for each model and dataset\n",
    "best_n_value_by_model = {}\n",
    "\n",
    "for model, dataset_dict in best_prompt_types.items():\n",
    "    best_n_value_by_model[model] = {}\n",
    "\n",
    "    for dataset_name, prompts in dataset_dict.items():\n",
    "        if prompts:\n",
    "            # Get the prompt type with the highest score\n",
    "            best_prompt_type = prompts[0][\"prompt_type\"]\n",
    "\n",
    "            # Extract the n value from the prompt type (format is \"n-X\")\n",
    "            n_value = int(best_prompt_type.split(\"-\")[1])\n",
    "\n",
    "            # Store in our dictionary with dataset as key and n as value\n",
    "            best_n_value_by_model[model][dataset_name] = n_value\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nBest n value for each model:\")\n",
    "for model, dataset_dict in best_n_value_by_model.items():\n",
    "    print(f\"{model}:\")\n",
    "    for dataset_name, n_value in dataset_dict.items():\n",
    "        print(f\"  - {dataset_name}: n={n_value}\")\n",
    "\n",
    "# Save the best prompt types\n",
    "with open(\"results/best_n_value_by_model_system-prompt.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_n_value_by_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
