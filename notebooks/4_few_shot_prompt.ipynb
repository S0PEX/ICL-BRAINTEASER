{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "from scripts.prompt_helpers import create_prompt_template\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "        \"answer\": string.ascii_uppercase[riddle_question.label],\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:50:42,736 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://50.173.30.254:40053\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b-instruct-q8_0\"),  # => 9 GB\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b-instruct-fp16\"),  # => 2.5 GB\n",
    "        model_builder.build_model(\"llama3.2:3b-instruct-fp16\"),  # => 6.4 GB\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b-mini-instruct-fp16\"),  # => 7.6 GB\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b-q8_0\"),  # => 16 GB\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b-instruct-fp16\"),  # => 1 GB\n",
    "        model_builder.build_model(\"qwen2.5:1.5b-instruct-fp16\"),  # => 3.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:3b-instruct-fp16\"),  # => 6.2 GB\n",
    "        model_builder.build_model(\"qwen2.5:7b-instruct-q8_0\"),  # => 8.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:14b-instruct-q8_0\"),  # => 16 GB\n",
    "        model_builder.build_model(\"qwen2.5:32b-instruct-q4_K_M\"),  # => 20 GB\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b-instruct-fp16\"),  # => 5.2 GB\n",
    "        model_builder.build_model(\"gemma2:9b-instruct-q8_0\"),  # => 9.8 GB\n",
    "        model_builder.build_model(\"gemma2:27b-instruct-q4_K_M\"),  # => 22 GB\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b-instruct-2407-q8_0\"),  # => 13 GB\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scripts.executor import Dataset\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "maximal_n = 8\n",
    "\n",
    "\n",
    "def create_n_shot_dataset(\n",
    "    data: list[RiddleQuestion],\n",
    "    name: str,\n",
    "    example_count: int = 10,\n",
    ") -> tuple[list[RiddleQuestion], Dataset]:\n",
    "    \"\"\"\n",
    "    Create a few-shot learning dataset by selecting diverse examples and the remaining data for testing.\n",
    "\n",
    "    Args:\n",
    "        data: List of riddle questions\n",
    "        name: Name of the dataset\n",
    "        example_count: Number of examples to use for few-shot learning\n",
    "\n",
    "    Returns:\n",
    "        tuple: (examples for few-shot learning, remaining dataset for testing)\n",
    "    \"\"\"\n",
    "    # Group data by answer choice\n",
    "    answer_groups = {}\n",
    "    for i, question in enumerate(data):\n",
    "        answer = question.label\n",
    "        if answer not in answer_groups:\n",
    "            answer_groups[answer] = []\n",
    "        answer_groups[answer].append(i)\n",
    "\n",
    "    # Select diverse examples for few-shot learning\n",
    "    example_indices = []\n",
    "    answers = list(answer_groups.keys())\n",
    "\n",
    "    # Distribute examples evenly across answer choices\n",
    "    while len(example_indices) < example_count and answers:\n",
    "        for answer in list(answers):  # Use a copy to safely modify during iteration\n",
    "            if answer_groups[answer]:\n",
    "                example_indices.append(answer_groups[answer].pop(0))\n",
    "                if len(example_indices) >= example_count:\n",
    "                    break\n",
    "            else:\n",
    "                answers.remove(answer)\n",
    "\n",
    "    # If we still need more examples, take randomly from remaining data\n",
    "    if len(example_indices) < example_count:\n",
    "        remaining_indices = [i for i in range(len(data)) if i not in example_indices]\n",
    "        np.random.shuffle(remaining_indices)\n",
    "        example_indices.extend(\n",
    "            remaining_indices[: example_count - len(example_indices)]\n",
    "        )\n",
    "\n",
    "    # Get the examples\n",
    "    examples = [data[i] for i in sorted(example_indices[:example_count])]\n",
    "\n",
    "    # Create dataset from all remaining data (not used as examples)\n",
    "    remaining_indices = [i for i in range(len(data)) if i not in example_indices]\n",
    "    test_dataset = Dataset(name=name, riddles=[data[i] for i in remaining_indices])\n",
    "\n",
    "    return examples, test_dataset\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "sp_examples, sp_data = create_n_shot_dataset(dataset.sp, \"sp\", example_count=maximal_n)\n",
    "wp_examples, wp_data = create_n_shot_dataset(dataset.wp, \"wp\", example_count=maximal_n)\n",
    "\n",
    "# Prepare executor data\n",
    "executor_data = [sp_data, wp_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Default Prompt N</th>\n",
       "      <th>Optimized System Prompt N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2:27b</td>\n",
       "      <td>sp</td>\n",
       "      <td>2</td>\n",
       "      <td>7 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2:27b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>6 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2:2b</td>\n",
       "      <td>sp</td>\n",
       "      <td>1</td>\n",
       "      <td>3 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma2:2b</td>\n",
       "      <td>wp</td>\n",
       "      <td>7</td>\n",
       "      <td>6 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>sp</td>\n",
       "      <td>2</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>wp</td>\n",
       "      <td>3</td>\n",
       "      <td>2 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>sp</td>\n",
       "      <td>3</td>\n",
       "      <td>4 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>wp</td>\n",
       "      <td>4</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>sp</td>\n",
       "      <td>8</td>\n",
       "      <td>7 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3.2:1b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>5 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama3.2:3b</td>\n",
       "      <td>sp</td>\n",
       "      <td>2</td>\n",
       "      <td>8 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama3.2:3b</td>\n",
       "      <td>wp</td>\n",
       "      <td>5</td>\n",
       "      <td>8 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>8 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phi3.5:3.8b</td>\n",
       "      <td>sp</td>\n",
       "      <td>7</td>\n",
       "      <td>4 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phi3.5:3.8b</td>\n",
       "      <td>wp</td>\n",
       "      <td>4</td>\n",
       "      <td>7 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phi4:14b</td>\n",
       "      <td>sp</td>\n",
       "      <td>3</td>\n",
       "      <td>6 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phi4:14b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>3 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2.5:0.5b</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>6 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2.5:0.5b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>6 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>sp</td>\n",
       "      <td>7</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>wp</td>\n",
       "      <td>2</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>qwen2.5:14b</td>\n",
       "      <td>sp</td>\n",
       "      <td>3</td>\n",
       "      <td>4 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>qwen2.5:14b</td>\n",
       "      <td>wp</td>\n",
       "      <td>7</td>\n",
       "      <td>4 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>qwen2.5:32b</td>\n",
       "      <td>sp</td>\n",
       "      <td>7</td>\n",
       "      <td>3 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>qwen2.5:32b</td>\n",
       "      <td>wp</td>\n",
       "      <td>7</td>\n",
       "      <td>3 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>3 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>wp</td>\n",
       "      <td>8</td>\n",
       "      <td>1 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>qwen2.5:7b</td>\n",
       "      <td>sp</td>\n",
       "      <td>8</td>\n",
       "      <td>5 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>qwen2.5:7b</td>\n",
       "      <td>wp</td>\n",
       "      <td>7</td>\n",
       "      <td>8 ↑</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Dataset  Default Prompt N Optimized System Prompt N\n",
       "0         gemma2:27b      sp                 2                       7 ↑\n",
       "1         gemma2:27b      wp                 8                       6 ↓\n",
       "2          gemma2:2b      sp                 1                       3 ↑\n",
       "3          gemma2:2b      wp                 7                       6 ↓\n",
       "4          gemma2:9b      sp                 2                       1 ↓\n",
       "5          gemma2:9b      wp                 3                       2 ↓\n",
       "6        llama3.1:8b      sp                 3                       4 ↑\n",
       "7        llama3.1:8b      wp                 4                       1 ↓\n",
       "8        llama3.2:1b      sp                 8                       7 ↓\n",
       "9        llama3.2:1b      wp                 8                       5 ↓\n",
       "10       llama3.2:3b      sp                 2                       8 ↑\n",
       "11       llama3.2:3b      wp                 5                       8 ↑\n",
       "12  mistral-nemo:12b      sp                 5                       8 ↑\n",
       "13  mistral-nemo:12b      wp                 8                       1 ↓\n",
       "14       phi3.5:3.8b      sp                 7                       4 ↓\n",
       "15       phi3.5:3.8b      wp                 4                       7 ↑\n",
       "16          phi4:14b      sp                 3                       6 ↑\n",
       "17          phi4:14b      wp                 8                       3 ↓\n",
       "18      qwen2.5:0.5b      sp                 5                       6 ↑\n",
       "19      qwen2.5:0.5b      wp                 8                       6 ↓\n",
       "20      qwen2.5:1.5b      sp                 7                       1 ↓\n",
       "21      qwen2.5:1.5b      wp                 2                       1 ↓\n",
       "22       qwen2.5:14b      sp                 3                       4 ↑\n",
       "23       qwen2.5:14b      wp                 7                       4 ↓\n",
       "24       qwen2.5:32b      sp                 7                       3 ↓\n",
       "25       qwen2.5:32b      wp                 7                       3 ↓\n",
       "26        qwen2.5:3b      sp                 5                       3 ↓\n",
       "27        qwen2.5:3b      wp                 8                       1 ↓\n",
       "28        qwen2.5:7b      sp                 8                       5 ↓\n",
       "29        qwen2.5:7b      wp                 7                       8 ↑"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: ↑ indicates an increase in optimal n-shot examples with system prompt optimization\n",
      "      ↓ indicates a decrease in optimal n-shot examples with system prompt optimization\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from scripts.prompt_helpers import TemplateNameType, get_few_shot_chat_template\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "with open(\"results/best_system_prompts_by_model.pkl\", \"rb\") as f:\n",
    "    best_prompt_types = pickle.load(f)\n",
    "\n",
    "with open(\"results/best_n_value_by_model.pkl\", \"rb\") as f:\n",
    "    best_n_value_by_model = pickle.load(f)\n",
    "\n",
    "with open(\"results/best_n_value_by_model_system_prompt.pkl\", \"rb\") as f:\n",
    "    best_n_value_by_model_system_prompt = pickle.load(f)\n",
    "\n",
    "\n",
    "def print_n_shot_comparison_table():\n",
    "    \"\"\"\n",
    "    Prints a comparison table showing the best n-shot values for each model\n",
    "    with default system prompt vs. optimized system prompt.\n",
    "    \"\"\"\n",
    "    # Create data for the DataFrame\n",
    "    data = []\n",
    "    model_names = sorted(best_n_value_by_model.keys())\n",
    "\n",
    "    for model in model_names:\n",
    "        for dataset_name in [\"sp\", \"wp\"]:\n",
    "            default_n = best_n_value_by_model[model][dataset_name]\n",
    "            system_prompt_n = best_n_value_by_model_system_prompt[model][dataset_name]\n",
    "\n",
    "            # Determine change direction\n",
    "            if default_n != system_prompt_n:\n",
    "                difference = \"↑\" if system_prompt_n > default_n else \"↓\"\n",
    "                system_prompt_display = f\"{system_prompt_n} {difference}\"\n",
    "            else:\n",
    "                system_prompt_display = f\"{system_prompt_n}\"\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Default Prompt N\": default_n,\n",
    "                    \"Optimized System Prompt N\": system_prompt_display,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "    print(\n",
    "        \"\\nNote: ↑ indicates an increase in optimal n-shot examples with system prompt optimization\"\n",
    "    )\n",
    "    print(\n",
    "        \"      ↓ indicates a decrease in optimal n-shot examples with system prompt optimization\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Print the comparison table\n",
    "print_n_shot_comparison_table()\n",
    "\n",
    "\n",
    "def few_shot_prompt_template_generator_baseline(\n",
    "    model_name: str, dataset: Dataset\n",
    ") -> Callable[[str], ChatPromptTemplate]:\n",
    "    if dataset.name == \"sp\":\n",
    "        few_shot_examples = sp_examples\n",
    "    elif dataset.name == \"wp\":\n",
    "        few_shot_examples = wp_examples\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset.name}\")\n",
    "\n",
    "    # Cleanup model name to match the keys in the best_prompt_types dictionary\n",
    "    model_name = model_name[0 : model_name.index(\"b-\") + 1]\n",
    "\n",
    "    # Use default system prompt for baseline\n",
    "    best_system_template_name: TemplateNameType = \"default\"\n",
    "\n",
    "    # Get the best number of shots for the model and dataset\n",
    "    number_of_shots = best_n_value_by_model[model_name][dataset.name]\n",
    "    template = get_few_shot_chat_template(\n",
    "        few_shot_examples,\n",
    "        args_generator,\n",
    "        best_system_template_name,\n",
    "        number_of_shots,\n",
    "    )\n",
    "    return template\n",
    "\n",
    "\n",
    "def few_shot_prompt_template_generator_system_prompt(\n",
    "    model_name: str, dataset: Dataset\n",
    ") -> Callable[[str], ChatPromptTemplate]:\n",
    "    if dataset.name == \"sp\":\n",
    "        few_shot_examples = sp_examples\n",
    "    elif dataset.name == \"wp\":\n",
    "        few_shot_examples = wp_examples\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset.name}\")\n",
    "\n",
    "    # Cleanup model name to match the keys in the best_prompt_types dictionary\n",
    "    model_name = model_name[0 : model_name.index(\"b-\") + 1]\n",
    "\n",
    "    # Get the best system template name for the model and dataset\n",
    "    best_system_template_name: TemplateNameType = best_prompt_types[model_name][\n",
    "        dataset.name\n",
    "    ][\"prompt_type\"]\n",
    "\n",
    "    # Get the best number of shots for the model and dataset\n",
    "    number_of_shots = best_n_value_by_model_system_prompt[model_name][dataset.name]\n",
    "\n",
    "    template = get_few_shot_chat_template(\n",
    "        few_shot_examples,\n",
    "        args_generator,\n",
    "        best_system_template_name,\n",
    "        number_of_shots,\n",
    "    )\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:50:42,957 - INFO - Starting execution 'baseline-few-shot-evaluation': 2 dataset(s) x 15 model(s) = 16545 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43af3a18a4614b7ea4581845ad43ede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "baseline-few-shot-evaluation:   0%|          | 0/16545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 12:45:29,260 - INFO - Saving results to results/baseline-few-shot-evaluation/baseline-few-shot-evaluation_results.pkl\n",
      "2025-03-12 12:45:41,552 - INFO - Execution 'baseline-few-shot-evaluation' completed successfully.\n"
     ]
    }
   ],
   "source": [
    "results_baseline = await executor.aexecute(\n",
    "    executor_data,\n",
    "    lambda model_name, dataset: few_shot_prompt_template_generator_baseline(\n",
    "        model_name, dataset\n",
    "    ),\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    run_name=\"baseline-few-shot-evaluation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 12:45:41,564 - INFO - Starting execution 'system-optimized-few-shot-evaluation': 2 dataset(s) x 15 model(s) = 16545 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09691062e89b4b0c9c55755dea2af633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "system-optimized-few-shot-evaluation:   0%|          | 0/16545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 14:26:31,671 - INFO - Saving results to results/system-optimized-few-shot-evaluation/system-optimized-few-shot-evaluation_results.pkl\n",
      "2025-03-12 14:26:44,267 - INFO - Execution 'system-optimized-few-shot-evaluation' completed successfully.\n"
     ]
    }
   ],
   "source": [
    "results_system_prompt = await executor.aexecute(\n",
    "    executor_data,\n",
    "    lambda model_name, dataset: few_shot_prompt_template_generator_system_prompt(\n",
    "        model_name, dataset\n",
    "    ),\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    run_name=\"system-optimized-few-shot-evaluation\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
