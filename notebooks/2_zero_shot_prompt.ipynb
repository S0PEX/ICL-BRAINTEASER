{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "from scripts.prompt_helpers import create_prompt_template\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 20:01:29,231 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModelBuilder\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://50.173.30.254:40106\"\n",
    "model_builder = OllamaModelBuilder(base_url)\n",
    "\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        model_builder.build_model(\"llama3.1:8b-instruct-q8_0\"),  # => 9 GB\n",
    "        # Llama3.2\n",
    "        model_builder.build_model(\"llama3.2:1b-instruct-fp16\"),  # => 2.5 GB\n",
    "        model_builder.build_model(\"llama3.2:3b-instruct-fp16\"),  # => 6.4 GB\n",
    "        # Phi3.5\n",
    "        model_builder.build_model(\"phi3.5:3.8b-mini-instruct-fp16\"),  # => 7.6 GB\n",
    "        # Phi4\n",
    "        model_builder.build_model(\"phi4:14b-q8_0\"),  # => 16 GB\n",
    "        # Qwen2.5\n",
    "        model_builder.build_model(\"qwen2.5:0.5b-instruct-fp16\"),  # => 1 GB\n",
    "        model_builder.build_model(\"qwen2.5:1.5b-instruct-fp16\"),  # => 3.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:3b-instruct-fp16\"),  # => 6.2 GB\n",
    "        model_builder.build_model(\"qwen2.5:7b-instruct-q8_0\"),  # => 8.1 GB\n",
    "        model_builder.build_model(\"qwen2.5:14b-instruct-q8_0\"),  # => 16 GB\n",
    "        model_builder.build_model(\"qwen2.5:32b-instruct-q4_K_M\"),  # => 20 GB\n",
    "        # Gemma2\n",
    "        model_builder.build_model(\"gemma2:2b-instruct-fp16\"),  # => 5.2 GB\n",
    "        model_builder.build_model(\"gemma2:9b-instruct-q8_0\"),  # => 9.8 GB\n",
    "        model_builder.build_model(\"gemma2:27b-instruct-q4_K_M\"),  # => 22 GB\n",
    "        # Mistral Nemo\n",
    "        model_builder.build_model(\"mistral-nemo:12b-instruct-2407-q8_0\"),  # => 13 GB\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Zero-Shot Evaluation\n",
    "\n",
    "Testing performance with the minimal default system prompt: `You are an AI assistant.` without any task-specific instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 20:01:29,288 - INFO - Starting execution 'baseline-zero-shot-evaluation': 2 dataset(s) x 15 model(s) = 16785 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a4daf90c0f4358aec002b645ecaf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "baseline-zero-shot-evaluation:   0%|          | 0/16785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.executor import Dataset\n",
    "\n",
    "executor_data = [\n",
    "    Dataset(name=\"sp\", riddles=dataset.sp),\n",
    "    Dataset(name=\"wp\", riddles=dataset.wp),\n",
    "]\n",
    "chat_prompt_template = create_prompt_template(\"default\")\n",
    "wrapped_results_baseline = await executor.aexecute(\n",
    "    executor_data,\n",
    "    chat_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    run_name=\"baseline-zero-shot-evaluation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Specific System Prompt Optimization: Comparative Analysis of Zero-Shot Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "# Get the best prompt type for each model\n",
    "with open(\"results/best_system_prompts_by_model.pkl\", \"rb\") as f:\n",
    "    best_prompt_types = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_prompt_template_by_model(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "):\n",
    "    best_system_template_name = best_prompt_types[model_name][dataset_name][\n",
    "        \"prompt_type\"\n",
    "    ]\n",
    "    return create_prompt_template(best_system_template_name)\n",
    "\n",
    "\n",
    "def get_prompt_template(model_name: str, dataset: Dataset):\n",
    "    # Split name after the b paramer, e.g., llama3.1:8b-instruct-fp16 => llama3.1:8b\n",
    "    model_name = model_name[0 : model_name.index(\"b-\") + 1]\n",
    "    chat_prompt_template = create_prompt_template_by_model(model_name, dataset.name)\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 20:01:30,741 - INFO - Starting execution 'system-optimized-zero-shot-evaluation': 2 dataset(s) x 15 model(s) = 16785 riddle evaluations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4ba844d0d84e3e8ff87ac35c206c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "system-optimized-zero-shot-evaluation:   0%|          | 0/16785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrapped_results_optimized = await executor.aexecute(\n",
    "    executor_data,\n",
    "    get_prompt_template,\n",
    "    args_generator,\n",
    "    dump_to_pickle=True,\n",
    "    create_checkpoints=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    run_name=\"system_optimized_zero_shot_evaluation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.executor import WrappedResults\n",
    "from scripts.evaluation import (\n",
    "    calculate_model_accuracy,\n",
    ")\n",
    "\n",
    "\n",
    "# Extract results into a DataFrame for easier plotting\n",
    "def wrapped_results_to_pd_frame(wrapped_results: WrappedResults) -> pd.DataFrame:\n",
    "    data = []\n",
    "    for dataset_name, dataset_results in wrapped_results.results.items():\n",
    "        for model_name, model_results in dataset_results.items():\n",
    "            raw_accuracy_percentage, _, postprocessed_accuracy_percentage, _ = (\n",
    "                calculate_model_accuracy(model_results)\n",
    "            )\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Accuracy(raw)\": raw_accuracy_percentage,\n",
    "                    \"Accuracy(postprocessed)\": postprocessed_accuracy_percentage,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def plot_model_accuracy(wrapped_results: WrappedResults):\n",
    "    # Create DataFrame\n",
    "    results_df = wrapped_results_to_pd_frame(wrapped_results)\n",
    "\n",
    "    # Set plot style\n",
    "    plt.style.use(\"ggplot\")\n",
    "    sns.set(font_scale=1.2)\n",
    "\n",
    "    # Create a figure with appropriate size\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    # Create grouped bar chart with both metrics for each dataset\n",
    "    ax_raw = sns.barplot(\n",
    "        x=\"Model\",\n",
    "        y=\"Accuracy(raw)\",\n",
    "        hue=\"Dataset\",\n",
    "        data=results_df,\n",
    "        palette=\"viridis\",\n",
    "        errorbar=None,\n",
    "    )\n",
    "\n",
    "    # Add the postprocessed bars\n",
    "    ax_postprocessed = sns.barplot(  # noqa: F841\n",
    "        x=\"Model\",\n",
    "        y=\"Accuracy(postprocessed)\",\n",
    "        hue=\"Dataset\",\n",
    "        data=results_df,\n",
    "        palette=\"viridis\",\n",
    "        errorbar=None,\n",
    "        alpha=0.6,  # Make them slightly transparent to distinguish\n",
    "        ax=ax_raw,\n",
    "    )\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f\"Model Accuracy by Dataset ({wrapped_results.run_name})\", fontsize=16)\n",
    "    plt.xlabel(\"Model\", fontsize=14)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 100)  # Assuming accuracy is between 0 and 100\n",
    "\n",
    "    # Create a custom legend\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = []\n",
    "    datasets = results_df[\"Dataset\"].unique()\n",
    "    colors = sns.color_palette(\"viridis\", len(datasets))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        legend_elements.append(Patch(facecolor=colors[i], label=f\"{dataset} (Raw)\"))\n",
    "        legend_elements.append(\n",
    "            Patch(facecolor=colors[i], alpha=0.6, label=f\"{dataset} (Processed)\")\n",
    "        )\n",
    "\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_delta(baseline_results, optimized_results):\n",
    "    \"\"\"\n",
    "    Plot the delta (difference) in accuracy between optimized and baseline results.\n",
    "\n",
    "    Args:\n",
    "        baseline_results: Wrapped results from baseline evaluation\n",
    "        optimized_results: Wrapped results from optimized evaluation\n",
    "    \"\"\"\n",
    "    # Extract dataframes from both result sets using the provided function\n",
    "    baseline_df = wrapped_results_to_pd_frame(baseline_results)\n",
    "    optimized_df = wrapped_results_to_pd_frame(optimized_results)\n",
    "\n",
    "    # Merge dataframes to calculate delta\n",
    "    merged_df = pd.merge(\n",
    "        baseline_df,\n",
    "        optimized_df,\n",
    "        on=[\"Model\", \"Dataset\"],\n",
    "        suffixes=(\"_baseline\", \"_optimized\"),\n",
    "    )\n",
    "\n",
    "    # Calculate deltas\n",
    "    merged_df[\"Raw_Delta\"] = (\n",
    "        merged_df[\"Accuracy(raw)_optimized\"] - merged_df[\"Accuracy(raw)_baseline\"]\n",
    "    )\n",
    "    merged_df[\"Processed_Delta\"] = (\n",
    "        merged_df[\"Accuracy(postprocessed)_optimized\"]\n",
    "        - merged_df[\"Accuracy(postprocessed)_baseline\"]\n",
    "    )\n",
    "\n",
    "    display(merged_df)\n",
    "    # Create a figure for the delta plot\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    # Plot the deltas\n",
    "    ax = sns.barplot(\n",
    "        x=\"Model\",\n",
    "        y=\"Raw_Delta\",\n",
    "        hue=\"Dataset\",\n",
    "        data=merged_df,\n",
    "        palette=\"coolwarm\",\n",
    "        errorbar=None,\n",
    "    )\n",
    "\n",
    "    # Add the processed deltas\n",
    "    ax_processed = sns.barplot(  # noqa: F841\n",
    "        x=\"Model\",\n",
    "        y=\"Processed_Delta\",\n",
    "        hue=\"Dataset\",\n",
    "        data=merged_df,\n",
    "        palette=\"coolwarm\",\n",
    "        errorbar=None,\n",
    "        alpha=0.6,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(\n",
    "        f\"Accuracy Improvement: {optimized_results.run_name} vs {baseline_results.run_name}\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "    plt.xlabel(\"Model\", fontsize=14)\n",
    "    plt.ylabel(\"Accuracy Improvement (percentage points)\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Create a custom legend\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = []\n",
    "    datasets = merged_df[\"Dataset\"].unique()\n",
    "    colors = sns.color_palette(\"coolwarm\", len(datasets))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        legend_elements.append(Patch(facecolor=colors[i], label=f\"{dataset} (Raw)\"))\n",
    "        legend_elements.append(\n",
    "            Patch(facecolor=colors[i], alpha=0.6, label=f\"{dataset} (Processed)\")\n",
    "        )\n",
    "\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return summary statistics\n",
    "    print(\n",
    "        f\"Average raw accuracy improvement: {merged_df['Raw_Delta'].mean():.2f} percentage points\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average processed accuracy improvement: {merged_df['Processed_Delta'].mean():.2f} percentage points\"\n",
    "    )\n",
    "\n",
    "    # Show models with biggest improvements\n",
    "    print(\"\\nTop 3 models with biggest raw accuracy improvements:\")\n",
    "    top_models = (\n",
    "        merged_df.groupby(\"Model\")[\"Processed_Delta\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    for model, delta in top_models.items():\n",
    "        print(f\"  {model}: {delta:.2f} percentage points\")\n",
    "\n",
    "\n",
    "plot_accuracy_delta(wrapped_results_baseline, wrapped_results_optimized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
