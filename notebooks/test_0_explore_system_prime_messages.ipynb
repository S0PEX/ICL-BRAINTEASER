{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import setup_environment\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# System message templates (priming)\n",
    "system_templates = {\n",
    "    \"default\": \"You are an AI assistant.\",\n",
    "    \"default_improved\": \"You are an AI assistant specialized in solving lateral thinking questions. You will receive a question with multiple choices and must determine the correct answer using logical reasoning and problem-solving techniques.\",\n",
    "    \"step_by_step\": \"You are a logical problem solver. Break down the question systematically, analyze each answer choice step by step, eliminate incorrect options, and select the best answer.\",\n",
    "    \"creative\": \"You are a lateral thinker. Approach each question with flexible reasoning, exploring unconventional yet valid interpretations before selecting the best answer.\",\n",
    "    \"elimination\": \"You are a strategic reasoner. First, identify and eliminate incorrect answer choices. Then, select the most logical remaining option.\",\n",
    "    \"metaphor\": \"You are skilled in abstract reasoning. Consider both literal and metaphorical meanings in the question and choices before selecting the most insightful answer.\",\n",
    "    \"confidence\": \"You are an analytical decision-maker. Assess the likelihood of correctness for each choice, score them internally, and select the answer with the highest confidence.\",\n",
    "    \"perspective_shift\": \"You are a multi-perspective analyst. Evaluate the question from different angles, considering alternative interpretations before determining the best answer.\",\n",
    "    \"common_sense\": \"You balance logic and practicality. Apply both structured reasoning and real-world common sense to determine the most reasonable answer.\",\n",
    "    \"assumption_challenge\": \"You are a critical thinker. Identify and question hidden assumptions in the question and choices before selecting the answer that best challenges or aligns with them.\",\n",
    "    \"pattern_matching\": \"You recognize patterns and relationships. Identify logical structures, recurring themes, or hidden connections in the question and choices before selecting the best answer.\",\n",
    "    \"intuitive\": \"You combine intuition with logic. Generate an initial answer instinctively, then critically evaluate it for logical soundness before finalizing your choice.\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_system_prompt(template_name: str):\n",
    "    system_prompt = system_templates[template_name]\n",
    "    system_prompt = textwrap.dedent(system_prompt)\n",
    "\n",
    "    system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "        system_prompt, id=template_name\n",
    "    )\n",
    "    return system_prompt_template\n",
    "\n",
    "\n",
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "    Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\n",
    "\n",
    "    Question: {question}\n",
    "    Choice:\n",
    "    {choices}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    prompt_template = HumanMessagePromptTemplate.from_template(prompt)\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_prompt_template(\n",
    "    system_prompt_template_name: str = \"default\",\n",
    "):\n",
    "    system_prompt_template = get_system_prompt(system_prompt_template_name)\n",
    "    user_prompt_template = get_user_prompt()\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [system_prompt_template, user_prompt_template]\n",
    "    )\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset import BrainteaserDataset\n",
    "\n",
    "dataset = BrainteaserDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from scripts.dataset import RiddleQuestion\n",
    "\n",
    "\n",
    "def args_generator(riddle_question: RiddleQuestion):\n",
    "    template_args = {\n",
    "        \"question\": riddle_question.question,\n",
    "        \"choices\": \"\\n\".join(\n",
    "            [\n",
    "                f\"({string.ascii_uppercase[j]}) {choice}\"\n",
    "                for j, choice in enumerate(riddle_question.choice_list)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return template_args\n",
    "\n",
    "\n",
    "chat_prompt_template = create_prompt_template(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:20:18,821 - INFO - Initialized executor with 15 models.\n"
     ]
    }
   ],
   "source": [
    "from scripts.lmm import OllamaModel\n",
    "from scripts.executor import Executor\n",
    "\n",
    "base_url = \"http://136.61.33.107:41302\"\n",
    "executor = Executor(\n",
    "    models=[\n",
    "        # Llama3.1\n",
    "        OllamaModel(\"llama3.1:8b\", base_url=base_url),\n",
    "        # Llama3.2\n",
    "        OllamaModel(\"llama3.2:1b\", base_url=base_url),\n",
    "        OllamaModel(\"llama3.2:3b\", base_url=base_url),\n",
    "        # Phi3.5\n",
    "        OllamaModel(\"phi3.5:3.8b\", base_url=base_url),\n",
    "        # Phi4\n",
    "        OllamaModel(\"phi4:14b\", base_url=base_url),\n",
    "        # Qwen2.5\n",
    "        OllamaModel(\"qwen2.5:0.5b\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:1.5b\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:3b\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:7b\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:14b\", base_url=base_url),\n",
    "        OllamaModel(\"qwen2.5:32b\", base_url=base_url),\n",
    "        # Gemma2\n",
    "        OllamaModel(\"gemma2:2b\", base_url=base_url),\n",
    "        OllamaModel(\"gemma2:9b\", base_url=base_url),\n",
    "        OllamaModel(\"gemma2:27b\", base_url=base_url),\n",
    "        # Mistral Nemo\n",
    "        OllamaModel(\"mistral-nemo:12b\", base_url=base_url),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size = 50\n",
    "sp_data = dataset.sp[0:test_sample_size]\n",
    "wp_data = dataset.wp[0:test_sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:20:18,835 - INFO - Loading results from result file: results/test_0/sp_results_default.pkl\n",
      "2025-02-17 20:20:18,930 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:18,930 - INFO - Loading results from result file: results/test_0/wp_results_default.pkl\n",
      "2025-02-17 20:20:18,938 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:18,938 - INFO - Loading results from result file: results/test_0/sp_results_default_improved.pkl\n",
      "2025-02-17 20:20:18,946 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:18,946 - INFO - Loading results from result file: results/test_0/wp_results_default_improved.pkl\n",
      "2025-02-17 20:20:18,955 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:18,956 - INFO - Loading results from result file: results/test_0/sp_results_step_by_step.pkl\n",
      "2025-02-17 20:20:19,049 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,050 - INFO - Loading results from result file: results/test_0/wp_results_step_by_step.pkl\n",
      "2025-02-17 20:20:19,061 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,061 - INFO - Loading results from result file: results/test_0/sp_results_creative.pkl\n",
      "2025-02-17 20:20:19,071 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,071 - INFO - Loading results from result file: results/test_0/wp_results_creative.pkl\n",
      "2025-02-17 20:20:19,185 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,186 - INFO - Loading results from result file: results/test_0/sp_results_elimination.pkl\n",
      "2025-02-17 20:20:19,194 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,195 - INFO - Loading results from result file: results/test_0/wp_results_elimination.pkl\n",
      "2025-02-17 20:20:19,202 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,203 - INFO - Loading results from result file: results/test_0/sp_results_metaphor.pkl\n",
      "2025-02-17 20:20:19,211 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,212 - INFO - Loading results from result file: results/test_0/wp_results_metaphor.pkl\n",
      "2025-02-17 20:20:19,219 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,220 - INFO - Loading results from result file: results/test_0/sp_results_confidence.pkl\n",
      "2025-02-17 20:20:19,363 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,364 - INFO - Loading results from result file: results/test_0/wp_results_confidence.pkl\n",
      "2025-02-17 20:20:19,372 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,373 - INFO - Loading results from result file: results/test_0/sp_results_perspective_shift.pkl\n",
      "2025-02-17 20:20:19,382 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,383 - INFO - Loading results from result file: results/test_0/wp_results_perspective_shift.pkl\n",
      "2025-02-17 20:20:19,391 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,391 - INFO - Loading results from result file: results/test_0/sp_results_common_sense.pkl\n",
      "2025-02-17 20:20:19,400 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,400 - INFO - Loading results from result file: results/test_0/wp_results_common_sense.pkl\n",
      "2025-02-17 20:20:19,590 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,591 - INFO - Loading results from result file: results/test_0/sp_results_assumption_challenge.pkl\n",
      "2025-02-17 20:20:19,599 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,600 - INFO - Loading results from result file: results/test_0/wp_results_assumption_challenge.pkl\n",
      "2025-02-17 20:20:19,608 - INFO - Results file is valid, returning results\n",
      "2025-02-17 20:20:19,609 - INFO - Starting asynchronous execution\n",
      "2025-02-17 20:20:19,609 - INFO - Split dataset of 50 items into 10 batches of size 5\n",
      "2025-02-17 20:20:19,610 - INFO - Processing llama3.1:8b\n",
      "2025-02-17 20:20:19,610 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd743c8c35446608c2129287e81009a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:20:41,991 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/llama3.1:8b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:20:42,002 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-17 20:20:42,003 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:20:42,003 - INFO - Processing llama3.2:1b\n",
      "2025-02-17 20:20:42,003 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2743de34b8bc45c391d327c14597c305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:20:56,874 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/llama3.2:1b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:20:56,886 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-17 20:20:56,886 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:20:56,887 - INFO - Processing llama3.2:3b\n",
      "2025-02-17 20:20:56,887 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9976580bf324304867035bb52aca904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:21:10,414 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/llama3.2:3b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:21:10,461 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-17 20:21:10,462 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:21:10,462 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-17 20:21:10,463 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a617032cf720400fb5692ee41b8af8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:22:57,163 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/phi3.5:3.8b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:22:57,175 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-17 20:22:57,176 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:22:57,176 - INFO - Processing phi4:14b\n",
      "2025-02-17 20:22:57,176 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623024b00a10471891f7e6a45bf08a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:24:14,807 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/phi4:14b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:24:14,820 - INFO - Cleaning up phi4:14b\n",
      "2025-02-17 20:24:14,820 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:24:14,821 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-17 20:24:14,821 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee59e542386b47a88bfac4b98f1a94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:24:59,220 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:0.5b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:24:59,232 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-17 20:24:59,233 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:24:59,233 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-17 20:24:59,233 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cedb9e11494bf6979267ef798a22e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:25:19,975 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:1.5b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:25:19,986 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-17 20:25:19,987 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:25:19,987 - INFO - Processing qwen2.5:3b\n",
      "2025-02-17 20:25:19,988 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07412ff26ba64d3cbf748b9a0aa4ee5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:25:39,938 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:3b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:25:39,950 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-17 20:25:39,950 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:25:39,951 - INFO - Processing qwen2.5:7b\n",
      "2025-02-17 20:25:39,951 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f078a0ad37b94b5f88ca7a9f71d15d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:26:04,687 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:7b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:26:04,698 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-17 20:26:04,698 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:26:04,699 - INFO - Processing qwen2.5:14b\n",
      "2025-02-17 20:26:04,699 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0f874f43c246a2afe8e31176c72a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:26:48,008 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:14b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:26:48,019 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-17 20:26:48,019 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:26:48,019 - INFO - Processing qwen2.5:32b\n",
      "2025-02-17 20:26:48,020 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6364c6d61a7c4a98802d9d4601335b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:28:12,063 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/qwen2.5:32b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:28:12,074 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-17 20:28:12,074 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:28:12,074 - INFO - Processing gemma2:2b\n",
      "2025-02-17 20:28:12,075 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09682656552b47deb83c493a743af134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:28:30,499 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/gemma2:2b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:28:30,510 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-17 20:28:30,511 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:28:30,511 - INFO - Processing gemma2:9b\n",
      "2025-02-17 20:28:30,512 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb1e98c09d54a068bd73cf2e370a34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:29:01,907 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/gemma2:9b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:29:01,921 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-17 20:29:01,921 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:29:01,922 - INFO - Processing gemma2:27b\n",
      "2025-02-17 20:29:01,922 - INFO - Pulling Ollama model: gemma2:27b\n",
      "2025-02-17 20:29:24,875 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-d7e4b00a7d7a8d03d4eed9b0f3f61a427e9f0fc5dea6aeb414e41dee23dc8ecc-partial: no space left on device (status code: 500)\n",
      "2025-02-17 20:29:24,875 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2ce746283747d7b5fac10117c0b1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:30:33,525 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/gemma2:27b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:30:33,537 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-17 20:30:33,538 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:30:33,538 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-17 20:30:33,539 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ff91455c9c43699ae77f83a2415a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:31:05,700 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_pattern_matching/mistral-nemo:12b_test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:31:05,713 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-17 20:31:05,713 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:31:05,713 - INFO - Asynchronous execution complete\n",
      "2025-02-17 20:31:05,713 - INFO - Dumping results to results/test_0/sp_results_pattern_matching.pkl\n",
      "2025-02-17 20:31:05,849 - INFO - Starting asynchronous execution\n",
      "2025-02-17 20:31:05,850 - INFO - Split dataset of 50 items into 10 batches of size 5\n",
      "2025-02-17 20:31:05,850 - INFO - Processing llama3.1:8b\n",
      "2025-02-17 20:31:05,850 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b503935f9af4c4fb17a47cb9edbc189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:31:30,991 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/llama3.1:8b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:31:31,002 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-17 20:31:31,003 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:31:31,003 - INFO - Processing llama3.2:1b\n",
      "2025-02-17 20:31:31,004 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564ff4352fdd4931b3ea2dd0dfd06686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:31:47,527 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/llama3.2:1b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:31:47,540 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-17 20:31:47,541 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:31:47,541 - INFO - Processing llama3.2:3b\n",
      "2025-02-17 20:31:47,542 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20d6acd685648a18fe1aa615d46f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:32:01,377 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/llama3.2:3b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:32:01,388 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-17 20:32:01,388 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:32:01,389 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-17 20:32:01,389 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c090c9b2f92644df85e524107246a8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:34:12,749 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/phi3.5:3.8b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:34:12,762 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-17 20:34:12,763 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:34:12,763 - INFO - Processing phi4:14b\n",
      "2025-02-17 20:34:12,763 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d3ce6a42f344bca1c8c064e84f5daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:35:39,363 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/phi4:14b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:35:39,375 - INFO - Cleaning up phi4:14b\n",
      "2025-02-17 20:35:39,375 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:35:39,375 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-17 20:35:39,375 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6641d5fe6d6b442b83d8a16f5159c6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:36:10,212 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:0.5b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:36:10,224 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-17 20:36:10,224 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:36:10,225 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-17 20:36:10,226 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4926019939714e40b6bf409fa91f88cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:36:26,927 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:1.5b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:36:26,939 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-17 20:36:26,940 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:36:26,940 - INFO - Processing qwen2.5:3b\n",
      "2025-02-17 20:36:26,940 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea36768941d47afa2b317419c207633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:36:43,910 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:3b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:36:43,925 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-17 20:36:43,926 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:36:43,926 - INFO - Processing qwen2.5:7b\n",
      "2025-02-17 20:36:43,927 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f95cb6948247db9dbf911657617b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:37:08,787 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:7b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:37:08,798 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-17 20:37:08,798 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:37:08,798 - INFO - Processing qwen2.5:14b\n",
      "2025-02-17 20:37:08,798 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e6e747f784459287b4c1fe6abb7b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:38:00,228 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:14b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:38:00,239 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-17 20:38:00,239 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:38:00,240 - INFO - Processing qwen2.5:32b\n",
      "2025-02-17 20:38:00,240 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb9b1bbc3db44b6b3c4631d422c97ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:39:41,077 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/qwen2.5:32b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:39:41,090 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-17 20:39:41,091 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:39:41,091 - INFO - Processing gemma2:2b\n",
      "2025-02-17 20:39:41,091 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c747afd07044e5bb25810a6145e551e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:39:57,108 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/gemma2:2b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:39:57,122 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-17 20:39:57,122 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:39:57,123 - INFO - Processing gemma2:9b\n",
      "2025-02-17 20:39:57,123 - INFO - Pulling Ollama model: gemma2:9b\n",
      "2025-02-17 20:40:08,532 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-ff1d1fc78170d787ee1201778e2dd65ea211654ca5fb7d69b5a2e7b123a50373-partial: no space left on device (status code: 500)\n",
      "2025-02-17 20:40:08,532 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb172654262d4cbc8cca34f84595afe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:40:54,603 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/gemma2:9b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:40:54,614 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-17 20:40:54,615 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:40:54,615 - INFO - Processing gemma2:27b\n",
      "2025-02-17 20:40:54,616 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047e3a87632b417d8a0f327fa6d76deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:42:10,849 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/gemma2:27b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:42:10,861 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-17 20:42:10,861 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:42:10,861 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-17 20:42:10,862 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f66faee7e4647858f6572357df2918f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:42:49,480 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_pattern_matching/mistral-nemo:12b_test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:42:49,490 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-17 20:42:49,491 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:42:49,491 - INFO - Asynchronous execution complete\n",
      "2025-02-17 20:42:49,491 - INFO - Dumping results to results/test_0/wp_results_pattern_matching.pkl\n",
      "2025-02-17 20:42:49,633 - INFO - Starting asynchronous execution\n",
      "2025-02-17 20:42:49,633 - INFO - Split dataset of 50 items into 10 batches of size 5\n",
      "2025-02-17 20:42:49,634 - INFO - Processing llama3.1:8b\n",
      "2025-02-17 20:42:49,634 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de151b0bb4a4bd0b0f0c0455d25c887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:43:18,386 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/llama3.1:8b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:43:18,398 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-17 20:43:18,399 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:43:18,399 - INFO - Processing llama3.2:1b\n",
      "2025-02-17 20:43:18,399 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc853391554342399d19be9fc7bc66a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:43:34,129 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/llama3.2:1b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:43:34,140 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-17 20:43:34,140 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:43:34,141 - INFO - Processing llama3.2:3b\n",
      "2025-02-17 20:43:34,141 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db57c2de3c194f6fafc4fc2881672bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:43:48,984 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/llama3.2:3b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:43:48,995 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-17 20:43:48,996 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:43:48,996 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-17 20:43:48,996 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d9b46136764e34b62e48927dfd01b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:46:08,608 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/phi3.5:3.8b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:46:08,621 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-17 20:46:08,622 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:46:08,622 - INFO - Processing phi4:14b\n",
      "2025-02-17 20:46:08,622 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4804eb7ce1ba4b1fa4741d22cdc74347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:48:10,156 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/phi4:14b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:48:10,167 - INFO - Cleaning up phi4:14b\n",
      "2025-02-17 20:48:10,167 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:48:10,168 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-17 20:48:10,168 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cea63fc48384ce788c83f706490c1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:48:58,379 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:0.5b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:48:58,391 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-17 20:48:58,391 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:48:58,392 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-17 20:48:58,392 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58bd8834ed646459101d4b58e280538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:49:29,928 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:1.5b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:49:29,939 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-17 20:49:29,939 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:49:29,939 - INFO - Processing qwen2.5:3b\n",
      "2025-02-17 20:49:29,940 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc410f9279bd4ff3b3049af05fae859c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:50:32,313 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:3b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:50:32,329 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-17 20:50:32,330 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:50:32,330 - INFO - Processing qwen2.5:7b\n",
      "2025-02-17 20:50:32,330 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc743f7605074169a1c8cd54a569c8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:51:26,931 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:7b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:51:26,942 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-17 20:51:26,942 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:51:26,942 - INFO - Processing qwen2.5:14b\n",
      "2025-02-17 20:51:26,943 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962ed493525d4c4ab9fbbb7e62d7d728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:52:57,096 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:14b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:52:57,109 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-17 20:52:57,109 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:52:57,110 - INFO - Processing qwen2.5:32b\n",
      "2025-02-17 20:52:57,110 - INFO - Pulling Ollama model: qwen2.5:32b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ee6e7359194090b8cffdc8cb805c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:54:38,346 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/qwen2.5:32b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:54:38,357 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-17 20:54:38,357 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:54:38,357 - INFO - Processing gemma2:2b\n",
      "2025-02-17 20:54:38,358 - INFO - Pulling Ollama model: gemma2:2b\n",
      "2025-02-17 20:54:41,158 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-7462734796d67c40ecec2ca98eddf970e171dbb6b370e43fd633ee75b69abe1b-partial: no space left on device (status code: 500)\n",
      "2025-02-17 20:54:41,158 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c277e1c18646558a988afc3ae181c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:55:29,042 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/gemma2:2b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:55:29,057 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-17 20:55:29,058 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:55:29,058 - INFO - Processing gemma2:9b\n",
      "2025-02-17 20:55:29,058 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45744a6aa4a44daeaaab69f9950dd41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:56:29,133 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/gemma2:9b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:56:29,145 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-17 20:56:29,145 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:56:29,146 - INFO - Processing gemma2:27b\n",
      "2025-02-17 20:56:29,146 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b4f12b43d744fb9ce6118b81f2506d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:58:00,019 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/gemma2:27b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:58:00,030 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-17 20:58:00,030 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:58:00,031 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-17 20:58:00,031 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b38615730404cbbab84ca4b57e57035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:58:38,983 - INFO - Creating checkpoint: results/checkpoints/test_0/sp_results_intuitive/mistral-nemo:12b_test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:58:38,995 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-17 20:58:38,995 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:58:38,996 - INFO - Asynchronous execution complete\n",
      "2025-02-17 20:58:38,996 - INFO - Dumping results to results/test_0/sp_results_intuitive.pkl\n",
      "2025-02-17 20:58:39,138 - INFO - Starting asynchronous execution\n",
      "2025-02-17 20:58:39,139 - INFO - Split dataset of 50 items into 10 batches of size 5\n",
      "2025-02-17 20:58:39,139 - INFO - Processing llama3.1:8b\n",
      "2025-02-17 20:58:39,139 - INFO - Pulling Ollama model: llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc8e7ed799c49ae96beec5df433719f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.1:8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:59:03,947 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/llama3.1:8b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 20:59:03,963 - INFO - Cleaning up llama3.1:8b\n",
      "2025-02-17 20:59:03,964 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:59:03,964 - INFO - Processing llama3.2:1b\n",
      "2025-02-17 20:59:03,965 - INFO - Pulling Ollama model: llama3.2:1b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffd63f9c6c94b71b91f832b3089071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:1b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:59:19,422 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/llama3.2:1b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 20:59:19,432 - INFO - Cleaning up llama3.2:1b\n",
      "2025-02-17 20:59:19,433 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:59:19,433 - INFO - Processing llama3.2:3b\n",
      "2025-02-17 20:59:19,433 - INFO - Pulling Ollama model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ea2a41e2ed466187cf8629686a5bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:59:33,603 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/llama3.2:3b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 20:59:33,614 - INFO - Cleaning up llama3.2:3b\n",
      "2025-02-17 20:59:33,614 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 20:59:33,615 - INFO - Processing phi3.5:3.8b\n",
      "2025-02-17 20:59:33,615 - INFO - Pulling Ollama model: phi3.5:3.8b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d217f7c23d0c45e8903c94ecb0e013a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi3.5:3.8b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:01:48,753 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/phi3.5:3.8b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:01:48,766 - INFO - Cleaning up phi3.5:3.8b\n",
      "2025-02-17 21:01:48,766 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:01:48,767 - INFO - Processing phi4:14b\n",
      "2025-02-17 21:01:48,767 - INFO - Pulling Ollama model: phi4:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287f2f17cf834b5686d0a0405443ee7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "phi4:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:04:13,687 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/phi4:14b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:04:13,698 - INFO - Cleaning up phi4:14b\n",
      "2025-02-17 21:04:13,699 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:04:13,699 - INFO - Processing qwen2.5:0.5b\n",
      "2025-02-17 21:04:13,700 - INFO - Pulling Ollama model: qwen2.5:0.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa3280e658f48cdad727587468f8a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:0.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:04:55,485 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:0.5b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:04:55,498 - INFO - Cleaning up qwen2.5:0.5b\n",
      "2025-02-17 21:04:55,498 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:04:55,498 - INFO - Processing qwen2.5:1.5b\n",
      "2025-02-17 21:04:55,499 - INFO - Pulling Ollama model: qwen2.5:1.5b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d891b4cd8494d2d8fd3eb71fcdd0f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:1.5b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:05:19,338 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:1.5b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:05:19,349 - INFO - Cleaning up qwen2.5:1.5b\n",
      "2025-02-17 21:05:19,349 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:05:19,350 - INFO - Processing qwen2.5:3b\n",
      "2025-02-17 21:05:19,350 - INFO - Pulling Ollama model: qwen2.5:3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac09b6164f8e42b7b324669adbf4cb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:3b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:06:15,667 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:3b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:06:15,681 - INFO - Cleaning up qwen2.5:3b\n",
      "2025-02-17 21:06:15,682 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:06:15,682 - INFO - Processing qwen2.5:7b\n",
      "2025-02-17 21:06:15,682 - INFO - Pulling Ollama model: qwen2.5:7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5790e659636e40d4adf3dd4cd2d07214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:7b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:07:27,084 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:7b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:07:27,096 - INFO - Cleaning up qwen2.5:7b\n",
      "2025-02-17 21:07:27,096 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:07:27,096 - INFO - Processing qwen2.5:14b\n",
      "2025-02-17 21:07:27,097 - INFO - Pulling Ollama model: qwen2.5:14b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b658214d71b4ebab388345a406e5f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:14b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:09:23,258 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:14b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:09:23,271 - INFO - Cleaning up qwen2.5:14b\n",
      "2025-02-17 21:09:23,272 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:09:23,272 - INFO - Processing qwen2.5:32b\n",
      "2025-02-17 21:09:23,272 - INFO - Pulling Ollama model: qwen2.5:32b\n",
      "2025-02-17 21:10:05,406 - ERROR - Error pulling Ollama model: write /root/.ollama/models/blobs/sha256-eabc98a9bcbfce7fd70f3e07de599f8fda98120fefed5881934161ede8bd1a41-partial: no space left on device (status code: 500)\n",
      "2025-02-17 21:10:05,407 - INFO - Deleting all ollama models to free up space\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a99037e9b094346afd355024bc4dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2.5:32b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:11:53,177 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/qwen2.5:32b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:11:53,188 - INFO - Cleaning up qwen2.5:32b\n",
      "2025-02-17 21:11:53,189 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:11:53,189 - INFO - Processing gemma2:2b\n",
      "2025-02-17 21:11:53,189 - INFO - Pulling Ollama model: gemma2:2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4838d27f95b42dba8a0fb71a7ee2b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:2b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:12:39,041 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/gemma2:2b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:12:39,051 - INFO - Cleaning up gemma2:2b\n",
      "2025-02-17 21:12:39,052 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:12:39,052 - INFO - Processing gemma2:9b\n",
      "2025-02-17 21:12:39,052 - INFO - Pulling Ollama model: gemma2:9b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0786b054445c46c1ac37cc8e5c37349e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:9b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:13:32,613 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/gemma2:9b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:13:32,625 - INFO - Cleaning up gemma2:9b\n",
      "2025-02-17 21:13:32,625 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:13:32,626 - INFO - Processing gemma2:27b\n",
      "2025-02-17 21:13:32,626 - INFO - Pulling Ollama model: gemma2:27b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998708a3cce2453daab51f07b3d1ce7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma2:27b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:15:26,140 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/gemma2:27b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:15:26,151 - INFO - Cleaning up gemma2:27b\n",
      "2025-02-17 21:15:26,151 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:15:26,152 - INFO - Processing mistral-nemo:12b\n",
      "2025-02-17 21:15:26,152 - INFO - Pulling Ollama model: mistral-nemo:12b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af355f76bb4c42d09b5a6df8a7d4da42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-nemo:12b:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:16:09,847 - INFO - Creating checkpoint: results/checkpoints/test_0/wp_results_intuitive/mistral-nemo:12b_test_0/wp_results_intuitive.pkl\n",
      "2025-02-17 21:16:09,860 - INFO - Cleaning up mistral-nemo:12b\n",
      "2025-02-17 21:16:09,860 - INFO - Ollama models will be deleted on demand and therefore this step is skipped!\n",
      "2025-02-17 21:16:09,861 - INFO - Asynchronous execution complete\n",
      "2025-02-17 21:16:09,861 - INFO - Dumping results to results/test_0/wp_results_intuitive.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "total_results = {}\n",
    "\n",
    "reduced_dataset = dataset.sp[0:]\n",
    "for technique in system_templates:\n",
    "    chat_prompt_template = create_prompt_template(technique)\n",
    "    sp_results = await executor.aexecute(\n",
    "        sp_data,\n",
    "        chat_prompt_template,\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        result_file_name=f\"test_0/sp_results_{technique}\",\n",
    "    )\n",
    "\n",
    "    wp_results = await executor.aexecute(\n",
    "        wp_data,\n",
    "        chat_prompt_template,\n",
    "        args_generator,\n",
    "        dump_to_pickle=True,\n",
    "        create_checkpoints=True,\n",
    "        resume_from_checkpoint=True,\n",
    "        result_file_name=f\"test_0/wp_results_{technique}\",\n",
    "    )\n",
    "\n",
    "    total_results[technique] = {\"sp\": sp_results, \"wp\": wp_results}\n",
    "\n",
    "results_file = Path(\"test_0/results_test_0_system-prime-messages.pkl\")\n",
    "results_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with results_file.open(\"wb\") as f:\n",
    "    pickle.dump(total_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
